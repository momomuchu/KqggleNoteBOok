{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "47f9758b",
   "metadata": {
    "papermill": {
     "duration": 0.025816,
     "end_time": "2021-11-06T11:46:24.632759",
     "exception": false,
     "start_time": "2021-11-06T11:46:24.606943",
     "status": "completed"
    },
    "tags": []
   },
   "source": [
    "Original notebook by @rhtsingh -  https://www.kaggle.com/rashmibanthia/xlm-roberta-jaccard-validation-mean-max-head\n",
    "\n",
    "I have added - \n",
    " - replace the mean/max head with weightedlayerpooling\n",
    " \n",
    " "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "86b822d0",
   "metadata": {
    "papermill": {
     "duration": 0.047006,
     "end_time": "2021-11-06T11:46:24.727747",
     "exception": false,
     "start_time": "2021-11-06T11:46:24.680741",
     "status": "completed"
    },
    "tags": []
   },
   "source": [
    "#### Import Dependencies"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "6f7e2cd9",
   "metadata": {
    "_cell_guid": "b1076dfc-b9ad-4769-8c92-a6c4dae69d19",
    "_uuid": "8f2839f25d086af736a60e9eeb907d3b93b6e0e5",
    "execution": {
     "iopub.execute_input": "2021-11-06T11:46:24.785498Z",
     "iopub.status.busy": "2021-11-06T11:46:24.783270Z",
     "iopub.status.idle": "2021-11-06T11:46:33.149894Z",
     "shell.execute_reply": "2021-11-06T11:46:33.149125Z",
     "shell.execute_reply.started": "2021-11-06T03:25:03.880147Z"
    },
    "papermill": {
     "duration": 8.398433,
     "end_time": "2021-11-06T11:46:33.150085",
     "exception": false,
     "start_time": "2021-11-06T11:46:24.751652",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "import os\n",
    "import gc\n",
    "gc.enable()\n",
    "import math\n",
    "import json\n",
    "import time\n",
    "import random\n",
    "import multiprocessing\n",
    "import warnings\n",
    "warnings.filterwarnings(\"ignore\", category=UserWarning)\n",
    "\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "from tqdm import tqdm, trange\n",
    "from sklearn import model_selection\n",
    "\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "from torch.nn import Parameter\n",
    "import torch.optim as optim\n",
    "from torch.utils.data import (\n",
    "    Dataset, DataLoader,\n",
    "    SequentialSampler, RandomSampler\n",
    ")\n",
    "from torch.utils.data.distributed import DistributedSampler\n",
    "\n",
    "try:\n",
    "    from apex import amp\n",
    "    APEX_INSTALLED = True\n",
    "except ImportError:\n",
    "    APEX_INSTALLED = False\n",
    "\n",
    "import transformers\n",
    "from transformers import (\n",
    "    WEIGHTS_NAME,\n",
    "    AdamW,\n",
    "    AutoConfig,\n",
    "    AutoModel,\n",
    "    AutoTokenizer,\n",
    "    get_cosine_schedule_with_warmup,\n",
    "    get_linear_schedule_with_warmup,\n",
    "    logging,\n",
    "    MODEL_FOR_QUESTION_ANSWERING_MAPPING,\n",
    ")\n",
    "logging.set_verbosity_warning()\n",
    "logging.set_verbosity_error()\n",
    "\n",
    "def fix_all_seeds(seed):\n",
    "    np.random.seed(seed)\n",
    "    random.seed(seed)\n",
    "    os.environ['PYTHONHASHSEED'] = str(seed)\n",
    "    torch.manual_seed(seed)\n",
    "    torch.cuda.manual_seed(seed)\n",
    "    torch.cuda.manual_seed_all(seed)\n",
    "\n",
    "def optimal_num_of_loader_workers():\n",
    "    num_cpus = multiprocessing.cpu_count()\n",
    "    num_gpus = torch.cuda.device_count()\n",
    "    optimal_value = min(num_cpus, num_gpus*4) if num_gpus else num_cpus - 1\n",
    "    return optimal_value\n",
    "\n",
    "MODEL_CONFIG_CLASSES = list(MODEL_FOR_QUESTION_ANSWERING_MAPPING.keys())\n",
    "MODEL_TYPES = tuple(conf.model_type for conf in MODEL_CONFIG_CLASSES)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d60e13d0",
   "metadata": {
    "papermill": {
     "duration": 0.024571,
     "end_time": "2021-11-06T11:46:33.198946",
     "exception": false,
     "start_time": "2021-11-06T11:46:33.174375",
     "status": "completed"
    },
    "tags": []
   },
   "source": [
    "#### Training Configuration"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "2c8ec988",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2021-11-06T11:46:33.255116Z",
     "iopub.status.busy": "2021-11-06T11:46:33.254153Z",
     "iopub.status.idle": "2021-11-06T11:46:33.257570Z",
     "shell.execute_reply": "2021-11-06T11:46:33.256965Z",
     "shell.execute_reply.started": "2021-11-06T03:25:11.213602Z"
    },
    "papermill": {
     "duration": 0.033798,
     "end_time": "2021-11-06T11:46:33.257779",
     "exception": false,
     "start_time": "2021-11-06T11:46:33.223981",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "class Config:\n",
    "    # model\n",
    "    model_type = 'xlm_roberta'\n",
    "    model_name_or_path = '../input/xlm-roberta-squad2/deepset/xlm-roberta-large-squad2'\n",
    "    config_name = '../input/xlm-roberta-squad2/deepset/xlm-roberta-large-squad2'\n",
    "    fp16 = True if APEX_INSTALLED else False\n",
    "    fp16_opt_level = \"O1\"\n",
    "    gradient_accumulation_steps = 2\n",
    "\n",
    "    # tokenizer\n",
    "    tokenizer_name = '../input/xlm-roberta-squad2/deepset/xlm-roberta-large-squad2'\n",
    "    max_seq_length = 384\n",
    "    doc_stride = 128\n",
    "\n",
    "    # train\n",
    "    epochs = 1\n",
    "    train_batch_size = 4\n",
    "    eval_batch_size = 8\n",
    "\n",
    "    # optimizer\n",
    "    optimizer_type = 'AdamW'\n",
    "    learning_rate = 1e-5 #1.5e-5\n",
    "    weight_decay = 1e-2\n",
    "    epsilon = 1e-8\n",
    "    max_grad_norm = 1.0\n",
    "\n",
    "    # scheduler\n",
    "    decay_name = 'linear-warmup'\n",
    "    warmup_ratio = 0.1\n",
    "\n",
    "    # logging\n",
    "    logging_steps = 10\n",
    "\n",
    "    # evaluate\n",
    "    output_dir = 'output'\n",
    "    seed = 2021"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "118887c8",
   "metadata": {
    "papermill": {
     "duration": 0.022649,
     "end_time": "2021-11-06T11:46:33.303971",
     "exception": false,
     "start_time": "2021-11-06T11:46:33.281322",
     "status": "completed"
    },
    "tags": []
   },
   "source": [
    "#### Data Factory"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "7e894168",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2021-11-06T11:46:33.362123Z",
     "iopub.status.busy": "2021-11-06T11:46:33.359184Z",
     "iopub.status.idle": "2021-11-06T11:46:34.676063Z",
     "shell.execute_reply": "2021-11-06T11:46:34.675194Z",
     "shell.execute_reply.started": "2021-11-06T03:25:11.222416Z"
    },
    "papermill": {
     "duration": 1.349513,
     "end_time": "2021-11-06T11:46:34.676193",
     "exception": false,
     "start_time": "2021-11-06T11:46:33.326680",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "train = pd.read_csv('../input/chaii-hindi-and-tamil-question-answering/train.csv')\n",
    "test = pd.read_csv('../input/chaii-hindi-and-tamil-question-answering/test.csv')\n",
    "external_mlqa = pd.read_csv('../input/mlqa-hindi-processed/mlqa_hindi.csv')\n",
    "external_xquad = pd.read_csv('../input/mlqa-hindi-processed/xquad.csv')\n",
    "external_train = pd.concat([external_mlqa, external_xquad])\n",
    "\n",
    "def create_folds(data, num_splits):\n",
    "    data[\"kfold\"] = -1\n",
    "    kf = model_selection.StratifiedKFold(n_splits=num_splits, shuffle=True, random_state=2021)\n",
    "    for f, (t_, v_) in enumerate(kf.split(X=data, y=data['language'])):\n",
    "        data.loc[v_, 'kfold'] = f\n",
    "    return data\n",
    "\n",
    "train = create_folds(train, num_splits=5)\n",
    "external_train[\"kfold\"] = -1\n",
    "external_train['id'] = list(np.arange(1, len(external_train)+1))\n",
    "train = pd.concat([train, external_train]).reset_index(drop=True)\n",
    "\n",
    "def convert_answers(row):\n",
    "    return {'answer_start': [row[0]], 'text': [row[1]]}\n",
    "\n",
    "train['answers'] = train[['answer_start', 'answer_text']].apply(convert_answers, axis=1)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8c35fd37",
   "metadata": {
    "papermill": {
     "duration": 0.023495,
     "end_time": "2021-11-06T11:46:34.722853",
     "exception": false,
     "start_time": "2021-11-06T11:46:34.699358",
     "status": "completed"
    },
    "tags": []
   },
   "source": [
    "#### Convert Examples to Features (Preprocess)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "cf964765",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2021-11-06T11:46:34.786584Z",
     "iopub.status.busy": "2021-11-06T11:46:34.785492Z",
     "iopub.status.idle": "2021-11-06T11:46:34.789529Z",
     "shell.execute_reply": "2021-11-06T11:46:34.788965Z",
     "shell.execute_reply.started": "2021-11-06T03:25:12.618697Z"
    },
    "papermill": {
     "duration": 0.04319,
     "end_time": "2021-11-06T11:46:34.789735",
     "exception": false,
     "start_time": "2021-11-06T11:46:34.746545",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "def prepare_train_features(args, example, tokenizer):\n",
    "    example[\"question\"] = example[\"question\"].lstrip()\n",
    "    tokenized_example = tokenizer(\n",
    "        example[\"question\"],\n",
    "        example[\"context\"],\n",
    "        truncation=\"only_second\",\n",
    "        max_length=args.max_seq_length,\n",
    "        stride=args.doc_stride,\n",
    "        return_overflowing_tokens=True,\n",
    "        return_offsets_mapping=True,\n",
    "        padding=\"max_length\",\n",
    "    )\n",
    "\n",
    "    sample_mapping = tokenized_example.pop(\"overflow_to_sample_mapping\")\n",
    "    offset_mapping = tokenized_example.pop(\"offset_mapping\")\n",
    "\n",
    "    features = []\n",
    "    for i, offsets in enumerate(offset_mapping):\n",
    "        feature = {}\n",
    "\n",
    "        input_ids = tokenized_example[\"input_ids\"][i]\n",
    "        attention_mask = tokenized_example[\"attention_mask\"][i]\n",
    "\n",
    "        feature['input_ids'] = input_ids\n",
    "        feature['attention_mask'] = attention_mask\n",
    "        feature['offset_mapping'] = offsets\n",
    "\n",
    "        cls_index = input_ids.index(tokenizer.cls_token_id)\n",
    "        sequence_ids = tokenized_example.sequence_ids(i)\n",
    "        \n",
    "        ## for validation\n",
    "        feature[\"example_id\"] = example['id'] \n",
    "        feature['sequence_ids'] = [0 if i is None else i for i in tokenized_example.sequence_ids(i)] \n",
    "        feature['context'] = example[\"context\"]\n",
    "        feature['question'] = example[\"question\"]\n",
    "        feature['hindi_tamil'] = 0 if example[\"language\"]=='hindi' else 1 \n",
    "        ## \n",
    "\n",
    "        sample_index = sample_mapping[i]\n",
    "        answers = example[\"answers\"]\n",
    "\n",
    "        if len(answers[\"answer_start\"]) == 0:\n",
    "            feature[\"start_position\"] = cls_index\n",
    "            feature[\"end_position\"] = cls_index\n",
    "        else:\n",
    "            start_char = answers[\"answer_start\"][0]\n",
    "            end_char = start_char + len(answers[\"text\"][0])\n",
    "\n",
    "            token_start_index = 0\n",
    "            while sequence_ids[token_start_index] != 1:\n",
    "                token_start_index += 1\n",
    "\n",
    "            token_end_index = len(input_ids) - 1\n",
    "            while sequence_ids[token_end_index] != 1:\n",
    "                token_end_index -= 1\n",
    "\n",
    "            if not (offsets[token_start_index][0] <= start_char and offsets[token_end_index][1] >= end_char):\n",
    "                feature[\"start_position\"] = cls_index\n",
    "                feature[\"end_position\"] = cls_index\n",
    "            else:\n",
    "                while token_start_index < len(offsets) and offsets[token_start_index][0] <= start_char:\n",
    "                    token_start_index += 1\n",
    "                feature[\"start_position\"] = token_start_index - 1\n",
    "                while offsets[token_end_index][1] >= end_char:\n",
    "                    token_end_index -= 1\n",
    "                feature[\"end_position\"] = token_end_index + 1\n",
    "\n",
    "        features.append(feature)\n",
    "    return features"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4f78cc7d",
   "metadata": {
    "papermill": {
     "duration": 0.024002,
     "end_time": "2021-11-06T11:46:34.838081",
     "exception": false,
     "start_time": "2021-11-06T11:46:34.814079",
     "status": "completed"
    },
    "tags": []
   },
   "source": [
    "#### Dataset Retriever"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "5671dedb",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2021-11-06T11:46:34.899738Z",
     "iopub.status.busy": "2021-11-06T11:46:34.898865Z",
     "iopub.status.idle": "2021-11-06T11:46:34.902714Z",
     "shell.execute_reply": "2021-11-06T11:46:34.902184Z",
     "shell.execute_reply.started": "2021-11-06T03:25:12.634271Z"
    },
    "papermill": {
     "duration": 0.040177,
     "end_time": "2021-11-06T11:46:34.902863",
     "exception": false,
     "start_time": "2021-11-06T11:46:34.862686",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "class DatasetRetriever(Dataset):\n",
    "    def __init__(self, features, mode='train'):\n",
    "        super(DatasetRetriever, self).__init__()\n",
    "        self.features = features\n",
    "        self.mode = mode\n",
    "        \n",
    "    def __len__(self):\n",
    "        return len(self.features)\n",
    "    \n",
    "    def __getitem__(self, item):   \n",
    "        feature = self.features[item]\n",
    "        if self.mode == 'train':\n",
    "            return {\n",
    "                'input_ids':torch.tensor(feature['input_ids'], dtype=torch.long),\n",
    "                'attention_mask':torch.tensor(feature['attention_mask'], dtype=torch.long),\n",
    "                'offset_mapping':torch.tensor(feature['offset_mapping'], dtype=torch.long),\n",
    "                'start_position':torch.tensor(feature['start_position'], dtype=torch.long),\n",
    "                'end_position':torch.tensor(feature['end_position'], dtype=torch.long)\n",
    "            }\n",
    "        else:\n",
    "            if self.mode == 'valid': \n",
    "                return {\n",
    "                'input_ids':torch.tensor(feature['input_ids'], dtype=torch.long),\n",
    "                'attention_mask':torch.tensor(feature['attention_mask'], dtype=torch.long),\n",
    "                'offset_mapping':torch.tensor(feature['offset_mapping'], dtype=torch.long),\n",
    "                'sequence_ids':feature['sequence_ids'],\n",
    "                'start_position':torch.tensor(feature['start_position'], dtype=torch.long),\n",
    "                'end_position':torch.tensor(feature['end_position'], dtype=torch.long),\n",
    "                'example_id':feature['example_id'],\n",
    "                'context': feature['context'],\n",
    "                }\n",
    "            else:\n",
    "                return {\n",
    "                    'input_ids':torch.tensor(feature['input_ids'], dtype=torch.long),\n",
    "                    'attention_mask':torch.tensor(feature['attention_mask'], dtype=torch.long),\n",
    "                    'offset_mapping':feature['offset_mapping'],\n",
    "                    'sequence_ids':feature['sequence_ids'],\n",
    "                    'id':feature['example_id'],\n",
    "                    'context': feature['context'],\n",
    "                    'question': feature['question']\n",
    "                }"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "36c9eeaa",
   "metadata": {
    "papermill": {
     "duration": 0.024408,
     "end_time": "2021-11-06T11:46:34.951617",
     "exception": false,
     "start_time": "2021-11-06T11:46:34.927209",
     "status": "completed"
    },
    "tags": []
   },
   "source": [
    "#### Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "f5cd5b17",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2021-11-06T11:46:35.014856Z",
     "iopub.status.busy": "2021-11-06T11:46:35.013959Z",
     "iopub.status.idle": "2021-11-06T11:46:35.017362Z",
     "shell.execute_reply": "2021-11-06T11:46:35.016716Z",
     "shell.execute_reply.started": "2021-11-06T03:25:12.650841Z"
    },
    "papermill": {
     "duration": 0.041052,
     "end_time": "2021-11-06T11:46:35.017496",
     "exception": false,
     "start_time": "2021-11-06T11:46:34.976444",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "class WeightedLayerPooling(nn.Module):\n",
    "    def __init__(self, num_hidden_layers, layer_start: int = 4, layer_weights=None):\n",
    "        super(WeightedLayerPooling, self).__init__()\n",
    "        self.layer_start = layer_start\n",
    "        self.num_hidden_layers = num_hidden_layers\n",
    "        self.layer_weights = layer_weights if layer_weights is not None \\\n",
    "            else nn.Parameter(\n",
    "            torch.tensor([1] * (num_hidden_layers + 1 - layer_start), dtype=torch.float)\n",
    "        )\n",
    "\n",
    "    def forward(self, all_hidden_states):\n",
    "        all_layer_embedding = all_hidden_states[self.layer_start:, :, :, :]\n",
    "        weight_factor = self.layer_weights.unsqueeze(-1).unsqueeze(-1).unsqueeze(-1).expand(all_layer_embedding.size())\n",
    "        weighted_average = (weight_factor * all_layer_embedding).sum(dim=0) / self.layer_weights.sum()\n",
    "        return weighted_average\n",
    "\n",
    "\n",
    "class Model(nn.Module):\n",
    "    def __init__(self, modelname_or_path, config, layer_start, layer_weights=None):\n",
    "        super(Model, self).__init__()\n",
    "        self.config = config\n",
    "        config.update({\n",
    "            \"hidden_dropout_prob\": 0.0,\n",
    "            \"layer_norm_eps\": 1e-7,\n",
    "            \"output_hidden_states\": True\n",
    "        })\n",
    "        self.xlm_roberta = AutoModel.from_pretrained(modelname_or_path, config=config)\n",
    "        self.layer_start = layer_start\n",
    "        self.pooling = WeightedLayerPooling(config.num_hidden_layers,\n",
    "                                            layer_start=layer_start,\n",
    "                                            layer_weights=None)\n",
    "        self.layer_norm = nn.LayerNorm(config.hidden_size)\n",
    "        self.dropout = torch.nn.Dropout(0.3)\n",
    "        self.qa_output = torch.nn.Linear(config.hidden_size, 2)\n",
    "        torch.nn.init.normal_(self.qa_output.weight, std=0.02)\n",
    "\n",
    "    def forward(self, input_ids, attention_mask=None):\n",
    "        outputs = self.xlm_roberta(input_ids, attention_mask=attention_mask)\n",
    "        all_hidden_states = torch.stack(outputs.hidden_states)\n",
    "        weighted_pooling_embeddings = self.layer_norm(self.pooling(all_hidden_states))\n",
    "        #weighted_pooling_embeddings = weighted_pooling_embeddings[:, 0]\n",
    "\n",
    "        norm_embeddings = self.dropout(weighted_pooling_embeddings)\n",
    "        logits = self.qa_output(norm_embeddings)\n",
    "        start_logits, end_logits = logits.split(1, dim=-1)\n",
    "\n",
    "        start_logits = start_logits.squeeze(-1)\n",
    "        end_logits = end_logits.squeeze(-1)\n",
    "\n",
    "        return start_logits, end_logits"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "652ea0a9",
   "metadata": {
    "papermill": {
     "duration": 0.023508,
     "end_time": "2021-11-06T11:46:35.064370",
     "exception": false,
     "start_time": "2021-11-06T11:46:35.040862",
     "status": "completed"
    },
    "tags": []
   },
   "source": [
    "#### Loss"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "1519367f",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2021-11-06T11:46:35.116345Z",
     "iopub.status.busy": "2021-11-06T11:46:35.115724Z",
     "iopub.status.idle": "2021-11-06T11:46:35.120780Z",
     "shell.execute_reply": "2021-11-06T11:46:35.120113Z",
     "shell.execute_reply.started": "2021-11-06T03:25:12.669886Z"
    },
    "papermill": {
     "duration": 0.033347,
     "end_time": "2021-11-06T11:46:35.120902",
     "exception": false,
     "start_time": "2021-11-06T11:46:35.087555",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "def loss_fn(preds, labels):\n",
    "    start_preds, end_preds = preds\n",
    "    start_labels, end_labels = labels\n",
    "    start_loss = nn.CrossEntropyLoss(ignore_index=-1)(start_preds, start_labels)\n",
    "    end_loss = nn.CrossEntropyLoss(ignore_index=-1)(end_preds, end_labels)\n",
    "    total_loss = (start_loss + end_loss) / 2\n",
    "    return total_loss"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d34868a7",
   "metadata": {
    "papermill": {
     "duration": 0.024069,
     "end_time": "2021-11-06T11:46:35.167795",
     "exception": false,
     "start_time": "2021-11-06T11:46:35.143726",
     "status": "completed"
    },
    "tags": []
   },
   "source": [
    "#### Grouped Layerwise Learning Rate Decay"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "cc4cc995",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2021-11-06T11:46:35.221747Z",
     "iopub.status.busy": "2021-11-06T11:46:35.220837Z",
     "iopub.status.idle": "2021-11-06T11:46:35.224333Z",
     "shell.execute_reply": "2021-11-06T11:46:35.223818Z",
     "shell.execute_reply.started": "2021-11-06T03:25:12.678166Z"
    },
    "papermill": {
     "duration": 0.032375,
     "end_time": "2021-11-06T11:46:35.224501",
     "exception": false,
     "start_time": "2021-11-06T11:46:35.192126",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "# def get_optimizer_grouped_parameters(args, model):\n",
    "#     no_decay = [\"bias\", \"LayerNorm.weight\"]\n",
    "#     group1=['layer.0.','layer.1.','layer.2.','layer.3.']\n",
    "#     group2=['layer.4.','layer.5.','layer.6.','layer.7.']    \n",
    "#     group3=['layer.8.','layer.9.','layer.10.','layer.11.']\n",
    "#     group_all=['layer.0.','layer.1.','layer.2.','layer.3.','layer.4.','layer.5.','layer.6.','layer.7.','layer.8.','layer.9.','layer.10.','layer.11.']\n",
    "#     optimizer_grouped_parameters = [\n",
    "#         {'params': [p for n, p in model.xlm_roberta.named_parameters() if not any(nd in n for nd in no_decay) and not any(nd in n for nd in group_all)],'weight_decay': args.weight_decay},\n",
    "#         {'params': [p for n, p in model.xlm_roberta.named_parameters() if not any(nd in n for nd in no_decay) and any(nd in n for nd in group1)],'weight_decay': args.weight_decay, 'lr': args.learning_rate/2.6},\n",
    "#         {'params': [p for n, p in model.xlm_roberta.named_parameters() if not any(nd in n for nd in no_decay) and any(nd in n for nd in group2)],'weight_decay': args.weight_decay, 'lr': args.learning_rate},\n",
    "#         {'params': [p for n, p in model.xlm_roberta.named_parameters() if not any(nd in n for nd in no_decay) and any(nd in n for nd in group3)],'weight_decay': args.weight_decay, 'lr': args.learning_rate*2.6},\n",
    "#         {'params': [p for n, p in model.xlm_roberta.named_parameters() if any(nd in n for nd in no_decay) and not any(nd in n for nd in group_all)],'weight_decay': 0.0},\n",
    "#         {'params': [p for n, p in model.xlm_roberta.named_parameters() if any(nd in n for nd in no_decay) and any(nd in n for nd in group1)],'weight_decay': 0.0, 'lr': args.learning_rate/2.6},\n",
    "#         {'params': [p for n, p in model.xlm_roberta.named_parameters() if any(nd in n for nd in no_decay) and any(nd in n for nd in group2)],'weight_decay': 0.0, 'lr': args.learning_rate},\n",
    "#         {'params': [p for n, p in model.xlm_roberta.named_parameters() if any(nd in n for nd in no_decay) and any(nd in n for nd in group3)],'weight_decay': 0.0, 'lr': args.learning_rate*2.6},\n",
    "#         {'params': [p for n, p in model.named_parameters() if args.model_type not in n], 'lr':args.learning_rate*20, \"weight_decay\": 0.0},\n",
    "#     ]\n",
    "#     return optimizer_grouped_parameters"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0fc0d4e2",
   "metadata": {
    "papermill": {
     "duration": 0.02588,
     "end_time": "2021-11-06T11:46:35.275472",
     "exception": false,
     "start_time": "2021-11-06T11:46:35.249592",
     "status": "completed"
    },
    "tags": []
   },
   "source": [
    "#### Metric Logger"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "df818f8b",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2021-11-06T11:46:35.333772Z",
     "iopub.status.busy": "2021-11-06T11:46:35.332658Z",
     "iopub.status.idle": "2021-11-06T11:46:35.336138Z",
     "shell.execute_reply": "2021-11-06T11:46:35.336669Z",
     "shell.execute_reply.started": "2021-11-06T03:25:12.688732Z"
    },
    "papermill": {
     "duration": 0.036626,
     "end_time": "2021-11-06T11:46:35.336943",
     "exception": false,
     "start_time": "2021-11-06T11:46:35.300317",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "class AverageMeter(object):\n",
    "    def __init__(self):\n",
    "        self.reset()\n",
    "\n",
    "    def reset(self):\n",
    "        self.val = 0\n",
    "        self.avg = 0\n",
    "        self.sum = 0\n",
    "        self.count = 0\n",
    "        self.max = 0\n",
    "        self.min = 1e5\n",
    "\n",
    "    def update(self, val, n=1):\n",
    "        self.val = val\n",
    "        self.sum += val * n\n",
    "        self.count += n\n",
    "        self.avg = self.sum / self.count\n",
    "        if val > self.max:\n",
    "            self.max = val\n",
    "        if val < self.min:\n",
    "            self.min = val"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "538bd502",
   "metadata": {
    "papermill": {
     "duration": 0.025707,
     "end_time": "2021-11-06T11:46:35.388286",
     "exception": false,
     "start_time": "2021-11-06T11:46:35.362579",
     "status": "completed"
    },
    "tags": []
   },
   "source": [
    "#### Utilities"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "d4831c15",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2021-11-06T11:46:35.462852Z",
     "iopub.status.busy": "2021-11-06T11:46:35.461644Z",
     "iopub.status.idle": "2021-11-06T11:46:35.465017Z",
     "shell.execute_reply": "2021-11-06T11:46:35.465585Z",
     "shell.execute_reply.started": "2021-11-06T03:25:12.700630Z"
    },
    "papermill": {
     "duration": 0.051874,
     "end_time": "2021-11-06T11:46:35.465773",
     "exception": false,
     "start_time": "2021-11-06T11:46:35.413899",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "def make_model(args):\n",
    "    config = AutoConfig.from_pretrained(args.config_name)\n",
    "    tokenizer = AutoTokenizer.from_pretrained(args.tokenizer_name)\n",
    "    model = Model(args.model_name_or_path, config=config,layer_start=12,layer_weights=None)\n",
    "    #model = Model(args.model_name_or_path, config=config)\n",
    "    return config, tokenizer, model\n",
    "\n",
    "def make_optimizer(args, model):\n",
    "    named_parameters = list(model.named_parameters())   \n",
    "\n",
    "    roberta_parameters = named_parameters[:389]   \n",
    "    pooler_parameters = named_parameters[389:391] \n",
    "    qa_parameters = named_parameters[391:]\n",
    "    \n",
    "    parameters = []\n",
    "    \n",
    "    # increase lr every k layer\n",
    "    increase_lr_every_k_layer = 1\n",
    "    lrs = np.linspace(1, 5, 24 // increase_lr_every_k_layer)\n",
    "    for layer_num, (name, params) in enumerate(roberta_parameters):\n",
    "        weight_decay = 0.0 if \"bias\" in name else 0.01\n",
    "        splitted_name = name.split('.')\n",
    "        lr = args.learning_rate #Config.lr\n",
    "        if len(splitted_name) >= 4 and str.isdigit(splitted_name[3]):\n",
    "            layer_num = int(splitted_name[3])\n",
    "            lr = lrs[layer_num // increase_lr_every_k_layer] * lr\n",
    "\n",
    "        parameters.append({\"params\": params,\n",
    "                           \"weight_decay\": weight_decay,\n",
    "                           \"lr\": lr})\n",
    "    \n",
    "    default_lr = 1e-3 #default LR for AdamW\n",
    "    for layer_num, (name,params) in enumerate(qa_parameters):\n",
    "        weight_decay = 0.0 if \"bias\" in name else 0.01\n",
    "        parameters.append({\"params\": params,\n",
    "                           \"weight_decay\": weight_decay,\n",
    "                           \"lr\": default_lr})\n",
    "    \n",
    "    for layer_num, (name,params) in enumerate(pooler_parameters):\n",
    "        weight_decay = 0.0 if \"bias\" in name else 0.01\n",
    "        parameters.append({\"params\": params,\n",
    "                           \"weight_decay\": weight_decay,\n",
    "                           \"lr\": default_lr})\n",
    "\n",
    "    return AdamW(parameters)\n",
    "\n",
    "\n",
    "def make_scheduler(\n",
    "    args, optimizer, \n",
    "    num_warmup_steps, \n",
    "    num_training_steps\n",
    "):\n",
    "    if args.decay_name == \"cosine-warmup\":\n",
    "        scheduler = get_cosine_schedule_with_warmup(\n",
    "            optimizer,\n",
    "            num_warmup_steps=num_warmup_steps,\n",
    "            num_training_steps=num_training_steps\n",
    "        )\n",
    "    else:\n",
    "        scheduler = get_linear_schedule_with_warmup(\n",
    "            optimizer,\n",
    "            num_warmup_steps=num_warmup_steps,\n",
    "            num_training_steps=num_training_steps\n",
    "        )\n",
    "    return scheduler    \n",
    "\n",
    "\n",
    "def make_loader(\n",
    "    args, data, \n",
    "    tokenizer, fold\n",
    "):\n",
    "    train_set, valid_set = data[data['kfold']!=fold], data[data['kfold']==fold].reset_index(drop=True)\n",
    "    \n",
    "    train_features, valid_features = [[] for _ in range(2)]\n",
    "    for i, row in train_set.iterrows():\n",
    "        train_features += prepare_train_features(args, row, tokenizer)\n",
    "    for i, row in valid_set.iterrows():\n",
    "        valid_features += prepare_train_features(args, row, tokenizer)\n",
    "\n",
    "    ## Weighted sampler\n",
    "    hindi_tamil_count = [] \n",
    "    for i, f in enumerate(train_features):\n",
    "        hindi_tamil_count.append(train_features[i]['hindi_tamil'])      \n",
    "    class_sample_count = pd.Series(hindi_tamil_count).value_counts().values\n",
    "    weight = 1. / class_sample_count\n",
    "    samples_weight = np.array([weight[t] for t in hindi_tamil_count]) \n",
    "    samples_weight = torch.from_numpy(samples_weight)\n",
    "    wsampler = torch.utils.data.sampler.WeightedRandomSampler(samples_weight.type('torch.DoubleTensor'), len(samples_weight))\n",
    "\n",
    "    train_dataset = DatasetRetriever(train_features, mode=\"train\")\n",
    "    valid_dataset = DatasetRetriever(valid_features, mode=\"valid\")\n",
    "    print(f\"Num examples Train= {len(train_dataset)}, Num examples Valid={len(valid_dataset)}\")\n",
    "    \n",
    "    train_sampler = RandomSampler(train_dataset)\n",
    "    valid_sampler = SequentialSampler(valid_dataset)\n",
    "\n",
    "    train_dataloader = DataLoader(\n",
    "        train_dataset,\n",
    "        batch_size=args.train_batch_size,\n",
    "        sampler=train_sampler, #wsampler\n",
    "        num_workers=optimal_num_of_loader_workers(),\n",
    "        pin_memory=True,\n",
    "        drop_last=False \n",
    "    )\n",
    "\n",
    "    valid_dataloader = DataLoader(\n",
    "        valid_dataset,\n",
    "        batch_size=args.eval_batch_size, \n",
    "        sampler=valid_sampler,\n",
    "        num_workers=optimal_num_of_loader_workers(),\n",
    "        pin_memory=True, \n",
    "        drop_last=False\n",
    "    )\n",
    "\n",
    "    return train_dataloader, valid_dataloader, valid_features, valid_set"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "240df91b",
   "metadata": {
    "papermill": {
     "duration": 0.025535,
     "end_time": "2021-11-06T11:46:35.517458",
     "exception": false,
     "start_time": "2021-11-06T11:46:35.491923",
     "status": "completed"
    },
    "tags": []
   },
   "source": [
    "#### Trainer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "100556d4",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2021-11-06T11:46:35.581061Z",
     "iopub.status.busy": "2021-11-06T11:46:35.579952Z",
     "iopub.status.idle": "2021-11-06T11:46:35.583234Z",
     "shell.execute_reply": "2021-11-06T11:46:35.583694Z",
     "shell.execute_reply.started": "2021-11-06T03:25:12.725143Z"
    },
    "papermill": {
     "duration": 0.041328,
     "end_time": "2021-11-06T11:46:35.583834",
     "exception": false,
     "start_time": "2021-11-06T11:46:35.542506",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "class Trainer:\n",
    "    def __init__(\n",
    "        self, model, tokenizer, \n",
    "        optimizer, scheduler\n",
    "    ):\n",
    "        self.model = model\n",
    "        self.tokenizer = tokenizer\n",
    "        self.optimizer = optimizer\n",
    "        self.scheduler = scheduler\n",
    "\n",
    "    def train(\n",
    "        self, args, \n",
    "        train_dataloader, \n",
    "        epoch, result_dict\n",
    "    ):\n",
    "        count = 0\n",
    "        losses = AverageMeter()\n",
    "        \n",
    "        self.model.zero_grad()\n",
    "        self.model.train()\n",
    "        \n",
    "        fix_all_seeds(args.seed)\n",
    "        \n",
    "        for batch_idx, batch_data in enumerate(train_dataloader):\n",
    "            input_ids, attention_mask, targets_start, targets_end = \\\n",
    "                batch_data['input_ids'], batch_data['attention_mask'], \\\n",
    "                    batch_data['start_position'], batch_data['end_position']\n",
    "            \n",
    "            input_ids, attention_mask, targets_start, targets_end = \\\n",
    "                input_ids.cuda(), attention_mask.cuda(), targets_start.cuda(), targets_end.cuda()\n",
    "\n",
    "            outputs_start, outputs_end = self.model(\n",
    "                input_ids=input_ids,\n",
    "                attention_mask=attention_mask,\n",
    "            )\n",
    "            \n",
    "            loss = loss_fn((outputs_start, outputs_end), (targets_start, targets_end))\n",
    "            loss = loss / args.gradient_accumulation_steps\n",
    "\n",
    "            if args.fp16:\n",
    "                with amp.scale_loss(loss, self.optimizer) as scaled_loss:\n",
    "                    scaled_loss.backward()\n",
    "            else:\n",
    "                loss.backward()\n",
    "\n",
    "            count += input_ids.size(0)\n",
    "            losses.update(loss.item(), input_ids.size(0))\n",
    "\n",
    "            # if args.fp16:\n",
    "            #     torch.nn.utils.clip_grad_norm_(amp.master_params(self.optimizer), args.max_grad_norm)\n",
    "            # else:\n",
    "            #     torch.nn.utils.clip_grad_norm_(self.model.parameters(), args.max_grad_norm)\n",
    "\n",
    "            if batch_idx % args.gradient_accumulation_steps == 0 or batch_idx == len(train_dataloader) - 1:\n",
    "                self.optimizer.step()\n",
    "                self.scheduler.step()\n",
    "                self.optimizer.zero_grad()\n",
    "\n",
    "            if (batch_idx % args.logging_steps == 0) or (batch_idx+1)==len(train_dataloader):\n",
    "                _s = str(len(str(len(train_dataloader.sampler))))\n",
    "                ret = [\n",
    "                    ('Epoch: {:0>2} [{: >' + _s + '}/{} ({: >3.0f}%)]').format(epoch, count, len(train_dataloader.sampler), 100 * count / len(train_dataloader.sampler)),\n",
    "                    'Train Loss: {: >4.5f}'.format(losses.avg),\n",
    "                ]\n",
    "                print(', '.join(ret))\n",
    "\n",
    "        result_dict['train_loss'].append(losses.avg)\n",
    "        return result_dict"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ac61deba",
   "metadata": {
    "papermill": {
     "duration": 0.024462,
     "end_time": "2021-11-06T11:46:35.632284",
     "exception": false,
     "start_time": "2021-11-06T11:46:35.607822",
     "status": "completed"
    },
    "tags": []
   },
   "source": [
    "#### Evaluator"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "46f29d91",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2021-11-06T11:46:35.692072Z",
     "iopub.status.busy": "2021-11-06T11:46:35.690984Z",
     "iopub.status.idle": "2021-11-06T11:46:35.693881Z",
     "shell.execute_reply": "2021-11-06T11:46:35.694331Z",
     "shell.execute_reply.started": "2021-11-06T03:25:12.742838Z"
    },
    "papermill": {
     "duration": 0.037988,
     "end_time": "2021-11-06T11:46:35.694487",
     "exception": false,
     "start_time": "2021-11-06T11:46:35.656499",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "class Evaluator:\n",
    "    def __init__(self, model):\n",
    "        self.model = model\n",
    "    \n",
    "    def save(self, result, output_dir):\n",
    "        with open(f'{output_dir}/result_dict.json', 'w') as f:\n",
    "            f.write(json.dumps(result, sort_keys=True, indent=4, ensure_ascii=False))\n",
    "\n",
    "    def evaluate(self, valid_dataloader, epoch, result_dict):\n",
    "        losses = AverageMeter()\n",
    "        all_outputs_start, all_outputs_end = [], []\n",
    "        for batch_idx, batch_data in enumerate(valid_dataloader):\n",
    "            self.model = self.model.eval()\n",
    "            input_ids, attention_mask, targets_start, targets_end = \\\n",
    "                batch_data['input_ids'], batch_data['attention_mask'], \\\n",
    "                    batch_data['start_position'], batch_data['end_position']\n",
    "            \n",
    "            input_ids, attention_mask, targets_start, targets_end = \\\n",
    "                input_ids.cuda(), attention_mask.cuda(), targets_start.cuda(), targets_end.cuda()\n",
    "            \n",
    "            with torch.no_grad():            \n",
    "                outputs_start, outputs_end = self.model(\n",
    "                    input_ids=input_ids,\n",
    "                    attention_mask=attention_mask,\n",
    "                )\n",
    "                all_outputs_start.append(outputs_start.cpu().numpy().tolist())\n",
    "                all_outputs_end.append(outputs_end.cpu().numpy().tolist())\n",
    "        \n",
    "                loss = loss_fn((outputs_start, outputs_end), (targets_start, targets_end))\n",
    "                losses.update(loss.item(), input_ids.size(0))\n",
    "                \n",
    "        all_outputs_start = np.vstack(all_outputs_start)\n",
    "        all_outputs_end = np.vstack(all_outputs_end)\n",
    "\n",
    "        print('----Validation Results Summary----')\n",
    "        print('Epoch: [{}] Valid Loss: {: >4.5f}'.format(epoch, losses.avg))\n",
    "        result_dict['val_loss'].append(losses.avg)        \n",
    "        return result_dict, all_outputs_start, all_outputs_end\n",
    "    \n",
    "    "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c14b2500",
   "metadata": {
    "papermill": {
     "duration": 0.02364,
     "end_time": "2021-11-06T11:46:35.740755",
     "exception": false,
     "start_time": "2021-11-06T11:46:35.717115",
     "status": "completed"
    },
    "tags": []
   },
   "source": [
    "#### Initialize Training"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "774c5f8a",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2021-11-06T11:46:35.798766Z",
     "iopub.status.busy": "2021-11-06T11:46:35.797638Z",
     "iopub.status.idle": "2021-11-06T11:46:35.800991Z",
     "shell.execute_reply": "2021-11-06T11:46:35.801518Z",
     "shell.execute_reply.started": "2021-11-06T03:25:12.758543Z"
    },
    "papermill": {
     "duration": 0.037298,
     "end_time": "2021-11-06T11:46:35.801665",
     "exception": false,
     "start_time": "2021-11-06T11:46:35.764367",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "def init_training(args, data, fold):\n",
    "    fix_all_seeds(args.seed)\n",
    "    \n",
    "    if not os.path.exists(args.output_dir):\n",
    "        os.makedirs(args.output_dir)\n",
    "    \n",
    "    # model\n",
    "    model_config, tokenizer, model = make_model(args)\n",
    "    if torch.cuda.device_count() >= 1:\n",
    "        print('Model pushed to {} GPU(s), type {}.'.format(\n",
    "            torch.cuda.device_count(), \n",
    "            torch.cuda.get_device_name(0))\n",
    "        )\n",
    "        model = model.cuda() \n",
    "    else:\n",
    "        raise ValueError('CPU training is not supported')\n",
    "    \n",
    "    # data loaders\n",
    "    train_dataloader, valid_dataloader, valid_features, valid_set = make_loader(args, data, tokenizer, fold)\n",
    "\n",
    "    # optimizer\n",
    "    optimizer = make_optimizer(args, model)\n",
    "\n",
    "    # scheduler\n",
    "    num_training_steps = math.ceil(len(train_dataloader) / args.gradient_accumulation_steps) * args.epochs\n",
    "    if args.warmup_ratio > 0:\n",
    "        num_warmup_steps = int(args.warmup_ratio * num_training_steps)\n",
    "    else:\n",
    "        num_warmup_steps = 0\n",
    "    print(f\"Total Training Steps: {num_training_steps}, Total Warmup Steps: {num_warmup_steps}\")\n",
    "    scheduler = make_scheduler(args, optimizer, num_warmup_steps, num_training_steps)\n",
    "\n",
    "    # mixed precision training with NVIDIA Apex\n",
    "    if args.fp16:\n",
    "        model, optimizer = amp.initialize(model, optimizer, opt_level=args.fp16_opt_level)\n",
    "    \n",
    "    result_dict = {\n",
    "        'epoch':[], \n",
    "        'train_loss': [], \n",
    "        'val_loss' : [], \n",
    "        'best_val_loss': np.inf\n",
    "    }\n",
    "\n",
    "    return (\n",
    "        model, model_config, tokenizer, optimizer, scheduler, \n",
    "        train_dataloader, valid_dataloader, result_dict, valid_features, valid_set\n",
    "    )"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b6f3d7b8",
   "metadata": {
    "papermill": {
     "duration": 0.023801,
     "end_time": "2021-11-06T11:46:35.850118",
     "exception": false,
     "start_time": "2021-11-06T11:46:35.826317",
     "status": "completed"
    },
    "tags": []
   },
   "source": [
    "#### Validation Jaccard"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "3b1fd07e",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2021-11-06T11:46:35.914018Z",
     "iopub.status.busy": "2021-11-06T11:46:35.911491Z",
     "iopub.status.idle": "2021-11-06T11:46:35.917888Z",
     "shell.execute_reply": "2021-11-06T11:46:35.917201Z",
     "shell.execute_reply.started": "2021-11-06T03:25:12.774921Z"
    },
    "papermill": {
     "duration": 0.045352,
     "end_time": "2021-11-06T11:46:35.918021",
     "exception": false,
     "start_time": "2021-11-06T11:46:35.872669",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "# Ref: https://www.kaggle.com/rhtsingh/chaii-qa-5-fold-xlmroberta-torch-infer\n",
    "import collections\n",
    "\n",
    "def postprocess_qa_predictions(examples, features1, raw_predictions, tokenizer, n_best_size = 20, max_answer_length = 30):\n",
    "    features = features1\n",
    "    all_start_logits, all_end_logits = raw_predictions\n",
    "    \n",
    "    example_id_to_index = {k: i for i, k in enumerate(examples[\"id\"])}\n",
    "    features_per_example = collections.defaultdict(list)\n",
    "    for i, feature in enumerate(features):\n",
    "        features_per_example[example_id_to_index[feature[\"example_id\"]]].append(i)\n",
    "\n",
    "    predictions = collections.OrderedDict()\n",
    "\n",
    "    print(f\"Post-processing {len(examples)} example predictions split into {len(features)} features.\")\n",
    "\n",
    "    for example_index, example in examples.iterrows():\n",
    "        feature_indices = features_per_example[example_index]\n",
    "        #print(example['id'],example_index,feature_indices)\n",
    "        min_null_score = None\n",
    "        valid_answers = []\n",
    "        \n",
    "        context = example[\"context\"]\n",
    "        for feature_index in feature_indices:\n",
    "            start_logits = all_start_logits[feature_index]\n",
    "            end_logits = all_end_logits[feature_index]\n",
    "\n",
    "            sequence_ids = features[feature_index][\"sequence_ids\"]\n",
    "            context_index = 1\n",
    "\n",
    "            offset_mapping = [\n",
    "                (o if sequence_ids[k] == context_index else None)\n",
    "                for k, o in enumerate(features[feature_index][\"offset_mapping\"])\n",
    "            ]    \n",
    "        \n",
    "            cls_index = features[feature_index][\"input_ids\"].index(tokenizer.cls_token_id)\n",
    "            feature_null_score = start_logits[cls_index] + end_logits[cls_index]\n",
    "            if min_null_score is None or min_null_score < feature_null_score:\n",
    "                min_null_score = feature_null_score\n",
    "\n",
    "            start_indexes = np.argsort(start_logits)[-1 : -n_best_size - 1 : -1].tolist()\n",
    "            end_indexes = np.argsort(end_logits)[-1 : -n_best_size - 1 : -1].tolist()\n",
    "            for start_index in start_indexes:\n",
    "                for end_index in end_indexes:\n",
    "                    if (\n",
    "                        start_index >= len(offset_mapping)\n",
    "                        or end_index >= len(offset_mapping)\n",
    "                        or offset_mapping[start_index] is None\n",
    "                        or offset_mapping[end_index] is None\n",
    "                    ):\n",
    "                        continue\n",
    "                    # Don't consider answers with a length that is either < 0 or > max_answer_length.\n",
    "                    if end_index < start_index or end_index - start_index + 1 > max_answer_length:\n",
    "                        continue\n",
    "\n",
    "                    start_char = offset_mapping[start_index][0]\n",
    "                    end_char = offset_mapping[end_index][1]\n",
    "                    valid_answers.append(\n",
    "                        {\n",
    "                            \"score\": start_logits[start_index] + end_logits[end_index],\n",
    "                            \"text\": context[start_char: end_char]\n",
    "                        }\n",
    "                    )\n",
    "        \n",
    "        if len(valid_answers) > 0:\n",
    "            best_answer = sorted(valid_answers, key=lambda x: x[\"score\"], reverse=True)[0]\n",
    "        else:\n",
    "            best_answer = {\"text\": \"\", \"score\": 0.0}\n",
    "        \n",
    "        predictions[example[\"id\"]] = best_answer[\"text\"]\n",
    "        \n",
    "        \n",
    "    return predictions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "e5eb327f",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2021-11-06T11:46:35.972496Z",
     "iopub.status.busy": "2021-11-06T11:46:35.971835Z",
     "iopub.status.idle": "2021-11-06T11:46:35.976173Z",
     "shell.execute_reply": "2021-11-06T11:46:35.975628Z",
     "shell.execute_reply.started": "2021-11-06T03:25:12.796593Z"
    },
    "papermill": {
     "duration": 0.033634,
     "end_time": "2021-11-06T11:46:35.976392",
     "exception": false,
     "start_time": "2021-11-06T11:46:35.942758",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "def jaccard(str1, str2): \n",
    "    a = set(str1.lower().split()) \n",
    "    b = set(str2.lower().split())\n",
    "    c = a.intersection(b)\n",
    "    return float(len(c)) / (len(a) + len(b) - len(c))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a6b82c18",
   "metadata": {
    "papermill": {
     "duration": 0.023497,
     "end_time": "2021-11-06T11:46:36.023222",
     "exception": false,
     "start_time": "2021-11-06T11:46:35.999725",
     "status": "completed"
    },
    "tags": []
   },
   "source": [
    "#### Run"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "415afbac",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2021-11-06T11:46:36.084659Z",
     "iopub.status.busy": "2021-11-06T11:46:36.083769Z",
     "iopub.status.idle": "2021-11-06T11:46:36.086837Z",
     "shell.execute_reply": "2021-11-06T11:46:36.087465Z",
     "shell.execute_reply.started": "2021-11-06T03:25:12.808098Z"
    },
    "papermill": {
     "duration": 0.041433,
     "end_time": "2021-11-06T11:46:36.087654",
     "exception": false,
     "start_time": "2021-11-06T11:46:36.046221",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "all_jacard_scores = []\n",
    "\n",
    "def run(data, fold):\n",
    "    args = Config()\n",
    "    model, model_config, tokenizer, optimizer, scheduler, train_dataloader, \\\n",
    "        valid_dataloader, result_dict, valid_features, valid_set = init_training(args, data, fold)\n",
    "    \n",
    "    trainer = Trainer(model, tokenizer, optimizer, scheduler)\n",
    "    evaluator = Evaluator(model)\n",
    "\n",
    "    train_time_list = []\n",
    "    valid_time_list = []\n",
    "\n",
    "    for epoch in range(args.epochs):\n",
    "        result_dict['epoch'].append(epoch)\n",
    "\n",
    "        # Train\n",
    "        torch.cuda.synchronize()\n",
    "        tic1 = time.time()\n",
    "        result_dict = trainer.train(\n",
    "            args, train_dataloader, \n",
    "            epoch, result_dict\n",
    "        )\n",
    "        torch.cuda.synchronize()\n",
    "        tic2 = time.time() \n",
    "        train_time_list.append(tic2 - tic1)\n",
    "        # Evaluate\n",
    "        torch.cuda.synchronize()\n",
    "        tic3 = time.time()\n",
    "        result_dict, all_outputs_start, all_outputs_end = evaluator.evaluate(\n",
    "            valid_dataloader, epoch, result_dict\n",
    "        )\n",
    "        torch.cuda.synchronize()\n",
    "#         # Get valid jaccard score\n",
    "        valid_features1 = valid_features.copy()\n",
    "        valid_preds = postprocess_qa_predictions(valid_set, valid_features1, (all_outputs_start, all_outputs_end), tokenizer)\n",
    "        valid_set['PredictionString'] = valid_set['id'].map(valid_preds)\n",
    "        valid_set['jaccard'] = valid_set[['answer_text','PredictionString']].apply(lambda x: jaccard(x[0],x[1]), axis=1)\n",
    "        print(\"valid jaccard: \",np.mean(valid_set.jaccard))\n",
    "        all_jacard_scores.append(np.mean(valid_set.jaccard))\n",
    "        \n",
    "        tic4 = time.time() \n",
    "        valid_time_list.append(tic4 - tic3)\n",
    "            \n",
    "        output_dir = os.path.join(args.output_dir, f\"checkpoint-fold-{fold}-epoch-{epoch}\")\n",
    "        os.makedirs(output_dir, exist_ok=True)\n",
    "        if result_dict['val_loss'][-1] < result_dict['best_val_loss']:\n",
    "            print(\"{} Epoch, Best epoch was updated! Valid Loss: {: >4.5f}\".format(epoch, result_dict['val_loss'][-1]))\n",
    "            result_dict[\"best_val_loss\"] = result_dict['val_loss'][-1]        \n",
    "            \n",
    "#             os.makedirs(output_dir, exist_ok=True)\n",
    "            torch.save(model.state_dict(), f\"{output_dir}/pytorch_model.bin\")\n",
    "            model_config.save_pretrained(output_dir)\n",
    "            tokenizer.save_pretrained(output_dir)\n",
    "            print(f\"Saving model checkpoint to {output_dir}.\")\n",
    "            \n",
    "        print()\n",
    "\n",
    "    evaluator.save(result_dict, output_dir)\n",
    "    \n",
    "    print(f\"Total Training Time: {np.sum(train_time_list)}secs, Average Training Time per Epoch: {np.mean(train_time_list)}secs.\")\n",
    "    print(f\"Total Validation Time: {np.sum(valid_time_list)}secs, Average Validation Time per Epoch: {np.mean(valid_time_list)}secs.\")\n",
    "    \n",
    "    #del trainer, evaluator\n",
    "    #del model, model_config, tokenizer\n",
    "    #del optimizer, scheduler\n",
    "    #del train_dataloader, valid_dataloader, result_dict"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "0d9e4e57",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2021-11-06T11:46:36.142086Z",
     "iopub.status.busy": "2021-11-06T11:46:36.141215Z",
     "iopub.status.idle": "2021-11-06T16:11:19.218231Z",
     "shell.execute_reply": "2021-11-06T16:11:19.219081Z",
     "shell.execute_reply.started": "2021-11-06T03:25:12.826669Z"
    },
    "papermill": {
     "duration": 15883.107826,
     "end_time": "2021-11-06T16:11:19.219374",
     "exception": false,
     "start_time": "2021-11-06T11:46:36.111548",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "\n",
      "--------------------------------------------------\n",
      "FOLD: 0\n",
      "--------------------------------------------------\n",
      "Model pushed to 1 GPU(s), type Tesla P100-PCIE-16GB.\n",
      "Num examples Train= 19985, Num examples Valid=3084\n",
      "Total Training Steps: 2499, Total Warmup Steps: 249\n",
      "Epoch: 00 [    4/19985 (  0%)], Train Loss: 3.06158\n",
      "Epoch: 00 [   44/19985 (  0%)], Train Loss: 3.08718\n",
      "Epoch: 00 [   84/19985 (  0%)], Train Loss: 3.03492\n",
      "Epoch: 00 [  124/19985 (  1%)], Train Loss: 2.99473\n",
      "Epoch: 00 [  164/19985 (  1%)], Train Loss: 2.92814\n",
      "Epoch: 00 [  204/19985 (  1%)], Train Loss: 2.83910\n",
      "Epoch: 00 [  244/19985 (  1%)], Train Loss: 2.73494\n",
      "Epoch: 00 [  284/19985 (  1%)], Train Loss: 2.60728\n",
      "Epoch: 00 [  324/19985 (  2%)], Train Loss: 2.46037\n",
      "Epoch: 00 [  364/19985 (  2%)], Train Loss: 2.32434\n",
      "Epoch: 00 [  404/19985 (  2%)], Train Loss: 2.19196\n",
      "Epoch: 00 [  444/19985 (  2%)], Train Loss: 2.05678\n",
      "Epoch: 00 [  484/19985 (  2%)], Train Loss: 1.95848\n",
      "Epoch: 00 [  524/19985 (  3%)], Train Loss: 1.83783\n",
      "Epoch: 00 [  564/19985 (  3%)], Train Loss: 1.75830\n",
      "Epoch: 00 [  604/19985 (  3%)], Train Loss: 1.67003\n",
      "Epoch: 00 [  644/19985 (  3%)], Train Loss: 1.59841\n",
      "Epoch: 00 [  684/19985 (  3%)], Train Loss: 1.52726\n",
      "Epoch: 00 [  724/19985 (  4%)], Train Loss: 1.47200\n",
      "Epoch: 00 [  764/19985 (  4%)], Train Loss: 1.42208\n",
      "Epoch: 00 [  804/19985 (  4%)], Train Loss: 1.38134\n",
      "Epoch: 00 [  844/19985 (  4%)], Train Loss: 1.34157\n",
      "Epoch: 00 [  884/19985 (  4%)], Train Loss: 1.30608\n",
      "Epoch: 00 [  924/19985 (  5%)], Train Loss: 1.26462\n",
      "Epoch: 00 [  964/19985 (  5%)], Train Loss: 1.22548\n",
      "Epoch: 00 [ 1004/19985 (  5%)], Train Loss: 1.19740\n",
      "Epoch: 00 [ 1044/19985 (  5%)], Train Loss: 1.16945\n",
      "Epoch: 00 [ 1084/19985 (  5%)], Train Loss: 1.14281\n",
      "Epoch: 00 [ 1124/19985 (  6%)], Train Loss: 1.12533\n",
      "Epoch: 00 [ 1164/19985 (  6%)], Train Loss: 1.10773\n",
      "Epoch: 00 [ 1204/19985 (  6%)], Train Loss: 1.09288\n",
      "Epoch: 00 [ 1244/19985 (  6%)], Train Loss: 1.06997\n",
      "Epoch: 00 [ 1284/19985 (  6%)], Train Loss: 1.04955\n",
      "Epoch: 00 [ 1324/19985 (  7%)], Train Loss: 1.02849\n",
      "Epoch: 00 [ 1364/19985 (  7%)], Train Loss: 1.02118\n",
      "Epoch: 00 [ 1404/19985 (  7%)], Train Loss: 1.00969\n",
      "Epoch: 00 [ 1444/19985 (  7%)], Train Loss: 0.99118\n",
      "Epoch: 00 [ 1484/19985 (  7%)], Train Loss: 0.97718\n",
      "Epoch: 00 [ 1524/19985 (  8%)], Train Loss: 0.96074\n",
      "Epoch: 00 [ 1564/19985 (  8%)], Train Loss: 0.94638\n",
      "Epoch: 00 [ 1604/19985 (  8%)], Train Loss: 0.93356\n",
      "Epoch: 00 [ 1644/19985 (  8%)], Train Loss: 0.91989\n",
      "Epoch: 00 [ 1684/19985 (  8%)], Train Loss: 0.91487\n",
      "Epoch: 00 [ 1724/19985 (  9%)], Train Loss: 0.90318\n",
      "Epoch: 00 [ 1764/19985 (  9%)], Train Loss: 0.89313\n",
      "Epoch: 00 [ 1804/19985 (  9%)], Train Loss: 0.88391\n",
      "Epoch: 00 [ 1844/19985 (  9%)], Train Loss: 0.87778\n",
      "Epoch: 00 [ 1884/19985 (  9%)], Train Loss: 0.86738\n",
      "Epoch: 00 [ 1924/19985 ( 10%)], Train Loss: 0.85637\n",
      "Epoch: 00 [ 1964/19985 ( 10%)], Train Loss: 0.85328\n",
      "Epoch: 00 [ 2004/19985 ( 10%)], Train Loss: 0.84618\n",
      "Epoch: 00 [ 2044/19985 ( 10%)], Train Loss: 0.83490\n",
      "Epoch: 00 [ 2084/19985 ( 10%)], Train Loss: 0.83267\n",
      "Epoch: 00 [ 2124/19985 ( 11%)], Train Loss: 0.82835\n",
      "Epoch: 00 [ 2164/19985 ( 11%)], Train Loss: 0.82121\n",
      "Epoch: 00 [ 2204/19985 ( 11%)], Train Loss: 0.81604\n",
      "Epoch: 00 [ 2244/19985 ( 11%)], Train Loss: 0.80959\n",
      "Epoch: 00 [ 2284/19985 ( 11%)], Train Loss: 0.80160\n",
      "Epoch: 00 [ 2324/19985 ( 12%)], Train Loss: 0.79683\n",
      "Epoch: 00 [ 2364/19985 ( 12%)], Train Loss: 0.79354\n",
      "Epoch: 00 [ 2404/19985 ( 12%)], Train Loss: 0.78655\n",
      "Epoch: 00 [ 2444/19985 ( 12%)], Train Loss: 0.78076\n",
      "Epoch: 00 [ 2484/19985 ( 12%)], Train Loss: 0.77730\n",
      "Epoch: 00 [ 2524/19985 ( 13%)], Train Loss: 0.77519\n",
      "Epoch: 00 [ 2564/19985 ( 13%)], Train Loss: 0.77250\n",
      "Epoch: 00 [ 2604/19985 ( 13%)], Train Loss: 0.76768\n",
      "Epoch: 00 [ 2644/19985 ( 13%)], Train Loss: 0.76381\n",
      "Epoch: 00 [ 2684/19985 ( 13%)], Train Loss: 0.75755\n",
      "Epoch: 00 [ 2724/19985 ( 14%)], Train Loss: 0.75248\n",
      "Epoch: 00 [ 2764/19985 ( 14%)], Train Loss: 0.75079\n",
      "Epoch: 00 [ 2804/19985 ( 14%)], Train Loss: 0.74667\n",
      "Epoch: 00 [ 2844/19985 ( 14%)], Train Loss: 0.74525\n",
      "Epoch: 00 [ 2884/19985 ( 14%)], Train Loss: 0.74576\n",
      "Epoch: 00 [ 2924/19985 ( 15%)], Train Loss: 0.74156\n",
      "Epoch: 00 [ 2964/19985 ( 15%)], Train Loss: 0.73978\n",
      "Epoch: 00 [ 3004/19985 ( 15%)], Train Loss: 0.73604\n",
      "Epoch: 00 [ 3044/19985 ( 15%)], Train Loss: 0.73270\n",
      "Epoch: 00 [ 3084/19985 ( 15%)], Train Loss: 0.72845\n",
      "Epoch: 00 [ 3124/19985 ( 16%)], Train Loss: 0.72489\n",
      "Epoch: 00 [ 3164/19985 ( 16%)], Train Loss: 0.72084\n",
      "Epoch: 00 [ 3204/19985 ( 16%)], Train Loss: 0.71631\n",
      "Epoch: 00 [ 3244/19985 ( 16%)], Train Loss: 0.71800\n",
      "Epoch: 00 [ 3284/19985 ( 16%)], Train Loss: 0.71626\n",
      "Epoch: 00 [ 3324/19985 ( 17%)], Train Loss: 0.70970\n",
      "Epoch: 00 [ 3364/19985 ( 17%)], Train Loss: 0.70580\n",
      "Epoch: 00 [ 3404/19985 ( 17%)], Train Loss: 0.70092\n",
      "Epoch: 00 [ 3444/19985 ( 17%)], Train Loss: 0.69718\n",
      "Epoch: 00 [ 3484/19985 ( 17%)], Train Loss: 0.69574\n",
      "Epoch: 00 [ 3524/19985 ( 18%)], Train Loss: 0.69559\n",
      "Epoch: 00 [ 3564/19985 ( 18%)], Train Loss: 0.69259\n",
      "Epoch: 00 [ 3604/19985 ( 18%)], Train Loss: 0.69136\n",
      "Epoch: 00 [ 3644/19985 ( 18%)], Train Loss: 0.68632\n",
      "Epoch: 00 [ 3684/19985 ( 18%)], Train Loss: 0.68241\n",
      "Epoch: 00 [ 3724/19985 ( 19%)], Train Loss: 0.67781\n",
      "Epoch: 00 [ 3764/19985 ( 19%)], Train Loss: 0.67531\n",
      "Epoch: 00 [ 3804/19985 ( 19%)], Train Loss: 0.67265\n",
      "Epoch: 00 [ 3844/19985 ( 19%)], Train Loss: 0.67206\n",
      "Epoch: 00 [ 3884/19985 ( 19%)], Train Loss: 0.67195\n",
      "Epoch: 00 [ 3924/19985 ( 20%)], Train Loss: 0.66818\n",
      "Epoch: 00 [ 3964/19985 ( 20%)], Train Loss: 0.66747\n",
      "Epoch: 00 [ 4004/19985 ( 20%)], Train Loss: 0.66493\n",
      "Epoch: 00 [ 4044/19985 ( 20%)], Train Loss: 0.66330\n",
      "Epoch: 00 [ 4084/19985 ( 20%)], Train Loss: 0.65854\n",
      "Epoch: 00 [ 4124/19985 ( 21%)], Train Loss: 0.65542\n",
      "Epoch: 00 [ 4164/19985 ( 21%)], Train Loss: 0.65292\n",
      "Epoch: 00 [ 4204/19985 ( 21%)], Train Loss: 0.65039\n",
      "Epoch: 00 [ 4244/19985 ( 21%)], Train Loss: 0.65030\n",
      "Epoch: 00 [ 4284/19985 ( 21%)], Train Loss: 0.64655\n",
      "Epoch: 00 [ 4324/19985 ( 22%)], Train Loss: 0.64414\n",
      "Epoch: 00 [ 4364/19985 ( 22%)], Train Loss: 0.64388\n",
      "Epoch: 00 [ 4404/19985 ( 22%)], Train Loss: 0.64035\n",
      "Epoch: 00 [ 4444/19985 ( 22%)], Train Loss: 0.63786\n",
      "Epoch: 00 [ 4484/19985 ( 22%)], Train Loss: 0.63642\n",
      "Epoch: 00 [ 4524/19985 ( 23%)], Train Loss: 0.63324\n",
      "Epoch: 00 [ 4564/19985 ( 23%)], Train Loss: 0.63220\n",
      "Epoch: 00 [ 4604/19985 ( 23%)], Train Loss: 0.63055\n",
      "Epoch: 00 [ 4644/19985 ( 23%)], Train Loss: 0.62745\n",
      "Epoch: 00 [ 4684/19985 ( 23%)], Train Loss: 0.62373\n",
      "Epoch: 00 [ 4724/19985 ( 24%)], Train Loss: 0.62338\n",
      "Epoch: 00 [ 4764/19985 ( 24%)], Train Loss: 0.62144\n",
      "Epoch: 00 [ 4804/19985 ( 24%)], Train Loss: 0.61843\n",
      "Epoch: 00 [ 4844/19985 ( 24%)], Train Loss: 0.61637\n",
      "Epoch: 00 [ 4884/19985 ( 24%)], Train Loss: 0.61416\n",
      "Epoch: 00 [ 4924/19985 ( 25%)], Train Loss: 0.61352\n",
      "Epoch: 00 [ 4964/19985 ( 25%)], Train Loss: 0.61136\n",
      "Epoch: 00 [ 5004/19985 ( 25%)], Train Loss: 0.60857\n",
      "Epoch: 00 [ 5044/19985 ( 25%)], Train Loss: 0.60766\n",
      "Epoch: 00 [ 5084/19985 ( 25%)], Train Loss: 0.60500\n",
      "Epoch: 00 [ 5124/19985 ( 26%)], Train Loss: 0.60302\n",
      "Epoch: 00 [ 5164/19985 ( 26%)], Train Loss: 0.60330\n",
      "Epoch: 00 [ 5204/19985 ( 26%)], Train Loss: 0.60182\n",
      "Epoch: 00 [ 5244/19985 ( 26%)], Train Loss: 0.59992\n",
      "Epoch: 00 [ 5284/19985 ( 26%)], Train Loss: 0.59858\n",
      "Epoch: 00 [ 5324/19985 ( 27%)], Train Loss: 0.59570\n",
      "Epoch: 00 [ 5364/19985 ( 27%)], Train Loss: 0.59395\n",
      "Epoch: 00 [ 5404/19985 ( 27%)], Train Loss: 0.59192\n",
      "Epoch: 00 [ 5444/19985 ( 27%)], Train Loss: 0.58993\n",
      "Epoch: 00 [ 5484/19985 ( 27%)], Train Loss: 0.59082\n",
      "Epoch: 00 [ 5524/19985 ( 28%)], Train Loss: 0.59028\n",
      "Epoch: 00 [ 5564/19985 ( 28%)], Train Loss: 0.58874\n",
      "Epoch: 00 [ 5604/19985 ( 28%)], Train Loss: 0.58830\n",
      "Epoch: 00 [ 5644/19985 ( 28%)], Train Loss: 0.58663\n",
      "Epoch: 00 [ 5684/19985 ( 28%)], Train Loss: 0.58460\n",
      "Epoch: 00 [ 5724/19985 ( 29%)], Train Loss: 0.58371\n",
      "Epoch: 00 [ 5764/19985 ( 29%)], Train Loss: 0.58083\n",
      "Epoch: 00 [ 5804/19985 ( 29%)], Train Loss: 0.57929\n",
      "Epoch: 00 [ 5844/19985 ( 29%)], Train Loss: 0.57761\n",
      "Epoch: 00 [ 5884/19985 ( 29%)], Train Loss: 0.57497\n",
      "Epoch: 00 [ 5924/19985 ( 30%)], Train Loss: 0.57300\n",
      "Epoch: 00 [ 5964/19985 ( 30%)], Train Loss: 0.57160\n",
      "Epoch: 00 [ 6004/19985 ( 30%)], Train Loss: 0.57025\n",
      "Epoch: 00 [ 6044/19985 ( 30%)], Train Loss: 0.56814\n",
      "Epoch: 00 [ 6084/19985 ( 30%)], Train Loss: 0.56640\n",
      "Epoch: 00 [ 6124/19985 ( 31%)], Train Loss: 0.56630\n",
      "Epoch: 00 [ 6164/19985 ( 31%)], Train Loss: 0.56594\n",
      "Epoch: 00 [ 6204/19985 ( 31%)], Train Loss: 0.56435\n",
      "Epoch: 00 [ 6244/19985 ( 31%)], Train Loss: 0.56333\n",
      "Epoch: 00 [ 6284/19985 ( 31%)], Train Loss: 0.56299\n",
      "Epoch: 00 [ 6324/19985 ( 32%)], Train Loss: 0.56194\n",
      "Epoch: 00 [ 6364/19985 ( 32%)], Train Loss: 0.56016\n",
      "Epoch: 00 [ 6404/19985 ( 32%)], Train Loss: 0.55986\n",
      "Epoch: 00 [ 6444/19985 ( 32%)], Train Loss: 0.55871\n",
      "Epoch: 00 [ 6484/19985 ( 32%)], Train Loss: 0.55737\n",
      "Epoch: 00 [ 6524/19985 ( 33%)], Train Loss: 0.55673\n",
      "Epoch: 00 [ 6564/19985 ( 33%)], Train Loss: 0.55642\n",
      "Epoch: 00 [ 6604/19985 ( 33%)], Train Loss: 0.55481\n",
      "Epoch: 00 [ 6644/19985 ( 33%)], Train Loss: 0.55343\n",
      "Epoch: 00 [ 6684/19985 ( 33%)], Train Loss: 0.55275\n",
      "Epoch: 00 [ 6724/19985 ( 34%)], Train Loss: 0.55165\n",
      "Epoch: 00 [ 6764/19985 ( 34%)], Train Loss: 0.55089\n",
      "Epoch: 00 [ 6804/19985 ( 34%)], Train Loss: 0.55039\n",
      "Epoch: 00 [ 6844/19985 ( 34%)], Train Loss: 0.55122\n",
      "Epoch: 00 [ 6884/19985 ( 34%)], Train Loss: 0.55045\n",
      "Epoch: 00 [ 6924/19985 ( 35%)], Train Loss: 0.54937\n",
      "Epoch: 00 [ 6964/19985 ( 35%)], Train Loss: 0.54952\n",
      "Epoch: 00 [ 7004/19985 ( 35%)], Train Loss: 0.54977\n",
      "Epoch: 00 [ 7044/19985 ( 35%)], Train Loss: 0.54829\n",
      "Epoch: 00 [ 7084/19985 ( 35%)], Train Loss: 0.54716\n",
      "Epoch: 00 [ 7124/19985 ( 36%)], Train Loss: 0.54694\n",
      "Epoch: 00 [ 7164/19985 ( 36%)], Train Loss: 0.54590\n",
      "Epoch: 00 [ 7204/19985 ( 36%)], Train Loss: 0.54592\n",
      "Epoch: 00 [ 7244/19985 ( 36%)], Train Loss: 0.54603\n",
      "Epoch: 00 [ 7284/19985 ( 36%)], Train Loss: 0.54484\n",
      "Epoch: 00 [ 7324/19985 ( 37%)], Train Loss: 0.54433\n",
      "Epoch: 00 [ 7364/19985 ( 37%)], Train Loss: 0.54302\n",
      "Epoch: 00 [ 7404/19985 ( 37%)], Train Loss: 0.54187\n",
      "Epoch: 00 [ 7444/19985 ( 37%)], Train Loss: 0.54019\n",
      "Epoch: 00 [ 7484/19985 ( 37%)], Train Loss: 0.53937\n",
      "Epoch: 00 [ 7524/19985 ( 38%)], Train Loss: 0.53886\n",
      "Epoch: 00 [ 7564/19985 ( 38%)], Train Loss: 0.53803\n",
      "Epoch: 00 [ 7604/19985 ( 38%)], Train Loss: 0.53781\n",
      "Epoch: 00 [ 7644/19985 ( 38%)], Train Loss: 0.53687\n",
      "Epoch: 00 [ 7684/19985 ( 38%)], Train Loss: 0.53681\n",
      "Epoch: 00 [ 7724/19985 ( 39%)], Train Loss: 0.53562\n",
      "Epoch: 00 [ 7764/19985 ( 39%)], Train Loss: 0.53393\n",
      "Epoch: 00 [ 7804/19985 ( 39%)], Train Loss: 0.53345\n",
      "Epoch: 00 [ 7844/19985 ( 39%)], Train Loss: 0.53302\n",
      "Epoch: 00 [ 7884/19985 ( 39%)], Train Loss: 0.53289\n",
      "Epoch: 00 [ 7924/19985 ( 40%)], Train Loss: 0.53227\n",
      "Epoch: 00 [ 7964/19985 ( 40%)], Train Loss: 0.53123\n",
      "Epoch: 00 [ 8004/19985 ( 40%)], Train Loss: 0.53099\n",
      "Epoch: 00 [ 8044/19985 ( 40%)], Train Loss: 0.52992\n",
      "Epoch: 00 [ 8084/19985 ( 40%)], Train Loss: 0.52854\n",
      "Epoch: 00 [ 8124/19985 ( 41%)], Train Loss: 0.52748\n",
      "Epoch: 00 [ 8164/19985 ( 41%)], Train Loss: 0.52652\n",
      "Epoch: 00 [ 8204/19985 ( 41%)], Train Loss: 0.52544\n",
      "Epoch: 00 [ 8244/19985 ( 41%)], Train Loss: 0.52540\n",
      "Epoch: 00 [ 8284/19985 ( 41%)], Train Loss: 0.52482\n",
      "Epoch: 00 [ 8324/19985 ( 42%)], Train Loss: 0.52426\n",
      "Epoch: 00 [ 8364/19985 ( 42%)], Train Loss: 0.52398\n",
      "Epoch: 00 [ 8404/19985 ( 42%)], Train Loss: 0.52328\n",
      "Epoch: 00 [ 8444/19985 ( 42%)], Train Loss: 0.52301\n",
      "Epoch: 00 [ 8484/19985 ( 42%)], Train Loss: 0.52228\n",
      "Epoch: 00 [ 8524/19985 ( 43%)], Train Loss: 0.52257\n",
      "Epoch: 00 [ 8564/19985 ( 43%)], Train Loss: 0.52210\n",
      "Epoch: 00 [ 8604/19985 ( 43%)], Train Loss: 0.52096\n",
      "Epoch: 00 [ 8644/19985 ( 43%)], Train Loss: 0.51975\n",
      "Epoch: 00 [ 8684/19985 ( 43%)], Train Loss: 0.51890\n",
      "Epoch: 00 [ 8724/19985 ( 44%)], Train Loss: 0.51810\n",
      "Epoch: 00 [ 8764/19985 ( 44%)], Train Loss: 0.51682\n",
      "Epoch: 00 [ 8804/19985 ( 44%)], Train Loss: 0.51678\n",
      "Epoch: 00 [ 8844/19985 ( 44%)], Train Loss: 0.51633\n",
      "Epoch: 00 [ 8884/19985 ( 44%)], Train Loss: 0.51666\n",
      "Epoch: 00 [ 8924/19985 ( 45%)], Train Loss: 0.51548\n",
      "Epoch: 00 [ 8964/19985 ( 45%)], Train Loss: 0.51506\n",
      "Epoch: 00 [ 9004/19985 ( 45%)], Train Loss: 0.51482\n",
      "Epoch: 00 [ 9044/19985 ( 45%)], Train Loss: 0.51432\n",
      "Epoch: 00 [ 9084/19985 ( 45%)], Train Loss: 0.51403\n",
      "Epoch: 00 [ 9124/19985 ( 46%)], Train Loss: 0.51380\n",
      "Epoch: 00 [ 9164/19985 ( 46%)], Train Loss: 0.51293\n",
      "Epoch: 00 [ 9204/19985 ( 46%)], Train Loss: 0.51239\n",
      "Epoch: 00 [ 9244/19985 ( 46%)], Train Loss: 0.51198\n",
      "Epoch: 00 [ 9284/19985 ( 46%)], Train Loss: 0.51211\n",
      "Epoch: 00 [ 9324/19985 ( 47%)], Train Loss: 0.51165\n",
      "Epoch: 00 [ 9364/19985 ( 47%)], Train Loss: 0.51128\n",
      "Epoch: 00 [ 9404/19985 ( 47%)], Train Loss: 0.51069\n",
      "Epoch: 00 [ 9444/19985 ( 47%)], Train Loss: 0.51033\n",
      "Epoch: 00 [ 9484/19985 ( 47%)], Train Loss: 0.50927\n",
      "Epoch: 00 [ 9524/19985 ( 48%)], Train Loss: 0.50888\n",
      "Epoch: 00 [ 9564/19985 ( 48%)], Train Loss: 0.50873\n",
      "Epoch: 00 [ 9604/19985 ( 48%)], Train Loss: 0.50859\n",
      "Epoch: 00 [ 9644/19985 ( 48%)], Train Loss: 0.50933\n",
      "Epoch: 00 [ 9684/19985 ( 48%)], Train Loss: 0.50851\n",
      "Epoch: 00 [ 9724/19985 ( 49%)], Train Loss: 0.50774\n",
      "Epoch: 00 [ 9764/19985 ( 49%)], Train Loss: 0.50680\n",
      "Epoch: 00 [ 9804/19985 ( 49%)], Train Loss: 0.50662\n",
      "Epoch: 00 [ 9844/19985 ( 49%)], Train Loss: 0.50607\n",
      "Epoch: 00 [ 9884/19985 ( 49%)], Train Loss: 0.50473\n",
      "Epoch: 00 [ 9924/19985 ( 50%)], Train Loss: 0.50397\n",
      "Epoch: 00 [ 9964/19985 ( 50%)], Train Loss: 0.50309\n",
      "Epoch: 00 [10004/19985 ( 50%)], Train Loss: 0.50243\n",
      "Epoch: 00 [10044/19985 ( 50%)], Train Loss: 0.50168\n",
      "Epoch: 00 [10084/19985 ( 50%)], Train Loss: 0.50130\n",
      "Epoch: 00 [10124/19985 ( 51%)], Train Loss: 0.50050\n",
      "Epoch: 00 [10164/19985 ( 51%)], Train Loss: 0.50017\n",
      "Epoch: 00 [10204/19985 ( 51%)], Train Loss: 0.50040\n",
      "Epoch: 00 [10244/19985 ( 51%)], Train Loss: 0.49940\n",
      "Epoch: 00 [10284/19985 ( 51%)], Train Loss: 0.49838\n",
      "Epoch: 00 [10324/19985 ( 52%)], Train Loss: 0.49758\n",
      "Epoch: 00 [10364/19985 ( 52%)], Train Loss: 0.49737\n",
      "Epoch: 00 [10404/19985 ( 52%)], Train Loss: 0.49701\n",
      "Epoch: 00 [10444/19985 ( 52%)], Train Loss: 0.49718\n",
      "Epoch: 00 [10484/19985 ( 52%)], Train Loss: 0.49656\n",
      "Epoch: 00 [10524/19985 ( 53%)], Train Loss: 0.49577\n",
      "Epoch: 00 [10564/19985 ( 53%)], Train Loss: 0.49562\n",
      "Epoch: 00 [10604/19985 ( 53%)], Train Loss: 0.49579\n",
      "Epoch: 00 [10644/19985 ( 53%)], Train Loss: 0.49543\n",
      "Epoch: 00 [10684/19985 ( 53%)], Train Loss: 0.49519\n",
      "Epoch: 00 [10724/19985 ( 54%)], Train Loss: 0.49467\n",
      "Epoch: 00 [10764/19985 ( 54%)], Train Loss: 0.49405\n",
      "Epoch: 00 [10804/19985 ( 54%)], Train Loss: 0.49372\n",
      "Epoch: 00 [10844/19985 ( 54%)], Train Loss: 0.49351\n",
      "Epoch: 00 [10884/19985 ( 54%)], Train Loss: 0.49347\n",
      "Epoch: 00 [10924/19985 ( 55%)], Train Loss: 0.49309\n",
      "Epoch: 00 [10964/19985 ( 55%)], Train Loss: 0.49260\n",
      "Epoch: 00 [11004/19985 ( 55%)], Train Loss: 0.49170\n",
      "Epoch: 00 [11044/19985 ( 55%)], Train Loss: 0.49172\n",
      "Epoch: 00 [11084/19985 ( 55%)], Train Loss: 0.49167\n",
      "Epoch: 00 [11124/19985 ( 56%)], Train Loss: 0.49119\n",
      "Epoch: 00 [11164/19985 ( 56%)], Train Loss: 0.49081\n",
      "Epoch: 00 [11204/19985 ( 56%)], Train Loss: 0.49061\n",
      "Epoch: 00 [11244/19985 ( 56%)], Train Loss: 0.48964\n",
      "Epoch: 00 [11284/19985 ( 56%)], Train Loss: 0.48922\n",
      "Epoch: 00 [11324/19985 ( 57%)], Train Loss: 0.48826\n",
      "Epoch: 00 [11364/19985 ( 57%)], Train Loss: 0.48724\n",
      "Epoch: 00 [11404/19985 ( 57%)], Train Loss: 0.48726\n",
      "Epoch: 00 [11444/19985 ( 57%)], Train Loss: 0.48704\n",
      "Epoch: 00 [11484/19985 ( 57%)], Train Loss: 0.48666\n",
      "Epoch: 00 [11524/19985 ( 58%)], Train Loss: 0.48719\n",
      "Epoch: 00 [11564/19985 ( 58%)], Train Loss: 0.48691\n",
      "Epoch: 00 [11604/19985 ( 58%)], Train Loss: 0.48625\n",
      "Epoch: 00 [11644/19985 ( 58%)], Train Loss: 0.48592\n",
      "Epoch: 00 [11684/19985 ( 58%)], Train Loss: 0.48525\n",
      "Epoch: 00 [11724/19985 ( 59%)], Train Loss: 0.48493\n",
      "Epoch: 00 [11764/19985 ( 59%)], Train Loss: 0.48399\n",
      "Epoch: 00 [11804/19985 ( 59%)], Train Loss: 0.48371\n",
      "Epoch: 00 [11844/19985 ( 59%)], Train Loss: 0.48282\n",
      "Epoch: 00 [11884/19985 ( 59%)], Train Loss: 0.48281\n",
      "Epoch: 00 [11924/19985 ( 60%)], Train Loss: 0.48252\n",
      "Epoch: 00 [11964/19985 ( 60%)], Train Loss: 0.48210\n",
      "Epoch: 00 [12004/19985 ( 60%)], Train Loss: 0.48180\n",
      "Epoch: 00 [12044/19985 ( 60%)], Train Loss: 0.48119\n",
      "Epoch: 00 [12084/19985 ( 60%)], Train Loss: 0.48070\n",
      "Epoch: 00 [12124/19985 ( 61%)], Train Loss: 0.48019\n",
      "Epoch: 00 [12164/19985 ( 61%)], Train Loss: 0.47992\n",
      "Epoch: 00 [12204/19985 ( 61%)], Train Loss: 0.48002\n",
      "Epoch: 00 [12244/19985 ( 61%)], Train Loss: 0.47944\n",
      "Epoch: 00 [12284/19985 ( 61%)], Train Loss: 0.47990\n",
      "Epoch: 00 [12324/19985 ( 62%)], Train Loss: 0.47907\n",
      "Epoch: 00 [12364/19985 ( 62%)], Train Loss: 0.47976\n",
      "Epoch: 00 [12404/19985 ( 62%)], Train Loss: 0.47908\n",
      "Epoch: 00 [12444/19985 ( 62%)], Train Loss: 0.47814\n",
      "Epoch: 00 [12484/19985 ( 62%)], Train Loss: 0.47789\n",
      "Epoch: 00 [12524/19985 ( 63%)], Train Loss: 0.47731\n",
      "Epoch: 00 [12564/19985 ( 63%)], Train Loss: 0.47677\n",
      "Epoch: 00 [12604/19985 ( 63%)], Train Loss: 0.47634\n",
      "Epoch: 00 [12644/19985 ( 63%)], Train Loss: 0.47609\n",
      "Epoch: 00 [12684/19985 ( 63%)], Train Loss: 0.47558\n",
      "Epoch: 00 [12724/19985 ( 64%)], Train Loss: 0.47550\n",
      "Epoch: 00 [12764/19985 ( 64%)], Train Loss: 0.47517\n",
      "Epoch: 00 [12804/19985 ( 64%)], Train Loss: 0.47470\n",
      "Epoch: 00 [12844/19985 ( 64%)], Train Loss: 0.47423\n",
      "Epoch: 00 [12884/19985 ( 64%)], Train Loss: 0.47382\n",
      "Epoch: 00 [12924/19985 ( 65%)], Train Loss: 0.47307\n",
      "Epoch: 00 [12964/19985 ( 65%)], Train Loss: 0.47284\n",
      "Epoch: 00 [13004/19985 ( 65%)], Train Loss: 0.47261\n",
      "Epoch: 00 [13044/19985 ( 65%)], Train Loss: 0.47202\n",
      "Epoch: 00 [13084/19985 ( 65%)], Train Loss: 0.47161\n",
      "Epoch: 00 [13124/19985 ( 66%)], Train Loss: 0.47101\n",
      "Epoch: 00 [13164/19985 ( 66%)], Train Loss: 0.47080\n",
      "Epoch: 00 [13204/19985 ( 66%)], Train Loss: 0.47017\n",
      "Epoch: 00 [13244/19985 ( 66%)], Train Loss: 0.46964\n",
      "Epoch: 00 [13284/19985 ( 66%)], Train Loss: 0.46949\n",
      "Epoch: 00 [13324/19985 ( 67%)], Train Loss: 0.46888\n",
      "Epoch: 00 [13364/19985 ( 67%)], Train Loss: 0.46858\n",
      "Epoch: 00 [13404/19985 ( 67%)], Train Loss: 0.46792\n",
      "Epoch: 00 [13444/19985 ( 67%)], Train Loss: 0.46768\n",
      "Epoch: 00 [13484/19985 ( 67%)], Train Loss: 0.46742\n",
      "Epoch: 00 [13524/19985 ( 68%)], Train Loss: 0.46683\n",
      "Epoch: 00 [13564/19985 ( 68%)], Train Loss: 0.46624\n",
      "Epoch: 00 [13604/19985 ( 68%)], Train Loss: 0.46646\n",
      "Epoch: 00 [13644/19985 ( 68%)], Train Loss: 0.46565\n",
      "Epoch: 00 [13684/19985 ( 68%)], Train Loss: 0.46545\n",
      "Epoch: 00 [13724/19985 ( 69%)], Train Loss: 0.46512\n",
      "Epoch: 00 [13764/19985 ( 69%)], Train Loss: 0.46477\n",
      "Epoch: 00 [13804/19985 ( 69%)], Train Loss: 0.46440\n",
      "Epoch: 00 [13844/19985 ( 69%)], Train Loss: 0.46467\n",
      "Epoch: 00 [13884/19985 ( 69%)], Train Loss: 0.46425\n",
      "Epoch: 00 [13924/19985 ( 70%)], Train Loss: 0.46399\n",
      "Epoch: 00 [13964/19985 ( 70%)], Train Loss: 0.46407\n",
      "Epoch: 00 [14004/19985 ( 70%)], Train Loss: 0.46400\n",
      "Epoch: 00 [14044/19985 ( 70%)], Train Loss: 0.46392\n",
      "Epoch: 00 [14084/19985 ( 70%)], Train Loss: 0.46336\n",
      "Epoch: 00 [14124/19985 ( 71%)], Train Loss: 0.46281\n",
      "Epoch: 00 [14164/19985 ( 71%)], Train Loss: 0.46222\n",
      "Epoch: 00 [14204/19985 ( 71%)], Train Loss: 0.46198\n",
      "Epoch: 00 [14244/19985 ( 71%)], Train Loss: 0.46147\n",
      "Epoch: 00 [14284/19985 ( 71%)], Train Loss: 0.46170\n",
      "Epoch: 00 [14324/19985 ( 72%)], Train Loss: 0.46064\n",
      "Epoch: 00 [14364/19985 ( 72%)], Train Loss: 0.46038\n",
      "Epoch: 00 [14404/19985 ( 72%)], Train Loss: 0.46017\n",
      "Epoch: 00 [14444/19985 ( 72%)], Train Loss: 0.45974\n",
      "Epoch: 00 [14484/19985 ( 72%)], Train Loss: 0.45951\n",
      "Epoch: 00 [14524/19985 ( 73%)], Train Loss: 0.45893\n",
      "Epoch: 00 [14564/19985 ( 73%)], Train Loss: 0.45861\n",
      "Epoch: 00 [14604/19985 ( 73%)], Train Loss: 0.45855\n",
      "Epoch: 00 [14644/19985 ( 73%)], Train Loss: 0.45819\n",
      "Epoch: 00 [14684/19985 ( 73%)], Train Loss: 0.45773\n",
      "Epoch: 00 [14724/19985 ( 74%)], Train Loss: 0.45702\n",
      "Epoch: 00 [14764/19985 ( 74%)], Train Loss: 0.45666\n",
      "Epoch: 00 [14804/19985 ( 74%)], Train Loss: 0.45601\n",
      "Epoch: 00 [14844/19985 ( 74%)], Train Loss: 0.45592\n",
      "Epoch: 00 [14884/19985 ( 74%)], Train Loss: 0.45536\n",
      "Epoch: 00 [14924/19985 ( 75%)], Train Loss: 0.45553\n",
      "Epoch: 00 [14964/19985 ( 75%)], Train Loss: 0.45545\n",
      "Epoch: 00 [15004/19985 ( 75%)], Train Loss: 0.45568\n",
      "Epoch: 00 [15044/19985 ( 75%)], Train Loss: 0.45519\n",
      "Epoch: 00 [15084/19985 ( 75%)], Train Loss: 0.45511\n",
      "Epoch: 00 [15124/19985 ( 76%)], Train Loss: 0.45463\n",
      "Epoch: 00 [15164/19985 ( 76%)], Train Loss: 0.45470\n",
      "Epoch: 00 [15204/19985 ( 76%)], Train Loss: 0.45439\n",
      "Epoch: 00 [15244/19985 ( 76%)], Train Loss: 0.45415\n",
      "Epoch: 00 [15284/19985 ( 76%)], Train Loss: 0.45413\n",
      "Epoch: 00 [15324/19985 ( 77%)], Train Loss: 0.45360\n",
      "Epoch: 00 [15364/19985 ( 77%)], Train Loss: 0.45305\n",
      "Epoch: 00 [15404/19985 ( 77%)], Train Loss: 0.45283\n",
      "Epoch: 00 [15444/19985 ( 77%)], Train Loss: 0.45266\n",
      "Epoch: 00 [15484/19985 ( 77%)], Train Loss: 0.45237\n",
      "Epoch: 00 [15524/19985 ( 78%)], Train Loss: 0.45188\n",
      "Epoch: 00 [15564/19985 ( 78%)], Train Loss: 0.45153\n",
      "Epoch: 00 [15604/19985 ( 78%)], Train Loss: 0.45101\n",
      "Epoch: 00 [15644/19985 ( 78%)], Train Loss: 0.45058\n",
      "Epoch: 00 [15684/19985 ( 78%)], Train Loss: 0.45013\n",
      "Epoch: 00 [15724/19985 ( 79%)], Train Loss: 0.45033\n",
      "Epoch: 00 [15764/19985 ( 79%)], Train Loss: 0.44970\n",
      "Epoch: 00 [15804/19985 ( 79%)], Train Loss: 0.44914\n",
      "Epoch: 00 [15844/19985 ( 79%)], Train Loss: 0.44866\n",
      "Epoch: 00 [15884/19985 ( 79%)], Train Loss: 0.44835\n",
      "Epoch: 00 [15924/19985 ( 80%)], Train Loss: 0.44795\n",
      "Epoch: 00 [15964/19985 ( 80%)], Train Loss: 0.44741\n",
      "Epoch: 00 [16004/19985 ( 80%)], Train Loss: 0.44718\n",
      "Epoch: 00 [16044/19985 ( 80%)], Train Loss: 0.44662\n",
      "Epoch: 00 [16084/19985 ( 80%)], Train Loss: 0.44603\n",
      "Epoch: 00 [16124/19985 ( 81%)], Train Loss: 0.44576\n",
      "Epoch: 00 [16164/19985 ( 81%)], Train Loss: 0.44529\n",
      "Epoch: 00 [16204/19985 ( 81%)], Train Loss: 0.44467\n",
      "Epoch: 00 [16244/19985 ( 81%)], Train Loss: 0.44441\n",
      "Epoch: 00 [16284/19985 ( 81%)], Train Loss: 0.44406\n",
      "Epoch: 00 [16324/19985 ( 82%)], Train Loss: 0.44372\n",
      "Epoch: 00 [16364/19985 ( 82%)], Train Loss: 0.44314\n",
      "Epoch: 00 [16404/19985 ( 82%)], Train Loss: 0.44270\n",
      "Epoch: 00 [16444/19985 ( 82%)], Train Loss: 0.44250\n",
      "Epoch: 00 [16484/19985 ( 82%)], Train Loss: 0.44219\n",
      "Epoch: 00 [16524/19985 ( 83%)], Train Loss: 0.44161\n",
      "Epoch: 00 [16564/19985 ( 83%)], Train Loss: 0.44138\n",
      "Epoch: 00 [16604/19985 ( 83%)], Train Loss: 0.44135\n",
      "Epoch: 00 [16644/19985 ( 83%)], Train Loss: 0.44088\n",
      "Epoch: 00 [16684/19985 ( 83%)], Train Loss: 0.44031\n",
      "Epoch: 00 [16724/19985 ( 84%)], Train Loss: 0.44015\n",
      "Epoch: 00 [16764/19985 ( 84%)], Train Loss: 0.44003\n",
      "Epoch: 00 [16804/19985 ( 84%)], Train Loss: 0.43984\n",
      "Epoch: 00 [16844/19985 ( 84%)], Train Loss: 0.43947\n",
      "Epoch: 00 [16884/19985 ( 84%)], Train Loss: 0.43904\n",
      "Epoch: 00 [16924/19985 ( 85%)], Train Loss: 0.43896\n",
      "Epoch: 00 [16964/19985 ( 85%)], Train Loss: 0.43877\n",
      "Epoch: 00 [17004/19985 ( 85%)], Train Loss: 0.43854\n",
      "Epoch: 00 [17044/19985 ( 85%)], Train Loss: 0.43805\n",
      "Epoch: 00 [17084/19985 ( 85%)], Train Loss: 0.43795\n",
      "Epoch: 00 [17124/19985 ( 86%)], Train Loss: 0.43747\n",
      "Epoch: 00 [17164/19985 ( 86%)], Train Loss: 0.43673\n",
      "Epoch: 00 [17204/19985 ( 86%)], Train Loss: 0.43628\n",
      "Epoch: 00 [17244/19985 ( 86%)], Train Loss: 0.43597\n",
      "Epoch: 00 [17284/19985 ( 86%)], Train Loss: 0.43561\n",
      "Epoch: 00 [17324/19985 ( 87%)], Train Loss: 0.43516\n",
      "Epoch: 00 [17364/19985 ( 87%)], Train Loss: 0.43443\n",
      "Epoch: 00 [17404/19985 ( 87%)], Train Loss: 0.43422\n",
      "Epoch: 00 [17444/19985 ( 87%)], Train Loss: 0.43435\n",
      "Epoch: 00 [17484/19985 ( 87%)], Train Loss: 0.43412\n",
      "Epoch: 00 [17524/19985 ( 88%)], Train Loss: 0.43375\n",
      "Epoch: 00 [17564/19985 ( 88%)], Train Loss: 0.43342\n",
      "Epoch: 00 [17604/19985 ( 88%)], Train Loss: 0.43304\n",
      "Epoch: 00 [17644/19985 ( 88%)], Train Loss: 0.43275\n",
      "Epoch: 00 [17684/19985 ( 88%)], Train Loss: 0.43292\n",
      "Epoch: 00 [17724/19985 ( 89%)], Train Loss: 0.43297\n",
      "Epoch: 00 [17764/19985 ( 89%)], Train Loss: 0.43267\n",
      "Epoch: 00 [17804/19985 ( 89%)], Train Loss: 0.43186\n",
      "Epoch: 00 [17844/19985 ( 89%)], Train Loss: 0.43163\n",
      "Epoch: 00 [17884/19985 ( 89%)], Train Loss: 0.43117\n",
      "Epoch: 00 [17924/19985 ( 90%)], Train Loss: 0.43085\n",
      "Epoch: 00 [17964/19985 ( 90%)], Train Loss: 0.43054\n",
      "Epoch: 00 [18004/19985 ( 90%)], Train Loss: 0.43020\n",
      "Epoch: 00 [18044/19985 ( 90%)], Train Loss: 0.42983\n",
      "Epoch: 00 [18084/19985 ( 90%)], Train Loss: 0.42934\n",
      "Epoch: 00 [18124/19985 ( 91%)], Train Loss: 0.42900\n",
      "Epoch: 00 [18164/19985 ( 91%)], Train Loss: 0.42863\n",
      "Epoch: 00 [18204/19985 ( 91%)], Train Loss: 0.42837\n",
      "Epoch: 00 [18244/19985 ( 91%)], Train Loss: 0.42823\n",
      "Epoch: 00 [18284/19985 ( 91%)], Train Loss: 0.42829\n",
      "Epoch: 00 [18324/19985 ( 92%)], Train Loss: 0.42796\n",
      "Epoch: 00 [18364/19985 ( 92%)], Train Loss: 0.42787\n",
      "Epoch: 00 [18404/19985 ( 92%)], Train Loss: 0.42745\n",
      "Epoch: 00 [18444/19985 ( 92%)], Train Loss: 0.42704\n",
      "Epoch: 00 [18484/19985 ( 92%)], Train Loss: 0.42676\n",
      "Epoch: 00 [18524/19985 ( 93%)], Train Loss: 0.42637\n",
      "Epoch: 00 [18564/19985 ( 93%)], Train Loss: 0.42652\n",
      "Epoch: 00 [18604/19985 ( 93%)], Train Loss: 0.42652\n",
      "Epoch: 00 [18644/19985 ( 93%)], Train Loss: 0.42616\n",
      "Epoch: 00 [18684/19985 ( 93%)], Train Loss: 0.42604\n",
      "Epoch: 00 [18724/19985 ( 94%)], Train Loss: 0.42593\n",
      "Epoch: 00 [18764/19985 ( 94%)], Train Loss: 0.42549\n",
      "Epoch: 00 [18804/19985 ( 94%)], Train Loss: 0.42511\n",
      "Epoch: 00 [18844/19985 ( 94%)], Train Loss: 0.42468\n",
      "Epoch: 00 [18884/19985 ( 94%)], Train Loss: 0.42455\n",
      "Epoch: 00 [18924/19985 ( 95%)], Train Loss: 0.42437\n",
      "Epoch: 00 [18964/19985 ( 95%)], Train Loss: 0.42414\n",
      "Epoch: 00 [19004/19985 ( 95%)], Train Loss: 0.42408\n",
      "Epoch: 00 [19044/19985 ( 95%)], Train Loss: 0.42397\n",
      "Epoch: 00 [19084/19985 ( 95%)], Train Loss: 0.42413\n",
      "Epoch: 00 [19124/19985 ( 96%)], Train Loss: 0.42383\n",
      "Epoch: 00 [19164/19985 ( 96%)], Train Loss: 0.42346\n",
      "Epoch: 00 [19204/19985 ( 96%)], Train Loss: 0.42349\n",
      "Epoch: 00 [19244/19985 ( 96%)], Train Loss: 0.42302\n",
      "Epoch: 00 [19284/19985 ( 96%)], Train Loss: 0.42329\n",
      "Epoch: 00 [19324/19985 ( 97%)], Train Loss: 0.42300\n",
      "Epoch: 00 [19364/19985 ( 97%)], Train Loss: 0.42312\n",
      "Epoch: 00 [19404/19985 ( 97%)], Train Loss: 0.42286\n",
      "Epoch: 00 [19444/19985 ( 97%)], Train Loss: 0.42255\n",
      "Epoch: 00 [19484/19985 ( 97%)], Train Loss: 0.42230\n",
      "Epoch: 00 [19524/19985 ( 98%)], Train Loss: 0.42203\n",
      "Epoch: 00 [19564/19985 ( 98%)], Train Loss: 0.42150\n",
      "Epoch: 00 [19604/19985 ( 98%)], Train Loss: 0.42114\n",
      "Epoch: 00 [19644/19985 ( 98%)], Train Loss: 0.42085\n",
      "Epoch: 00 [19684/19985 ( 98%)], Train Loss: 0.42131\n",
      "Epoch: 00 [19724/19985 ( 99%)], Train Loss: 0.42153\n",
      "Epoch: 00 [19764/19985 ( 99%)], Train Loss: 0.42122\n",
      "Epoch: 00 [19804/19985 ( 99%)], Train Loss: 0.42103\n",
      "Epoch: 00 [19844/19985 ( 99%)], Train Loss: 0.42068\n",
      "Epoch: 00 [19884/19985 ( 99%)], Train Loss: 0.42062\n",
      "Epoch: 00 [19924/19985 (100%)], Train Loss: 0.42028\n",
      "Epoch: 00 [19964/19985 (100%)], Train Loss: 0.42009\n",
      "Epoch: 00 [19985/19985 (100%)], Train Loss: 0.41986\n",
      "----Validation Results Summary----\n",
      "Epoch: [0] Valid Loss: 0.22611\n",
      "Post-processing 223 example predictions split into 3084 features.\n",
      "valid jaccard:  0.6770358005223476\n",
      "0 Epoch, Best epoch was updated! Valid Loss: 0.22611\n",
      "Saving model checkpoint to output/checkpoint-fold-0-epoch-0.\n",
      "\n",
      "Total Training Time: 2847.1625854969025secs, Average Training Time per Epoch: 2847.1625854969025secs.\n",
      "Total Validation Time: 185.56472182273865secs, Average Validation Time per Epoch: 185.56472182273865secs.\n",
      "\n",
      "\n",
      "--------------------------------------------------\n",
      "FOLD: 1\n",
      "--------------------------------------------------\n",
      "Model pushed to 1 GPU(s), type Tesla P100-PCIE-16GB.\n",
      "Num examples Train= 20298, Num examples Valid=2771\n",
      "Total Training Steps: 2538, Total Warmup Steps: 253\n",
      "Epoch: 00 [    4/20298 (  0%)], Train Loss: 3.13621\n",
      "Epoch: 00 [   44/20298 (  0%)], Train Loss: 3.04069\n",
      "Epoch: 00 [   84/20298 (  0%)], Train Loss: 2.99566\n",
      "Epoch: 00 [  124/20298 (  1%)], Train Loss: 2.96958\n",
      "Epoch: 00 [  164/20298 (  1%)], Train Loss: 2.89235\n",
      "Epoch: 00 [  204/20298 (  1%)], Train Loss: 2.81204\n",
      "Epoch: 00 [  244/20298 (  1%)], Train Loss: 2.71492\n",
      "Epoch: 00 [  284/20298 (  1%)], Train Loss: 2.58209\n",
      "Epoch: 00 [  324/20298 (  2%)], Train Loss: 2.42298\n",
      "Epoch: 00 [  364/20298 (  2%)], Train Loss: 2.28594\n",
      "Epoch: 00 [  404/20298 (  2%)], Train Loss: 2.14787\n",
      "Epoch: 00 [  444/20298 (  2%)], Train Loss: 2.02054\n",
      "Epoch: 00 [  484/20298 (  2%)], Train Loss: 1.92569\n",
      "Epoch: 00 [  524/20298 (  3%)], Train Loss: 1.82380\n",
      "Epoch: 00 [  564/20298 (  3%)], Train Loss: 1.72883\n",
      "Epoch: 00 [  604/20298 (  3%)], Train Loss: 1.63824\n",
      "Epoch: 00 [  644/20298 (  3%)], Train Loss: 1.55968\n",
      "Epoch: 00 [  684/20298 (  3%)], Train Loss: 1.48707\n",
      "Epoch: 00 [  724/20298 (  4%)], Train Loss: 1.44793\n",
      "Epoch: 00 [  764/20298 (  4%)], Train Loss: 1.40087\n",
      "Epoch: 00 [  804/20298 (  4%)], Train Loss: 1.36119\n",
      "Epoch: 00 [  844/20298 (  4%)], Train Loss: 1.32299\n",
      "Epoch: 00 [  884/20298 (  4%)], Train Loss: 1.28993\n",
      "Epoch: 00 [  924/20298 (  5%)], Train Loss: 1.25270\n",
      "Epoch: 00 [  964/20298 (  5%)], Train Loss: 1.21008\n",
      "Epoch: 00 [ 1004/20298 (  5%)], Train Loss: 1.18425\n",
      "Epoch: 00 [ 1044/20298 (  5%)], Train Loss: 1.15009\n",
      "Epoch: 00 [ 1084/20298 (  5%)], Train Loss: 1.12021\n",
      "Epoch: 00 [ 1124/20298 (  6%)], Train Loss: 1.09909\n",
      "Epoch: 00 [ 1164/20298 (  6%)], Train Loss: 1.07643\n",
      "Epoch: 00 [ 1204/20298 (  6%)], Train Loss: 1.05256\n",
      "Epoch: 00 [ 1244/20298 (  6%)], Train Loss: 1.03644\n",
      "Epoch: 00 [ 1284/20298 (  6%)], Train Loss: 1.02669\n",
      "Epoch: 00 [ 1324/20298 (  7%)], Train Loss: 1.00904\n",
      "Epoch: 00 [ 1364/20298 (  7%)], Train Loss: 0.99251\n",
      "Epoch: 00 [ 1404/20298 (  7%)], Train Loss: 0.97765\n",
      "Epoch: 00 [ 1444/20298 (  7%)], Train Loss: 0.96253\n",
      "Epoch: 00 [ 1484/20298 (  7%)], Train Loss: 0.94455\n",
      "Epoch: 00 [ 1524/20298 (  8%)], Train Loss: 0.93291\n",
      "Epoch: 00 [ 1564/20298 (  8%)], Train Loss: 0.92138\n",
      "Epoch: 00 [ 1604/20298 (  8%)], Train Loss: 0.90524\n",
      "Epoch: 00 [ 1644/20298 (  8%)], Train Loss: 0.89329\n",
      "Epoch: 00 [ 1684/20298 (  8%)], Train Loss: 0.87958\n",
      "Epoch: 00 [ 1724/20298 (  8%)], Train Loss: 0.87055\n",
      "Epoch: 00 [ 1764/20298 (  9%)], Train Loss: 0.85584\n",
      "Epoch: 00 [ 1804/20298 (  9%)], Train Loss: 0.84697\n",
      "Epoch: 00 [ 1844/20298 (  9%)], Train Loss: 0.84074\n",
      "Epoch: 00 [ 1884/20298 (  9%)], Train Loss: 0.83533\n",
      "Epoch: 00 [ 1924/20298 (  9%)], Train Loss: 0.83019\n",
      "Epoch: 00 [ 1964/20298 ( 10%)], Train Loss: 0.82179\n",
      "Epoch: 00 [ 2004/20298 ( 10%)], Train Loss: 0.81533\n",
      "Epoch: 00 [ 2044/20298 ( 10%)], Train Loss: 0.80552\n",
      "Epoch: 00 [ 2084/20298 ( 10%)], Train Loss: 0.79901\n",
      "Epoch: 00 [ 2124/20298 ( 10%)], Train Loss: 0.79519\n",
      "Epoch: 00 [ 2164/20298 ( 11%)], Train Loss: 0.78930\n",
      "Epoch: 00 [ 2204/20298 ( 11%)], Train Loss: 0.77837\n",
      "Epoch: 00 [ 2244/20298 ( 11%)], Train Loss: 0.77219\n",
      "Epoch: 00 [ 2284/20298 ( 11%)], Train Loss: 0.76762\n",
      "Epoch: 00 [ 2324/20298 ( 11%)], Train Loss: 0.76576\n",
      "Epoch: 00 [ 2364/20298 ( 12%)], Train Loss: 0.75994\n",
      "Epoch: 00 [ 2404/20298 ( 12%)], Train Loss: 0.75326\n",
      "Epoch: 00 [ 2444/20298 ( 12%)], Train Loss: 0.74837\n",
      "Epoch: 00 [ 2484/20298 ( 12%)], Train Loss: 0.74181\n",
      "Epoch: 00 [ 2524/20298 ( 12%)], Train Loss: 0.73787\n",
      "Epoch: 00 [ 2564/20298 ( 13%)], Train Loss: 0.73357\n",
      "Epoch: 00 [ 2604/20298 ( 13%)], Train Loss: 0.73035\n",
      "Epoch: 00 [ 2644/20298 ( 13%)], Train Loss: 0.72517\n",
      "Epoch: 00 [ 2684/20298 ( 13%)], Train Loss: 0.72100\n",
      "Epoch: 00 [ 2724/20298 ( 13%)], Train Loss: 0.71670\n",
      "Epoch: 00 [ 2764/20298 ( 14%)], Train Loss: 0.71259\n",
      "Epoch: 00 [ 2804/20298 ( 14%)], Train Loss: 0.71280\n",
      "Epoch: 00 [ 2844/20298 ( 14%)], Train Loss: 0.70913\n",
      "Epoch: 00 [ 2884/20298 ( 14%)], Train Loss: 0.70641\n",
      "Epoch: 00 [ 2924/20298 ( 14%)], Train Loss: 0.70222\n",
      "Epoch: 00 [ 2964/20298 ( 15%)], Train Loss: 0.69813\n",
      "Epoch: 00 [ 3004/20298 ( 15%)], Train Loss: 0.69271\n",
      "Epoch: 00 [ 3044/20298 ( 15%)], Train Loss: 0.69014\n",
      "Epoch: 00 [ 3084/20298 ( 15%)], Train Loss: 0.68708\n",
      "Epoch: 00 [ 3124/20298 ( 15%)], Train Loss: 0.68162\n",
      "Epoch: 00 [ 3164/20298 ( 16%)], Train Loss: 0.67826\n",
      "Epoch: 00 [ 3204/20298 ( 16%)], Train Loss: 0.67363\n",
      "Epoch: 00 [ 3244/20298 ( 16%)], Train Loss: 0.67252\n",
      "Epoch: 00 [ 3284/20298 ( 16%)], Train Loss: 0.66704\n",
      "Epoch: 00 [ 3324/20298 ( 16%)], Train Loss: 0.66545\n",
      "Epoch: 00 [ 3364/20298 ( 17%)], Train Loss: 0.66171\n",
      "Epoch: 00 [ 3404/20298 ( 17%)], Train Loss: 0.66280\n",
      "Epoch: 00 [ 3444/20298 ( 17%)], Train Loss: 0.66037\n",
      "Epoch: 00 [ 3484/20298 ( 17%)], Train Loss: 0.65767\n",
      "Epoch: 00 [ 3524/20298 ( 17%)], Train Loss: 0.65631\n",
      "Epoch: 00 [ 3564/20298 ( 18%)], Train Loss: 0.65333\n",
      "Epoch: 00 [ 3604/20298 ( 18%)], Train Loss: 0.65255\n",
      "Epoch: 00 [ 3644/20298 ( 18%)], Train Loss: 0.65028\n",
      "Epoch: 00 [ 3684/20298 ( 18%)], Train Loss: 0.64708\n",
      "Epoch: 00 [ 3724/20298 ( 18%)], Train Loss: 0.64378\n",
      "Epoch: 00 [ 3764/20298 ( 19%)], Train Loss: 0.64221\n",
      "Epoch: 00 [ 3804/20298 ( 19%)], Train Loss: 0.63853\n",
      "Epoch: 00 [ 3844/20298 ( 19%)], Train Loss: 0.63543\n",
      "Epoch: 00 [ 3884/20298 ( 19%)], Train Loss: 0.63494\n",
      "Epoch: 00 [ 3924/20298 ( 19%)], Train Loss: 0.63217\n",
      "Epoch: 00 [ 3964/20298 ( 20%)], Train Loss: 0.62981\n",
      "Epoch: 00 [ 4004/20298 ( 20%)], Train Loss: 0.62817\n",
      "Epoch: 00 [ 4044/20298 ( 20%)], Train Loss: 0.62694\n",
      "Epoch: 00 [ 4084/20298 ( 20%)], Train Loss: 0.62455\n",
      "Epoch: 00 [ 4124/20298 ( 20%)], Train Loss: 0.62279\n",
      "Epoch: 00 [ 4164/20298 ( 21%)], Train Loss: 0.62042\n",
      "Epoch: 00 [ 4204/20298 ( 21%)], Train Loss: 0.61987\n",
      "Epoch: 00 [ 4244/20298 ( 21%)], Train Loss: 0.61920\n",
      "Epoch: 00 [ 4284/20298 ( 21%)], Train Loss: 0.61742\n",
      "Epoch: 00 [ 4324/20298 ( 21%)], Train Loss: 0.61415\n",
      "Epoch: 00 [ 4364/20298 ( 21%)], Train Loss: 0.61424\n",
      "Epoch: 00 [ 4404/20298 ( 22%)], Train Loss: 0.61320\n",
      "Epoch: 00 [ 4444/20298 ( 22%)], Train Loss: 0.61238\n",
      "Epoch: 00 [ 4484/20298 ( 22%)], Train Loss: 0.61179\n",
      "Epoch: 00 [ 4524/20298 ( 22%)], Train Loss: 0.61020\n",
      "Epoch: 00 [ 4564/20298 ( 22%)], Train Loss: 0.60827\n",
      "Epoch: 00 [ 4604/20298 ( 23%)], Train Loss: 0.60719\n",
      "Epoch: 00 [ 4644/20298 ( 23%)], Train Loss: 0.60454\n",
      "Epoch: 00 [ 4684/20298 ( 23%)], Train Loss: 0.60351\n",
      "Epoch: 00 [ 4724/20298 ( 23%)], Train Loss: 0.60082\n",
      "Epoch: 00 [ 4764/20298 ( 23%)], Train Loss: 0.59890\n",
      "Epoch: 00 [ 4804/20298 ( 24%)], Train Loss: 0.59647\n",
      "Epoch: 00 [ 4844/20298 ( 24%)], Train Loss: 0.59763\n",
      "Epoch: 00 [ 4884/20298 ( 24%)], Train Loss: 0.59667\n",
      "Epoch: 00 [ 4924/20298 ( 24%)], Train Loss: 0.59537\n",
      "Epoch: 00 [ 4964/20298 ( 24%)], Train Loss: 0.59239\n",
      "Epoch: 00 [ 5004/20298 ( 25%)], Train Loss: 0.59025\n",
      "Epoch: 00 [ 5044/20298 ( 25%)], Train Loss: 0.58845\n",
      "Epoch: 00 [ 5084/20298 ( 25%)], Train Loss: 0.58671\n",
      "Epoch: 00 [ 5124/20298 ( 25%)], Train Loss: 0.58684\n",
      "Epoch: 00 [ 5164/20298 ( 25%)], Train Loss: 0.58773\n",
      "Epoch: 00 [ 5204/20298 ( 26%)], Train Loss: 0.58605\n",
      "Epoch: 00 [ 5244/20298 ( 26%)], Train Loss: 0.58389\n",
      "Epoch: 00 [ 5284/20298 ( 26%)], Train Loss: 0.58116\n",
      "Epoch: 00 [ 5324/20298 ( 26%)], Train Loss: 0.58036\n",
      "Epoch: 00 [ 5364/20298 ( 26%)], Train Loss: 0.57873\n",
      "Epoch: 00 [ 5404/20298 ( 27%)], Train Loss: 0.57713\n",
      "Epoch: 00 [ 5444/20298 ( 27%)], Train Loss: 0.57587\n",
      "Epoch: 00 [ 5484/20298 ( 27%)], Train Loss: 0.57340\n",
      "Epoch: 00 [ 5524/20298 ( 27%)], Train Loss: 0.57208\n",
      "Epoch: 00 [ 5564/20298 ( 27%)], Train Loss: 0.57100\n",
      "Epoch: 00 [ 5604/20298 ( 28%)], Train Loss: 0.57049\n",
      "Epoch: 00 [ 5644/20298 ( 28%)], Train Loss: 0.57023\n",
      "Epoch: 00 [ 5684/20298 ( 28%)], Train Loss: 0.56929\n",
      "Epoch: 00 [ 5724/20298 ( 28%)], Train Loss: 0.56885\n",
      "Epoch: 00 [ 5764/20298 ( 28%)], Train Loss: 0.56757\n",
      "Epoch: 00 [ 5804/20298 ( 29%)], Train Loss: 0.56564\n",
      "Epoch: 00 [ 5844/20298 ( 29%)], Train Loss: 0.56604\n",
      "Epoch: 00 [ 5884/20298 ( 29%)], Train Loss: 0.56486\n",
      "Epoch: 00 [ 5924/20298 ( 29%)], Train Loss: 0.56468\n",
      "Epoch: 00 [ 5964/20298 ( 29%)], Train Loss: 0.56315\n",
      "Epoch: 00 [ 6004/20298 ( 30%)], Train Loss: 0.56251\n",
      "Epoch: 00 [ 6044/20298 ( 30%)], Train Loss: 0.56110\n",
      "Epoch: 00 [ 6084/20298 ( 30%)], Train Loss: 0.55972\n",
      "Epoch: 00 [ 6124/20298 ( 30%)], Train Loss: 0.55967\n",
      "Epoch: 00 [ 6164/20298 ( 30%)], Train Loss: 0.55884\n",
      "Epoch: 00 [ 6204/20298 ( 31%)], Train Loss: 0.56099\n",
      "Epoch: 00 [ 6244/20298 ( 31%)], Train Loss: 0.55942\n",
      "Epoch: 00 [ 6284/20298 ( 31%)], Train Loss: 0.55903\n",
      "Epoch: 00 [ 6324/20298 ( 31%)], Train Loss: 0.55784\n",
      "Epoch: 00 [ 6364/20298 ( 31%)], Train Loss: 0.55782\n",
      "Epoch: 00 [ 6404/20298 ( 32%)], Train Loss: 0.55663\n",
      "Epoch: 00 [ 6444/20298 ( 32%)], Train Loss: 0.55577\n",
      "Epoch: 00 [ 6484/20298 ( 32%)], Train Loss: 0.55383\n",
      "Epoch: 00 [ 6524/20298 ( 32%)], Train Loss: 0.55337\n",
      "Epoch: 00 [ 6564/20298 ( 32%)], Train Loss: 0.55275\n",
      "Epoch: 00 [ 6604/20298 ( 33%)], Train Loss: 0.55178\n",
      "Epoch: 00 [ 6644/20298 ( 33%)], Train Loss: 0.55165\n",
      "Epoch: 00 [ 6684/20298 ( 33%)], Train Loss: 0.54983\n",
      "Epoch: 00 [ 6724/20298 ( 33%)], Train Loss: 0.54924\n",
      "Epoch: 00 [ 6764/20298 ( 33%)], Train Loss: 0.54909\n",
      "Epoch: 00 [ 6804/20298 ( 34%)], Train Loss: 0.54771\n",
      "Epoch: 00 [ 6844/20298 ( 34%)], Train Loss: 0.54655\n",
      "Epoch: 00 [ 6884/20298 ( 34%)], Train Loss: 0.54587\n",
      "Epoch: 00 [ 6924/20298 ( 34%)], Train Loss: 0.54481\n",
      "Epoch: 00 [ 6964/20298 ( 34%)], Train Loss: 0.54452\n",
      "Epoch: 00 [ 7004/20298 ( 35%)], Train Loss: 0.54344\n",
      "Epoch: 00 [ 7044/20298 ( 35%)], Train Loss: 0.54279\n",
      "Epoch: 00 [ 7084/20298 ( 35%)], Train Loss: 0.54229\n",
      "Epoch: 00 [ 7124/20298 ( 35%)], Train Loss: 0.54100\n",
      "Epoch: 00 [ 7164/20298 ( 35%)], Train Loss: 0.54044\n",
      "Epoch: 00 [ 7204/20298 ( 35%)], Train Loss: 0.53888\n",
      "Epoch: 00 [ 7244/20298 ( 36%)], Train Loss: 0.53764\n",
      "Epoch: 00 [ 7284/20298 ( 36%)], Train Loss: 0.53666\n",
      "Epoch: 00 [ 7324/20298 ( 36%)], Train Loss: 0.53558\n",
      "Epoch: 00 [ 7364/20298 ( 36%)], Train Loss: 0.53494\n",
      "Epoch: 00 [ 7404/20298 ( 36%)], Train Loss: 0.53451\n",
      "Epoch: 00 [ 7444/20298 ( 37%)], Train Loss: 0.53438\n",
      "Epoch: 00 [ 7484/20298 ( 37%)], Train Loss: 0.53295\n",
      "Epoch: 00 [ 7524/20298 ( 37%)], Train Loss: 0.53242\n",
      "Epoch: 00 [ 7564/20298 ( 37%)], Train Loss: 0.53245\n",
      "Epoch: 00 [ 7604/20298 ( 37%)], Train Loss: 0.53163\n",
      "Epoch: 00 [ 7644/20298 ( 38%)], Train Loss: 0.53082\n",
      "Epoch: 00 [ 7684/20298 ( 38%)], Train Loss: 0.53008\n",
      "Epoch: 00 [ 7724/20298 ( 38%)], Train Loss: 0.52948\n",
      "Epoch: 00 [ 7764/20298 ( 38%)], Train Loss: 0.52955\n",
      "Epoch: 00 [ 7804/20298 ( 38%)], Train Loss: 0.52818\n",
      "Epoch: 00 [ 7844/20298 ( 39%)], Train Loss: 0.52806\n",
      "Epoch: 00 [ 7884/20298 ( 39%)], Train Loss: 0.52754\n",
      "Epoch: 00 [ 7924/20298 ( 39%)], Train Loss: 0.52662\n",
      "Epoch: 00 [ 7964/20298 ( 39%)], Train Loss: 0.52598\n",
      "Epoch: 00 [ 8004/20298 ( 39%)], Train Loss: 0.52463\n",
      "Epoch: 00 [ 8044/20298 ( 40%)], Train Loss: 0.52373\n",
      "Epoch: 00 [ 8084/20298 ( 40%)], Train Loss: 0.52399\n",
      "Epoch: 00 [ 8124/20298 ( 40%)], Train Loss: 0.52379\n",
      "Epoch: 00 [ 8164/20298 ( 40%)], Train Loss: 0.52295\n",
      "Epoch: 00 [ 8204/20298 ( 40%)], Train Loss: 0.52239\n",
      "Epoch: 00 [ 8244/20298 ( 41%)], Train Loss: 0.52088\n",
      "Epoch: 00 [ 8284/20298 ( 41%)], Train Loss: 0.52034\n",
      "Epoch: 00 [ 8324/20298 ( 41%)], Train Loss: 0.51956\n",
      "Epoch: 00 [ 8364/20298 ( 41%)], Train Loss: 0.51880\n",
      "Epoch: 00 [ 8404/20298 ( 41%)], Train Loss: 0.51843\n",
      "Epoch: 00 [ 8444/20298 ( 42%)], Train Loss: 0.51785\n",
      "Epoch: 00 [ 8484/20298 ( 42%)], Train Loss: 0.51713\n",
      "Epoch: 00 [ 8524/20298 ( 42%)], Train Loss: 0.51674\n",
      "Epoch: 00 [ 8564/20298 ( 42%)], Train Loss: 0.51569\n",
      "Epoch: 00 [ 8604/20298 ( 42%)], Train Loss: 0.51520\n",
      "Epoch: 00 [ 8644/20298 ( 43%)], Train Loss: 0.51429\n",
      "Epoch: 00 [ 8684/20298 ( 43%)], Train Loss: 0.51386\n",
      "Epoch: 00 [ 8724/20298 ( 43%)], Train Loss: 0.51296\n",
      "Epoch: 00 [ 8764/20298 ( 43%)], Train Loss: 0.51332\n",
      "Epoch: 00 [ 8804/20298 ( 43%)], Train Loss: 0.51328\n",
      "Epoch: 00 [ 8844/20298 ( 44%)], Train Loss: 0.51286\n",
      "Epoch: 00 [ 8884/20298 ( 44%)], Train Loss: 0.51213\n",
      "Epoch: 00 [ 8924/20298 ( 44%)], Train Loss: 0.51113\n",
      "Epoch: 00 [ 8964/20298 ( 44%)], Train Loss: 0.51078\n",
      "Epoch: 00 [ 9004/20298 ( 44%)], Train Loss: 0.51013\n",
      "Epoch: 00 [ 9044/20298 ( 45%)], Train Loss: 0.51019\n",
      "Epoch: 00 [ 9084/20298 ( 45%)], Train Loss: 0.50989\n",
      "Epoch: 00 [ 9124/20298 ( 45%)], Train Loss: 0.50936\n",
      "Epoch: 00 [ 9164/20298 ( 45%)], Train Loss: 0.50940\n",
      "Epoch: 00 [ 9204/20298 ( 45%)], Train Loss: 0.50799\n",
      "Epoch: 00 [ 9244/20298 ( 46%)], Train Loss: 0.50682\n",
      "Epoch: 00 [ 9284/20298 ( 46%)], Train Loss: 0.50624\n",
      "Epoch: 00 [ 9324/20298 ( 46%)], Train Loss: 0.50542\n",
      "Epoch: 00 [ 9364/20298 ( 46%)], Train Loss: 0.50463\n",
      "Epoch: 00 [ 9404/20298 ( 46%)], Train Loss: 0.50356\n",
      "Epoch: 00 [ 9444/20298 ( 47%)], Train Loss: 0.50262\n",
      "Epoch: 00 [ 9484/20298 ( 47%)], Train Loss: 0.50237\n",
      "Epoch: 00 [ 9524/20298 ( 47%)], Train Loss: 0.50219\n",
      "Epoch: 00 [ 9564/20298 ( 47%)], Train Loss: 0.50167\n",
      "Epoch: 00 [ 9604/20298 ( 47%)], Train Loss: 0.50185\n",
      "Epoch: 00 [ 9644/20298 ( 48%)], Train Loss: 0.50225\n",
      "Epoch: 00 [ 9684/20298 ( 48%)], Train Loss: 0.50192\n",
      "Epoch: 00 [ 9724/20298 ( 48%)], Train Loss: 0.50160\n",
      "Epoch: 00 [ 9764/20298 ( 48%)], Train Loss: 0.50116\n",
      "Epoch: 00 [ 9804/20298 ( 48%)], Train Loss: 0.50102\n",
      "Epoch: 00 [ 9844/20298 ( 48%)], Train Loss: 0.50047\n",
      "Epoch: 00 [ 9884/20298 ( 49%)], Train Loss: 0.50006\n",
      "Epoch: 00 [ 9924/20298 ( 49%)], Train Loss: 0.49997\n",
      "Epoch: 00 [ 9964/20298 ( 49%)], Train Loss: 0.49959\n",
      "Epoch: 00 [10004/20298 ( 49%)], Train Loss: 0.49945\n",
      "Epoch: 00 [10044/20298 ( 49%)], Train Loss: 0.49788\n",
      "Epoch: 00 [10084/20298 ( 50%)], Train Loss: 0.49705\n",
      "Epoch: 00 [10124/20298 ( 50%)], Train Loss: 0.49594\n",
      "Epoch: 00 [10164/20298 ( 50%)], Train Loss: 0.49525\n",
      "Epoch: 00 [10204/20298 ( 50%)], Train Loss: 0.49480\n",
      "Epoch: 00 [10244/20298 ( 50%)], Train Loss: 0.49501\n",
      "Epoch: 00 [10284/20298 ( 51%)], Train Loss: 0.49422\n",
      "Epoch: 00 [10324/20298 ( 51%)], Train Loss: 0.49395\n",
      "Epoch: 00 [10364/20298 ( 51%)], Train Loss: 0.49323\n",
      "Epoch: 00 [10404/20298 ( 51%)], Train Loss: 0.49268\n",
      "Epoch: 00 [10444/20298 ( 51%)], Train Loss: 0.49227\n",
      "Epoch: 00 [10484/20298 ( 52%)], Train Loss: 0.49138\n",
      "Epoch: 00 [10524/20298 ( 52%)], Train Loss: 0.49093\n",
      "Epoch: 00 [10564/20298 ( 52%)], Train Loss: 0.49073\n",
      "Epoch: 00 [10604/20298 ( 52%)], Train Loss: 0.49054\n",
      "Epoch: 00 [10644/20298 ( 52%)], Train Loss: 0.48968\n",
      "Epoch: 00 [10684/20298 ( 53%)], Train Loss: 0.48925\n",
      "Epoch: 00 [10724/20298 ( 53%)], Train Loss: 0.48873\n",
      "Epoch: 00 [10764/20298 ( 53%)], Train Loss: 0.48804\n",
      "Epoch: 00 [10804/20298 ( 53%)], Train Loss: 0.48836\n",
      "Epoch: 00 [10844/20298 ( 53%)], Train Loss: 0.48728\n",
      "Epoch: 00 [10884/20298 ( 54%)], Train Loss: 0.48593\n",
      "Epoch: 00 [10924/20298 ( 54%)], Train Loss: 0.48557\n",
      "Epoch: 00 [10964/20298 ( 54%)], Train Loss: 0.48551\n",
      "Epoch: 00 [11004/20298 ( 54%)], Train Loss: 0.48609\n",
      "Epoch: 00 [11044/20298 ( 54%)], Train Loss: 0.48532\n",
      "Epoch: 00 [11084/20298 ( 55%)], Train Loss: 0.48464\n",
      "Epoch: 00 [11124/20298 ( 55%)], Train Loss: 0.48417\n",
      "Epoch: 00 [11164/20298 ( 55%)], Train Loss: 0.48398\n",
      "Epoch: 00 [11204/20298 ( 55%)], Train Loss: 0.48418\n",
      "Epoch: 00 [11244/20298 ( 55%)], Train Loss: 0.48407\n",
      "Epoch: 00 [11284/20298 ( 56%)], Train Loss: 0.48368\n",
      "Epoch: 00 [11324/20298 ( 56%)], Train Loss: 0.48289\n",
      "Epoch: 00 [11364/20298 ( 56%)], Train Loss: 0.48263\n",
      "Epoch: 00 [11404/20298 ( 56%)], Train Loss: 0.48255\n",
      "Epoch: 00 [11444/20298 ( 56%)], Train Loss: 0.48253\n",
      "Epoch: 00 [11484/20298 ( 57%)], Train Loss: 0.48177\n",
      "Epoch: 00 [11524/20298 ( 57%)], Train Loss: 0.48145\n",
      "Epoch: 00 [11564/20298 ( 57%)], Train Loss: 0.48091\n",
      "Epoch: 00 [11604/20298 ( 57%)], Train Loss: 0.48038\n",
      "Epoch: 00 [11644/20298 ( 57%)], Train Loss: 0.47998\n",
      "Epoch: 00 [11684/20298 ( 58%)], Train Loss: 0.47987\n",
      "Epoch: 00 [11724/20298 ( 58%)], Train Loss: 0.47928\n",
      "Epoch: 00 [11764/20298 ( 58%)], Train Loss: 0.47852\n",
      "Epoch: 00 [11804/20298 ( 58%)], Train Loss: 0.47722\n",
      "Epoch: 00 [11844/20298 ( 58%)], Train Loss: 0.47721\n",
      "Epoch: 00 [11884/20298 ( 59%)], Train Loss: 0.47668\n",
      "Epoch: 00 [11924/20298 ( 59%)], Train Loss: 0.47672\n",
      "Epoch: 00 [11964/20298 ( 59%)], Train Loss: 0.47640\n",
      "Epoch: 00 [12004/20298 ( 59%)], Train Loss: 0.47572\n",
      "Epoch: 00 [12044/20298 ( 59%)], Train Loss: 0.47583\n",
      "Epoch: 00 [12084/20298 ( 60%)], Train Loss: 0.47586\n",
      "Epoch: 00 [12124/20298 ( 60%)], Train Loss: 0.47540\n",
      "Epoch: 00 [12164/20298 ( 60%)], Train Loss: 0.47485\n",
      "Epoch: 00 [12204/20298 ( 60%)], Train Loss: 0.47491\n",
      "Epoch: 00 [12244/20298 ( 60%)], Train Loss: 0.47430\n",
      "Epoch: 00 [12284/20298 ( 61%)], Train Loss: 0.47381\n",
      "Epoch: 00 [12324/20298 ( 61%)], Train Loss: 0.47312\n",
      "Epoch: 00 [12364/20298 ( 61%)], Train Loss: 0.47246\n",
      "Epoch: 00 [12404/20298 ( 61%)], Train Loss: 0.47263\n",
      "Epoch: 00 [12444/20298 ( 61%)], Train Loss: 0.47236\n",
      "Epoch: 00 [12484/20298 ( 62%)], Train Loss: 0.47278\n",
      "Epoch: 00 [12524/20298 ( 62%)], Train Loss: 0.47211\n",
      "Epoch: 00 [12564/20298 ( 62%)], Train Loss: 0.47123\n",
      "Epoch: 00 [12604/20298 ( 62%)], Train Loss: 0.47162\n",
      "Epoch: 00 [12644/20298 ( 62%)], Train Loss: 0.47146\n",
      "Epoch: 00 [12684/20298 ( 62%)], Train Loss: 0.47180\n",
      "Epoch: 00 [12724/20298 ( 63%)], Train Loss: 0.47141\n",
      "Epoch: 00 [12764/20298 ( 63%)], Train Loss: 0.47082\n",
      "Epoch: 00 [12804/20298 ( 63%)], Train Loss: 0.47013\n",
      "Epoch: 00 [12844/20298 ( 63%)], Train Loss: 0.46953\n",
      "Epoch: 00 [12884/20298 ( 63%)], Train Loss: 0.46918\n",
      "Epoch: 00 [12924/20298 ( 64%)], Train Loss: 0.46866\n",
      "Epoch: 00 [12964/20298 ( 64%)], Train Loss: 0.46854\n",
      "Epoch: 00 [13004/20298 ( 64%)], Train Loss: 0.46824\n",
      "Epoch: 00 [13044/20298 ( 64%)], Train Loss: 0.46760\n",
      "Epoch: 00 [13084/20298 ( 64%)], Train Loss: 0.46693\n",
      "Epoch: 00 [13124/20298 ( 65%)], Train Loss: 0.46614\n",
      "Epoch: 00 [13164/20298 ( 65%)], Train Loss: 0.46616\n",
      "Epoch: 00 [13204/20298 ( 65%)], Train Loss: 0.46647\n",
      "Epoch: 00 [13244/20298 ( 65%)], Train Loss: 0.46666\n",
      "Epoch: 00 [13284/20298 ( 65%)], Train Loss: 0.46625\n",
      "Epoch: 00 [13324/20298 ( 66%)], Train Loss: 0.46664\n",
      "Epoch: 00 [13364/20298 ( 66%)], Train Loss: 0.46635\n",
      "Epoch: 00 [13404/20298 ( 66%)], Train Loss: 0.46582\n",
      "Epoch: 00 [13444/20298 ( 66%)], Train Loss: 0.46613\n",
      "Epoch: 00 [13484/20298 ( 66%)], Train Loss: 0.46544\n",
      "Epoch: 00 [13524/20298 ( 67%)], Train Loss: 0.46521\n",
      "Epoch: 00 [13564/20298 ( 67%)], Train Loss: 0.46446\n",
      "Epoch: 00 [13604/20298 ( 67%)], Train Loss: 0.46410\n",
      "Epoch: 00 [13644/20298 ( 67%)], Train Loss: 0.46354\n",
      "Epoch: 00 [13684/20298 ( 67%)], Train Loss: 0.46304\n",
      "Epoch: 00 [13724/20298 ( 68%)], Train Loss: 0.46281\n",
      "Epoch: 00 [13764/20298 ( 68%)], Train Loss: 0.46262\n",
      "Epoch: 00 [13804/20298 ( 68%)], Train Loss: 0.46257\n",
      "Epoch: 00 [13844/20298 ( 68%)], Train Loss: 0.46253\n",
      "Epoch: 00 [13884/20298 ( 68%)], Train Loss: 0.46199\n",
      "Epoch: 00 [13924/20298 ( 69%)], Train Loss: 0.46141\n",
      "Epoch: 00 [13964/20298 ( 69%)], Train Loss: 0.46102\n",
      "Epoch: 00 [14004/20298 ( 69%)], Train Loss: 0.46007\n",
      "Epoch: 00 [14044/20298 ( 69%)], Train Loss: 0.45942\n",
      "Epoch: 00 [14084/20298 ( 69%)], Train Loss: 0.45933\n",
      "Epoch: 00 [14124/20298 ( 70%)], Train Loss: 0.45922\n",
      "Epoch: 00 [14164/20298 ( 70%)], Train Loss: 0.45877\n",
      "Epoch: 00 [14204/20298 ( 70%)], Train Loss: 0.45817\n",
      "Epoch: 00 [14244/20298 ( 70%)], Train Loss: 0.45786\n",
      "Epoch: 00 [14284/20298 ( 70%)], Train Loss: 0.45715\n",
      "Epoch: 00 [14324/20298 ( 71%)], Train Loss: 0.45703\n",
      "Epoch: 00 [14364/20298 ( 71%)], Train Loss: 0.45701\n",
      "Epoch: 00 [14404/20298 ( 71%)], Train Loss: 0.45626\n",
      "Epoch: 00 [14444/20298 ( 71%)], Train Loss: 0.45580\n",
      "Epoch: 00 [14484/20298 ( 71%)], Train Loss: 0.45554\n",
      "Epoch: 00 [14524/20298 ( 72%)], Train Loss: 0.45505\n",
      "Epoch: 00 [14564/20298 ( 72%)], Train Loss: 0.45450\n",
      "Epoch: 00 [14604/20298 ( 72%)], Train Loss: 0.45413\n",
      "Epoch: 00 [14644/20298 ( 72%)], Train Loss: 0.45381\n",
      "Epoch: 00 [14684/20298 ( 72%)], Train Loss: 0.45346\n",
      "Epoch: 00 [14724/20298 ( 73%)], Train Loss: 0.45331\n",
      "Epoch: 00 [14764/20298 ( 73%)], Train Loss: 0.45298\n",
      "Epoch: 00 [14804/20298 ( 73%)], Train Loss: 0.45326\n",
      "Epoch: 00 [14844/20298 ( 73%)], Train Loss: 0.45278\n",
      "Epoch: 00 [14884/20298 ( 73%)], Train Loss: 0.45260\n",
      "Epoch: 00 [14924/20298 ( 74%)], Train Loss: 0.45254\n",
      "Epoch: 00 [14964/20298 ( 74%)], Train Loss: 0.45215\n",
      "Epoch: 00 [15004/20298 ( 74%)], Train Loss: 0.45162\n",
      "Epoch: 00 [15044/20298 ( 74%)], Train Loss: 0.45134\n",
      "Epoch: 00 [15084/20298 ( 74%)], Train Loss: 0.45099\n",
      "Epoch: 00 [15124/20298 ( 75%)], Train Loss: 0.45052\n",
      "Epoch: 00 [15164/20298 ( 75%)], Train Loss: 0.45010\n",
      "Epoch: 00 [15204/20298 ( 75%)], Train Loss: 0.44993\n",
      "Epoch: 00 [15244/20298 ( 75%)], Train Loss: 0.44942\n",
      "Epoch: 00 [15284/20298 ( 75%)], Train Loss: 0.44900\n",
      "Epoch: 00 [15324/20298 ( 75%)], Train Loss: 0.44925\n",
      "Epoch: 00 [15364/20298 ( 76%)], Train Loss: 0.44892\n",
      "Epoch: 00 [15404/20298 ( 76%)], Train Loss: 0.44848\n",
      "Epoch: 00 [15444/20298 ( 76%)], Train Loss: 0.44784\n",
      "Epoch: 00 [15484/20298 ( 76%)], Train Loss: 0.44698\n",
      "Epoch: 00 [15524/20298 ( 76%)], Train Loss: 0.44665\n",
      "Epoch: 00 [15564/20298 ( 77%)], Train Loss: 0.44668\n",
      "Epoch: 00 [15604/20298 ( 77%)], Train Loss: 0.44611\n",
      "Epoch: 00 [15644/20298 ( 77%)], Train Loss: 0.44539\n",
      "Epoch: 00 [15684/20298 ( 77%)], Train Loss: 0.44529\n",
      "Epoch: 00 [15724/20298 ( 77%)], Train Loss: 0.44526\n",
      "Epoch: 00 [15764/20298 ( 78%)], Train Loss: 0.44474\n",
      "Epoch: 00 [15804/20298 ( 78%)], Train Loss: 0.44443\n",
      "Epoch: 00 [15844/20298 ( 78%)], Train Loss: 0.44431\n",
      "Epoch: 00 [15884/20298 ( 78%)], Train Loss: 0.44426\n",
      "Epoch: 00 [15924/20298 ( 78%)], Train Loss: 0.44388\n",
      "Epoch: 00 [15964/20298 ( 79%)], Train Loss: 0.44341\n",
      "Epoch: 00 [16004/20298 ( 79%)], Train Loss: 0.44301\n",
      "Epoch: 00 [16044/20298 ( 79%)], Train Loss: 0.44333\n",
      "Epoch: 00 [16084/20298 ( 79%)], Train Loss: 0.44287\n",
      "Epoch: 00 [16124/20298 ( 79%)], Train Loss: 0.44257\n",
      "Epoch: 00 [16164/20298 ( 80%)], Train Loss: 0.44221\n",
      "Epoch: 00 [16204/20298 ( 80%)], Train Loss: 0.44196\n",
      "Epoch: 00 [16244/20298 ( 80%)], Train Loss: 0.44185\n",
      "Epoch: 00 [16284/20298 ( 80%)], Train Loss: 0.44163\n",
      "Epoch: 00 [16324/20298 ( 80%)], Train Loss: 0.44134\n",
      "Epoch: 00 [16364/20298 ( 81%)], Train Loss: 0.44099\n",
      "Epoch: 00 [16404/20298 ( 81%)], Train Loss: 0.44033\n",
      "Epoch: 00 [16444/20298 ( 81%)], Train Loss: 0.44025\n",
      "Epoch: 00 [16484/20298 ( 81%)], Train Loss: 0.43952\n",
      "Epoch: 00 [16524/20298 ( 81%)], Train Loss: 0.43910\n",
      "Epoch: 00 [16564/20298 ( 82%)], Train Loss: 0.43896\n",
      "Epoch: 00 [16604/20298 ( 82%)], Train Loss: 0.43874\n",
      "Epoch: 00 [16644/20298 ( 82%)], Train Loss: 0.43833\n",
      "Epoch: 00 [16684/20298 ( 82%)], Train Loss: 0.43807\n",
      "Epoch: 00 [16724/20298 ( 82%)], Train Loss: 0.43804\n",
      "Epoch: 00 [16764/20298 ( 83%)], Train Loss: 0.43772\n",
      "Epoch: 00 [16804/20298 ( 83%)], Train Loss: 0.43720\n",
      "Epoch: 00 [16844/20298 ( 83%)], Train Loss: 0.43725\n",
      "Epoch: 00 [16884/20298 ( 83%)], Train Loss: 0.43686\n",
      "Epoch: 00 [16924/20298 ( 83%)], Train Loss: 0.43627\n",
      "Epoch: 00 [16964/20298 ( 84%)], Train Loss: 0.43626\n",
      "Epoch: 00 [17004/20298 ( 84%)], Train Loss: 0.43592\n",
      "Epoch: 00 [17044/20298 ( 84%)], Train Loss: 0.43603\n",
      "Epoch: 00 [17084/20298 ( 84%)], Train Loss: 0.43589\n",
      "Epoch: 00 [17124/20298 ( 84%)], Train Loss: 0.43573\n",
      "Epoch: 00 [17164/20298 ( 85%)], Train Loss: 0.43555\n",
      "Epoch: 00 [17204/20298 ( 85%)], Train Loss: 0.43510\n",
      "Epoch: 00 [17244/20298 ( 85%)], Train Loss: 0.43481\n",
      "Epoch: 00 [17284/20298 ( 85%)], Train Loss: 0.43487\n",
      "Epoch: 00 [17324/20298 ( 85%)], Train Loss: 0.43464\n",
      "Epoch: 00 [17364/20298 ( 86%)], Train Loss: 0.43450\n",
      "Epoch: 00 [17404/20298 ( 86%)], Train Loss: 0.43395\n",
      "Epoch: 00 [17444/20298 ( 86%)], Train Loss: 0.43348\n",
      "Epoch: 00 [17484/20298 ( 86%)], Train Loss: 0.43326\n",
      "Epoch: 00 [17524/20298 ( 86%)], Train Loss: 0.43294\n",
      "Epoch: 00 [17564/20298 ( 87%)], Train Loss: 0.43266\n",
      "Epoch: 00 [17604/20298 ( 87%)], Train Loss: 0.43266\n",
      "Epoch: 00 [17644/20298 ( 87%)], Train Loss: 0.43253\n",
      "Epoch: 00 [17684/20298 ( 87%)], Train Loss: 0.43191\n",
      "Epoch: 00 [17724/20298 ( 87%)], Train Loss: 0.43123\n",
      "Epoch: 00 [17764/20298 ( 88%)], Train Loss: 0.43061\n",
      "Epoch: 00 [17804/20298 ( 88%)], Train Loss: 0.43041\n",
      "Epoch: 00 [17844/20298 ( 88%)], Train Loss: 0.43013\n",
      "Epoch: 00 [17884/20298 ( 88%)], Train Loss: 0.42955\n",
      "Epoch: 00 [17924/20298 ( 88%)], Train Loss: 0.42912\n",
      "Epoch: 00 [17964/20298 ( 89%)], Train Loss: 0.42910\n",
      "Epoch: 00 [18004/20298 ( 89%)], Train Loss: 0.42854\n",
      "Epoch: 00 [18044/20298 ( 89%)], Train Loss: 0.42798\n",
      "Epoch: 00 [18084/20298 ( 89%)], Train Loss: 0.42755\n",
      "Epoch: 00 [18124/20298 ( 89%)], Train Loss: 0.42731\n",
      "Epoch: 00 [18164/20298 ( 89%)], Train Loss: 0.42747\n",
      "Epoch: 00 [18204/20298 ( 90%)], Train Loss: 0.42728\n",
      "Epoch: 00 [18244/20298 ( 90%)], Train Loss: 0.42692\n",
      "Epoch: 00 [18284/20298 ( 90%)], Train Loss: 0.42687\n",
      "Epoch: 00 [18324/20298 ( 90%)], Train Loss: 0.42693\n",
      "Epoch: 00 [18364/20298 ( 90%)], Train Loss: 0.42673\n",
      "Epoch: 00 [18404/20298 ( 91%)], Train Loss: 0.42627\n",
      "Epoch: 00 [18444/20298 ( 91%)], Train Loss: 0.42609\n",
      "Epoch: 00 [18484/20298 ( 91%)], Train Loss: 0.42581\n",
      "Epoch: 00 [18524/20298 ( 91%)], Train Loss: 0.42550\n",
      "Epoch: 00 [18564/20298 ( 91%)], Train Loss: 0.42521\n",
      "Epoch: 00 [18604/20298 ( 92%)], Train Loss: 0.42487\n",
      "Epoch: 00 [18644/20298 ( 92%)], Train Loss: 0.42466\n",
      "Epoch: 00 [18684/20298 ( 92%)], Train Loss: 0.42459\n",
      "Epoch: 00 [18724/20298 ( 92%)], Train Loss: 0.42449\n",
      "Epoch: 00 [18764/20298 ( 92%)], Train Loss: 0.42440\n",
      "Epoch: 00 [18804/20298 ( 93%)], Train Loss: 0.42423\n",
      "Epoch: 00 [18844/20298 ( 93%)], Train Loss: 0.42403\n",
      "Epoch: 00 [18884/20298 ( 93%)], Train Loss: 0.42387\n",
      "Epoch: 00 [18924/20298 ( 93%)], Train Loss: 0.42337\n",
      "Epoch: 00 [18964/20298 ( 93%)], Train Loss: 0.42348\n",
      "Epoch: 00 [19004/20298 ( 94%)], Train Loss: 0.42334\n",
      "Epoch: 00 [19044/20298 ( 94%)], Train Loss: 0.42327\n",
      "Epoch: 00 [19084/20298 ( 94%)], Train Loss: 0.42319\n",
      "Epoch: 00 [19124/20298 ( 94%)], Train Loss: 0.42281\n",
      "Epoch: 00 [19164/20298 ( 94%)], Train Loss: 0.42268\n",
      "Epoch: 00 [19204/20298 ( 95%)], Train Loss: 0.42263\n",
      "Epoch: 00 [19244/20298 ( 95%)], Train Loss: 0.42219\n",
      "Epoch: 00 [19284/20298 ( 95%)], Train Loss: 0.42223\n",
      "Epoch: 00 [19324/20298 ( 95%)], Train Loss: 0.42187\n",
      "Epoch: 00 [19364/20298 ( 95%)], Train Loss: 0.42146\n",
      "Epoch: 00 [19404/20298 ( 96%)], Train Loss: 0.42156\n",
      "Epoch: 00 [19444/20298 ( 96%)], Train Loss: 0.42127\n",
      "Epoch: 00 [19484/20298 ( 96%)], Train Loss: 0.42119\n",
      "Epoch: 00 [19524/20298 ( 96%)], Train Loss: 0.42118\n",
      "Epoch: 00 [19564/20298 ( 96%)], Train Loss: 0.42088\n",
      "Epoch: 00 [19604/20298 ( 97%)], Train Loss: 0.42057\n",
      "Epoch: 00 [19644/20298 ( 97%)], Train Loss: 0.42055\n",
      "Epoch: 00 [19684/20298 ( 97%)], Train Loss: 0.42026\n",
      "Epoch: 00 [19724/20298 ( 97%)], Train Loss: 0.41987\n",
      "Epoch: 00 [19764/20298 ( 97%)], Train Loss: 0.41936\n",
      "Epoch: 00 [19804/20298 ( 98%)], Train Loss: 0.41908\n",
      "Epoch: 00 [19844/20298 ( 98%)], Train Loss: 0.41883\n",
      "Epoch: 00 [19884/20298 ( 98%)], Train Loss: 0.41881\n",
      "Epoch: 00 [19924/20298 ( 98%)], Train Loss: 0.41844\n",
      "Epoch: 00 [19964/20298 ( 98%)], Train Loss: 0.41808\n",
      "Epoch: 00 [20004/20298 ( 99%)], Train Loss: 0.41759\n",
      "Epoch: 00 [20044/20298 ( 99%)], Train Loss: 0.41776\n",
      "Epoch: 00 [20084/20298 ( 99%)], Train Loss: 0.41768\n",
      "Epoch: 00 [20124/20298 ( 99%)], Train Loss: 0.41718\n",
      "Epoch: 00 [20164/20298 ( 99%)], Train Loss: 0.41730\n",
      "Epoch: 00 [20204/20298 (100%)], Train Loss: 0.41698\n",
      "Epoch: 00 [20244/20298 (100%)], Train Loss: 0.41668\n",
      "Epoch: 00 [20284/20298 (100%)], Train Loss: 0.41642\n",
      "Epoch: 00 [20298/20298 (100%)], Train Loss: 0.41621\n",
      "----Validation Results Summary----\n",
      "Epoch: [0] Valid Loss: 0.24908\n",
      "Post-processing 223 example predictions split into 2771 features.\n",
      "valid jaccard:  0.6708510473084465\n",
      "0 Epoch, Best epoch was updated! Valid Loss: 0.24908\n",
      "Saving model checkpoint to output/checkpoint-fold-1-epoch-0.\n",
      "\n",
      "Total Training Time: 2891.2111299037933secs, Average Training Time per Epoch: 2891.2111299037933secs.\n",
      "Total Validation Time: 164.8709692955017secs, Average Validation Time per Epoch: 164.8709692955017secs.\n",
      "\n",
      "\n",
      "--------------------------------------------------\n",
      "FOLD: 2\n",
      "--------------------------------------------------\n",
      "Model pushed to 1 GPU(s), type Tesla P100-PCIE-16GB.\n",
      "Num examples Train= 19954, Num examples Valid=3115\n",
      "Total Training Steps: 2495, Total Warmup Steps: 249\n",
      "Epoch: 00 [    4/19954 (  0%)], Train Loss: 2.99975\n",
      "Epoch: 00 [   44/19954 (  0%)], Train Loss: 2.99756\n",
      "Epoch: 00 [   84/19954 (  0%)], Train Loss: 2.99217\n",
      "Epoch: 00 [  124/19954 (  1%)], Train Loss: 2.95068\n",
      "Epoch: 00 [  164/19954 (  1%)], Train Loss: 2.90305\n",
      "Epoch: 00 [  204/19954 (  1%)], Train Loss: 2.82830\n",
      "Epoch: 00 [  244/19954 (  1%)], Train Loss: 2.72182\n",
      "Epoch: 00 [  284/19954 (  1%)], Train Loss: 2.59488\n",
      "Epoch: 00 [  324/19954 (  2%)], Train Loss: 2.44184\n",
      "Epoch: 00 [  364/19954 (  2%)], Train Loss: 2.30466\n",
      "Epoch: 00 [  404/19954 (  2%)], Train Loss: 2.16876\n",
      "Epoch: 00 [  444/19954 (  2%)], Train Loss: 2.05806\n",
      "Epoch: 00 [  484/19954 (  2%)], Train Loss: 1.94929\n",
      "Epoch: 00 [  524/19954 (  3%)], Train Loss: 1.84523\n",
      "Epoch: 00 [  564/19954 (  3%)], Train Loss: 1.74543\n",
      "Epoch: 00 [  604/19954 (  3%)], Train Loss: 1.65903\n",
      "Epoch: 00 [  644/19954 (  3%)], Train Loss: 1.57894\n",
      "Epoch: 00 [  684/19954 (  3%)], Train Loss: 1.51135\n",
      "Epoch: 00 [  724/19954 (  4%)], Train Loss: 1.45179\n",
      "Epoch: 00 [  764/19954 (  4%)], Train Loss: 1.39947\n",
      "Epoch: 00 [  804/19954 (  4%)], Train Loss: 1.35535\n",
      "Epoch: 00 [  844/19954 (  4%)], Train Loss: 1.31156\n",
      "Epoch: 00 [  884/19954 (  4%)], Train Loss: 1.27612\n",
      "Epoch: 00 [  924/19954 (  5%)], Train Loss: 1.23300\n",
      "Epoch: 00 [  964/19954 (  5%)], Train Loss: 1.19997\n",
      "Epoch: 00 [ 1004/19954 (  5%)], Train Loss: 1.18446\n",
      "Epoch: 00 [ 1044/19954 (  5%)], Train Loss: 1.16168\n",
      "Epoch: 00 [ 1084/19954 (  5%)], Train Loss: 1.12958\n",
      "Epoch: 00 [ 1124/19954 (  6%)], Train Loss: 1.10557\n",
      "Epoch: 00 [ 1164/19954 (  6%)], Train Loss: 1.08997\n",
      "Epoch: 00 [ 1204/19954 (  6%)], Train Loss: 1.06537\n",
      "Epoch: 00 [ 1244/19954 (  6%)], Train Loss: 1.04378\n",
      "Epoch: 00 [ 1284/19954 (  6%)], Train Loss: 1.02397\n",
      "Epoch: 00 [ 1324/19954 (  7%)], Train Loss: 1.01581\n",
      "Epoch: 00 [ 1364/19954 (  7%)], Train Loss: 1.00150\n",
      "Epoch: 00 [ 1404/19954 (  7%)], Train Loss: 0.98424\n",
      "Epoch: 00 [ 1444/19954 (  7%)], Train Loss: 0.96710\n",
      "Epoch: 00 [ 1484/19954 (  7%)], Train Loss: 0.95526\n",
      "Epoch: 00 [ 1524/19954 (  8%)], Train Loss: 0.94740\n",
      "Epoch: 00 [ 1564/19954 (  8%)], Train Loss: 0.93783\n",
      "Epoch: 00 [ 1604/19954 (  8%)], Train Loss: 0.92609\n",
      "Epoch: 00 [ 1644/19954 (  8%)], Train Loss: 0.91230\n",
      "Epoch: 00 [ 1684/19954 (  8%)], Train Loss: 0.90504\n",
      "Epoch: 00 [ 1724/19954 (  9%)], Train Loss: 0.89540\n",
      "Epoch: 00 [ 1764/19954 (  9%)], Train Loss: 0.88354\n",
      "Epoch: 00 [ 1804/19954 (  9%)], Train Loss: 0.87406\n",
      "Epoch: 00 [ 1844/19954 (  9%)], Train Loss: 0.86713\n",
      "Epoch: 00 [ 1884/19954 (  9%)], Train Loss: 0.85436\n",
      "Epoch: 00 [ 1924/19954 ( 10%)], Train Loss: 0.84681\n",
      "Epoch: 00 [ 1964/19954 ( 10%)], Train Loss: 0.84048\n",
      "Epoch: 00 [ 2004/19954 ( 10%)], Train Loss: 0.83215\n",
      "Epoch: 00 [ 2044/19954 ( 10%)], Train Loss: 0.82286\n",
      "Epoch: 00 [ 2084/19954 ( 10%)], Train Loss: 0.81450\n",
      "Epoch: 00 [ 2124/19954 ( 11%)], Train Loss: 0.81330\n",
      "Epoch: 00 [ 2164/19954 ( 11%)], Train Loss: 0.80814\n",
      "Epoch: 00 [ 2204/19954 ( 11%)], Train Loss: 0.80435\n",
      "Epoch: 00 [ 2244/19954 ( 11%)], Train Loss: 0.79894\n",
      "Epoch: 00 [ 2284/19954 ( 11%)], Train Loss: 0.78918\n",
      "Epoch: 00 [ 2324/19954 ( 12%)], Train Loss: 0.78493\n",
      "Epoch: 00 [ 2364/19954 ( 12%)], Train Loss: 0.78437\n",
      "Epoch: 00 [ 2404/19954 ( 12%)], Train Loss: 0.77951\n",
      "Epoch: 00 [ 2444/19954 ( 12%)], Train Loss: 0.77143\n",
      "Epoch: 00 [ 2484/19954 ( 12%)], Train Loss: 0.76976\n",
      "Epoch: 00 [ 2524/19954 ( 13%)], Train Loss: 0.76274\n",
      "Epoch: 00 [ 2564/19954 ( 13%)], Train Loss: 0.75617\n",
      "Epoch: 00 [ 2604/19954 ( 13%)], Train Loss: 0.75533\n",
      "Epoch: 00 [ 2644/19954 ( 13%)], Train Loss: 0.75321\n",
      "Epoch: 00 [ 2684/19954 ( 13%)], Train Loss: 0.74732\n",
      "Epoch: 00 [ 2724/19954 ( 14%)], Train Loss: 0.74162\n",
      "Epoch: 00 [ 2764/19954 ( 14%)], Train Loss: 0.74019\n",
      "Epoch: 00 [ 2804/19954 ( 14%)], Train Loss: 0.73450\n",
      "Epoch: 00 [ 2844/19954 ( 14%)], Train Loss: 0.72883\n",
      "Epoch: 00 [ 2884/19954 ( 14%)], Train Loss: 0.72690\n",
      "Epoch: 00 [ 2924/19954 ( 15%)], Train Loss: 0.72479\n",
      "Epoch: 00 [ 2964/19954 ( 15%)], Train Loss: 0.72367\n",
      "Epoch: 00 [ 3004/19954 ( 15%)], Train Loss: 0.71996\n",
      "Epoch: 00 [ 3044/19954 ( 15%)], Train Loss: 0.71603\n",
      "Epoch: 00 [ 3084/19954 ( 15%)], Train Loss: 0.71085\n",
      "Epoch: 00 [ 3124/19954 ( 16%)], Train Loss: 0.70683\n",
      "Epoch: 00 [ 3164/19954 ( 16%)], Train Loss: 0.70298\n",
      "Epoch: 00 [ 3204/19954 ( 16%)], Train Loss: 0.69870\n",
      "Epoch: 00 [ 3244/19954 ( 16%)], Train Loss: 0.69709\n",
      "Epoch: 00 [ 3284/19954 ( 16%)], Train Loss: 0.69513\n",
      "Epoch: 00 [ 3324/19954 ( 17%)], Train Loss: 0.69105\n",
      "Epoch: 00 [ 3364/19954 ( 17%)], Train Loss: 0.68768\n",
      "Epoch: 00 [ 3404/19954 ( 17%)], Train Loss: 0.68578\n",
      "Epoch: 00 [ 3444/19954 ( 17%)], Train Loss: 0.68160\n",
      "Epoch: 00 [ 3484/19954 ( 17%)], Train Loss: 0.67882\n",
      "Epoch: 00 [ 3524/19954 ( 18%)], Train Loss: 0.67467\n",
      "Epoch: 00 [ 3564/19954 ( 18%)], Train Loss: 0.67270\n",
      "Epoch: 00 [ 3604/19954 ( 18%)], Train Loss: 0.66998\n",
      "Epoch: 00 [ 3644/19954 ( 18%)], Train Loss: 0.66610\n",
      "Epoch: 00 [ 3684/19954 ( 18%)], Train Loss: 0.66323\n",
      "Epoch: 00 [ 3724/19954 ( 19%)], Train Loss: 0.66119\n",
      "Epoch: 00 [ 3764/19954 ( 19%)], Train Loss: 0.65812\n",
      "Epoch: 00 [ 3804/19954 ( 19%)], Train Loss: 0.65431\n",
      "Epoch: 00 [ 3844/19954 ( 19%)], Train Loss: 0.65534\n",
      "Epoch: 00 [ 3884/19954 ( 19%)], Train Loss: 0.65501\n",
      "Epoch: 00 [ 3924/19954 ( 20%)], Train Loss: 0.65275\n",
      "Epoch: 00 [ 3964/19954 ( 20%)], Train Loss: 0.64942\n",
      "Epoch: 00 [ 4004/19954 ( 20%)], Train Loss: 0.64685\n",
      "Epoch: 00 [ 4044/19954 ( 20%)], Train Loss: 0.64701\n",
      "Epoch: 00 [ 4084/19954 ( 20%)], Train Loss: 0.64554\n",
      "Epoch: 00 [ 4124/19954 ( 21%)], Train Loss: 0.64405\n",
      "Epoch: 00 [ 4164/19954 ( 21%)], Train Loss: 0.64379\n",
      "Epoch: 00 [ 4204/19954 ( 21%)], Train Loss: 0.64116\n",
      "Epoch: 00 [ 4244/19954 ( 21%)], Train Loss: 0.64068\n",
      "Epoch: 00 [ 4284/19954 ( 21%)], Train Loss: 0.63968\n",
      "Epoch: 00 [ 4324/19954 ( 22%)], Train Loss: 0.63590\n",
      "Epoch: 00 [ 4364/19954 ( 22%)], Train Loss: 0.63401\n",
      "Epoch: 00 [ 4404/19954 ( 22%)], Train Loss: 0.63288\n",
      "Epoch: 00 [ 4444/19954 ( 22%)], Train Loss: 0.63053\n",
      "Epoch: 00 [ 4484/19954 ( 22%)], Train Loss: 0.62851\n",
      "Epoch: 00 [ 4524/19954 ( 23%)], Train Loss: 0.62828\n",
      "Epoch: 00 [ 4564/19954 ( 23%)], Train Loss: 0.62644\n",
      "Epoch: 00 [ 4604/19954 ( 23%)], Train Loss: 0.62469\n",
      "Epoch: 00 [ 4644/19954 ( 23%)], Train Loss: 0.62327\n",
      "Epoch: 00 [ 4684/19954 ( 23%)], Train Loss: 0.62029\n",
      "Epoch: 00 [ 4724/19954 ( 24%)], Train Loss: 0.61985\n",
      "Epoch: 00 [ 4764/19954 ( 24%)], Train Loss: 0.61906\n",
      "Epoch: 00 [ 4804/19954 ( 24%)], Train Loss: 0.61951\n",
      "Epoch: 00 [ 4844/19954 ( 24%)], Train Loss: 0.61689\n",
      "Epoch: 00 [ 4884/19954 ( 24%)], Train Loss: 0.61588\n",
      "Epoch: 00 [ 4924/19954 ( 25%)], Train Loss: 0.61459\n",
      "Epoch: 00 [ 4964/19954 ( 25%)], Train Loss: 0.61344\n",
      "Epoch: 00 [ 5004/19954 ( 25%)], Train Loss: 0.61025\n",
      "Epoch: 00 [ 5044/19954 ( 25%)], Train Loss: 0.60852\n",
      "Epoch: 00 [ 5084/19954 ( 25%)], Train Loss: 0.60619\n",
      "Epoch: 00 [ 5124/19954 ( 26%)], Train Loss: 0.60334\n",
      "Epoch: 00 [ 5164/19954 ( 26%)], Train Loss: 0.60375\n",
      "Epoch: 00 [ 5204/19954 ( 26%)], Train Loss: 0.60271\n",
      "Epoch: 00 [ 5244/19954 ( 26%)], Train Loss: 0.60168\n",
      "Epoch: 00 [ 5284/19954 ( 26%)], Train Loss: 0.59936\n",
      "Epoch: 00 [ 5324/19954 ( 27%)], Train Loss: 0.59832\n",
      "Epoch: 00 [ 5364/19954 ( 27%)], Train Loss: 0.59578\n",
      "Epoch: 00 [ 5404/19954 ( 27%)], Train Loss: 0.59437\n",
      "Epoch: 00 [ 5444/19954 ( 27%)], Train Loss: 0.59410\n",
      "Epoch: 00 [ 5484/19954 ( 27%)], Train Loss: 0.59244\n",
      "Epoch: 00 [ 5524/19954 ( 28%)], Train Loss: 0.59160\n",
      "Epoch: 00 [ 5564/19954 ( 28%)], Train Loss: 0.59150\n",
      "Epoch: 00 [ 5604/19954 ( 28%)], Train Loss: 0.59095\n",
      "Epoch: 00 [ 5644/19954 ( 28%)], Train Loss: 0.59060\n",
      "Epoch: 00 [ 5684/19954 ( 28%)], Train Loss: 0.58886\n",
      "Epoch: 00 [ 5724/19954 ( 29%)], Train Loss: 0.58697\n",
      "Epoch: 00 [ 5764/19954 ( 29%)], Train Loss: 0.58607\n",
      "Epoch: 00 [ 5804/19954 ( 29%)], Train Loss: 0.58555\n",
      "Epoch: 00 [ 5844/19954 ( 29%)], Train Loss: 0.58570\n",
      "Epoch: 00 [ 5884/19954 ( 29%)], Train Loss: 0.58524\n",
      "Epoch: 00 [ 5924/19954 ( 30%)], Train Loss: 0.58445\n",
      "Epoch: 00 [ 5964/19954 ( 30%)], Train Loss: 0.58433\n",
      "Epoch: 00 [ 6004/19954 ( 30%)], Train Loss: 0.58241\n",
      "Epoch: 00 [ 6044/19954 ( 30%)], Train Loss: 0.58120\n",
      "Epoch: 00 [ 6084/19954 ( 30%)], Train Loss: 0.57936\n",
      "Epoch: 00 [ 6124/19954 ( 31%)], Train Loss: 0.57816\n",
      "Epoch: 00 [ 6164/19954 ( 31%)], Train Loss: 0.57697\n",
      "Epoch: 00 [ 6204/19954 ( 31%)], Train Loss: 0.57689\n",
      "Epoch: 00 [ 6244/19954 ( 31%)], Train Loss: 0.57516\n",
      "Epoch: 00 [ 6284/19954 ( 31%)], Train Loss: 0.57470\n",
      "Epoch: 00 [ 6324/19954 ( 32%)], Train Loss: 0.57319\n",
      "Epoch: 00 [ 6364/19954 ( 32%)], Train Loss: 0.57222\n",
      "Epoch: 00 [ 6404/19954 ( 32%)], Train Loss: 0.57058\n",
      "Epoch: 00 [ 6444/19954 ( 32%)], Train Loss: 0.57082\n",
      "Epoch: 00 [ 6484/19954 ( 32%)], Train Loss: 0.56947\n",
      "Epoch: 00 [ 6524/19954 ( 33%)], Train Loss: 0.56867\n",
      "Epoch: 00 [ 6564/19954 ( 33%)], Train Loss: 0.56812\n",
      "Epoch: 00 [ 6604/19954 ( 33%)], Train Loss: 0.56621\n",
      "Epoch: 00 [ 6644/19954 ( 33%)], Train Loss: 0.56632\n",
      "Epoch: 00 [ 6684/19954 ( 33%)], Train Loss: 0.56504\n",
      "Epoch: 00 [ 6724/19954 ( 34%)], Train Loss: 0.56450\n",
      "Epoch: 00 [ 6764/19954 ( 34%)], Train Loss: 0.56379\n",
      "Epoch: 00 [ 6804/19954 ( 34%)], Train Loss: 0.56252\n",
      "Epoch: 00 [ 6844/19954 ( 34%)], Train Loss: 0.56209\n",
      "Epoch: 00 [ 6884/19954 ( 34%)], Train Loss: 0.56086\n",
      "Epoch: 00 [ 6924/19954 ( 35%)], Train Loss: 0.56045\n",
      "Epoch: 00 [ 6964/19954 ( 35%)], Train Loss: 0.55907\n",
      "Epoch: 00 [ 7004/19954 ( 35%)], Train Loss: 0.55789\n",
      "Epoch: 00 [ 7044/19954 ( 35%)], Train Loss: 0.55659\n",
      "Epoch: 00 [ 7084/19954 ( 36%)], Train Loss: 0.55504\n",
      "Epoch: 00 [ 7124/19954 ( 36%)], Train Loss: 0.55540\n",
      "Epoch: 00 [ 7164/19954 ( 36%)], Train Loss: 0.55568\n",
      "Epoch: 00 [ 7204/19954 ( 36%)], Train Loss: 0.55491\n",
      "Epoch: 00 [ 7244/19954 ( 36%)], Train Loss: 0.55473\n",
      "Epoch: 00 [ 7284/19954 ( 37%)], Train Loss: 0.55471\n",
      "Epoch: 00 [ 7324/19954 ( 37%)], Train Loss: 0.55325\n",
      "Epoch: 00 [ 7364/19954 ( 37%)], Train Loss: 0.55296\n",
      "Epoch: 00 [ 7404/19954 ( 37%)], Train Loss: 0.55219\n",
      "Epoch: 00 [ 7444/19954 ( 37%)], Train Loss: 0.55082\n",
      "Epoch: 00 [ 7484/19954 ( 38%)], Train Loss: 0.54916\n",
      "Epoch: 00 [ 7524/19954 ( 38%)], Train Loss: 0.54887\n",
      "Epoch: 00 [ 7564/19954 ( 38%)], Train Loss: 0.54846\n",
      "Epoch: 00 [ 7604/19954 ( 38%)], Train Loss: 0.54772\n",
      "Epoch: 00 [ 7644/19954 ( 38%)], Train Loss: 0.54622\n",
      "Epoch: 00 [ 7684/19954 ( 39%)], Train Loss: 0.54533\n",
      "Epoch: 00 [ 7724/19954 ( 39%)], Train Loss: 0.54658\n",
      "Epoch: 00 [ 7764/19954 ( 39%)], Train Loss: 0.54674\n",
      "Epoch: 00 [ 7804/19954 ( 39%)], Train Loss: 0.54544\n",
      "Epoch: 00 [ 7844/19954 ( 39%)], Train Loss: 0.54525\n",
      "Epoch: 00 [ 7884/19954 ( 40%)], Train Loss: 0.54470\n",
      "Epoch: 00 [ 7924/19954 ( 40%)], Train Loss: 0.54342\n",
      "Epoch: 00 [ 7964/19954 ( 40%)], Train Loss: 0.54284\n",
      "Epoch: 00 [ 8004/19954 ( 40%)], Train Loss: 0.54138\n",
      "Epoch: 00 [ 8044/19954 ( 40%)], Train Loss: 0.54105\n",
      "Epoch: 00 [ 8084/19954 ( 41%)], Train Loss: 0.53977\n",
      "Epoch: 00 [ 8124/19954 ( 41%)], Train Loss: 0.53912\n",
      "Epoch: 00 [ 8164/19954 ( 41%)], Train Loss: 0.53887\n",
      "Epoch: 00 [ 8204/19954 ( 41%)], Train Loss: 0.53741\n",
      "Epoch: 00 [ 8244/19954 ( 41%)], Train Loss: 0.53697\n",
      "Epoch: 00 [ 8284/19954 ( 42%)], Train Loss: 0.53606\n",
      "Epoch: 00 [ 8324/19954 ( 42%)], Train Loss: 0.53488\n",
      "Epoch: 00 [ 8364/19954 ( 42%)], Train Loss: 0.53396\n",
      "Epoch: 00 [ 8404/19954 ( 42%)], Train Loss: 0.53421\n",
      "Epoch: 00 [ 8444/19954 ( 42%)], Train Loss: 0.53325\n",
      "Epoch: 00 [ 8484/19954 ( 43%)], Train Loss: 0.53236\n",
      "Epoch: 00 [ 8524/19954 ( 43%)], Train Loss: 0.53190\n",
      "Epoch: 00 [ 8564/19954 ( 43%)], Train Loss: 0.53170\n",
      "Epoch: 00 [ 8604/19954 ( 43%)], Train Loss: 0.53052\n",
      "Epoch: 00 [ 8644/19954 ( 43%)], Train Loss: 0.52936\n",
      "Epoch: 00 [ 8684/19954 ( 44%)], Train Loss: 0.52884\n",
      "Epoch: 00 [ 8724/19954 ( 44%)], Train Loss: 0.52900\n",
      "Epoch: 00 [ 8764/19954 ( 44%)], Train Loss: 0.52826\n",
      "Epoch: 00 [ 8804/19954 ( 44%)], Train Loss: 0.52716\n",
      "Epoch: 00 [ 8844/19954 ( 44%)], Train Loss: 0.52603\n",
      "Epoch: 00 [ 8884/19954 ( 45%)], Train Loss: 0.52567\n",
      "Epoch: 00 [ 8924/19954 ( 45%)], Train Loss: 0.52420\n",
      "Epoch: 00 [ 8964/19954 ( 45%)], Train Loss: 0.52299\n",
      "Epoch: 00 [ 9004/19954 ( 45%)], Train Loss: 0.52263\n",
      "Epoch: 00 [ 9044/19954 ( 45%)], Train Loss: 0.52287\n",
      "Epoch: 00 [ 9084/19954 ( 46%)], Train Loss: 0.52240\n",
      "Epoch: 00 [ 9124/19954 ( 46%)], Train Loss: 0.52153\n",
      "Epoch: 00 [ 9164/19954 ( 46%)], Train Loss: 0.52037\n",
      "Epoch: 00 [ 9204/19954 ( 46%)], Train Loss: 0.52019\n",
      "Epoch: 00 [ 9244/19954 ( 46%)], Train Loss: 0.51999\n",
      "Epoch: 00 [ 9284/19954 ( 47%)], Train Loss: 0.51928\n",
      "Epoch: 00 [ 9324/19954 ( 47%)], Train Loss: 0.51817\n",
      "Epoch: 00 [ 9364/19954 ( 47%)], Train Loss: 0.51794\n",
      "Epoch: 00 [ 9404/19954 ( 47%)], Train Loss: 0.51775\n",
      "Epoch: 00 [ 9444/19954 ( 47%)], Train Loss: 0.51760\n",
      "Epoch: 00 [ 9484/19954 ( 48%)], Train Loss: 0.51720\n",
      "Epoch: 00 [ 9524/19954 ( 48%)], Train Loss: 0.51613\n",
      "Epoch: 00 [ 9564/19954 ( 48%)], Train Loss: 0.51457\n",
      "Epoch: 00 [ 9604/19954 ( 48%)], Train Loss: 0.51420\n",
      "Epoch: 00 [ 9644/19954 ( 48%)], Train Loss: 0.51338\n",
      "Epoch: 00 [ 9684/19954 ( 49%)], Train Loss: 0.51246\n",
      "Epoch: 00 [ 9724/19954 ( 49%)], Train Loss: 0.51211\n",
      "Epoch: 00 [ 9764/19954 ( 49%)], Train Loss: 0.51134\n",
      "Epoch: 00 [ 9804/19954 ( 49%)], Train Loss: 0.51067\n",
      "Epoch: 00 [ 9844/19954 ( 49%)], Train Loss: 0.51018\n",
      "Epoch: 00 [ 9884/19954 ( 50%)], Train Loss: 0.50940\n",
      "Epoch: 00 [ 9924/19954 ( 50%)], Train Loss: 0.50843\n",
      "Epoch: 00 [ 9964/19954 ( 50%)], Train Loss: 0.50721\n",
      "Epoch: 00 [10004/19954 ( 50%)], Train Loss: 0.50740\n",
      "Epoch: 00 [10044/19954 ( 50%)], Train Loss: 0.50665\n",
      "Epoch: 00 [10084/19954 ( 51%)], Train Loss: 0.50634\n",
      "Epoch: 00 [10124/19954 ( 51%)], Train Loss: 0.50562\n",
      "Epoch: 00 [10164/19954 ( 51%)], Train Loss: 0.50587\n",
      "Epoch: 00 [10204/19954 ( 51%)], Train Loss: 0.50651\n",
      "Epoch: 00 [10244/19954 ( 51%)], Train Loss: 0.50618\n",
      "Epoch: 00 [10284/19954 ( 52%)], Train Loss: 0.50500\n",
      "Epoch: 00 [10324/19954 ( 52%)], Train Loss: 0.50444\n",
      "Epoch: 00 [10364/19954 ( 52%)], Train Loss: 0.50422\n",
      "Epoch: 00 [10404/19954 ( 52%)], Train Loss: 0.50387\n",
      "Epoch: 00 [10444/19954 ( 52%)], Train Loss: 0.50393\n",
      "Epoch: 00 [10484/19954 ( 53%)], Train Loss: 0.50413\n",
      "Epoch: 00 [10524/19954 ( 53%)], Train Loss: 0.50385\n",
      "Epoch: 00 [10564/19954 ( 53%)], Train Loss: 0.50293\n",
      "Epoch: 00 [10604/19954 ( 53%)], Train Loss: 0.50299\n",
      "Epoch: 00 [10644/19954 ( 53%)], Train Loss: 0.50227\n",
      "Epoch: 00 [10684/19954 ( 54%)], Train Loss: 0.50132\n",
      "Epoch: 00 [10724/19954 ( 54%)], Train Loss: 0.50098\n",
      "Epoch: 00 [10764/19954 ( 54%)], Train Loss: 0.50029\n",
      "Epoch: 00 [10804/19954 ( 54%)], Train Loss: 0.50026\n",
      "Epoch: 00 [10844/19954 ( 54%)], Train Loss: 0.49945\n",
      "Epoch: 00 [10884/19954 ( 55%)], Train Loss: 0.49974\n",
      "Epoch: 00 [10924/19954 ( 55%)], Train Loss: 0.49901\n",
      "Epoch: 00 [10964/19954 ( 55%)], Train Loss: 0.49812\n",
      "Epoch: 00 [11004/19954 ( 55%)], Train Loss: 0.49837\n",
      "Epoch: 00 [11044/19954 ( 55%)], Train Loss: 0.49797\n",
      "Epoch: 00 [11084/19954 ( 56%)], Train Loss: 0.49775\n",
      "Epoch: 00 [11124/19954 ( 56%)], Train Loss: 0.49732\n",
      "Epoch: 00 [11164/19954 ( 56%)], Train Loss: 0.49632\n",
      "Epoch: 00 [11204/19954 ( 56%)], Train Loss: 0.49596\n",
      "Epoch: 00 [11244/19954 ( 56%)], Train Loss: 0.49529\n",
      "Epoch: 00 [11284/19954 ( 57%)], Train Loss: 0.49445\n",
      "Epoch: 00 [11324/19954 ( 57%)], Train Loss: 0.49386\n",
      "Epoch: 00 [11364/19954 ( 57%)], Train Loss: 0.49341\n",
      "Epoch: 00 [11404/19954 ( 57%)], Train Loss: 0.49304\n",
      "Epoch: 00 [11444/19954 ( 57%)], Train Loss: 0.49221\n",
      "Epoch: 00 [11484/19954 ( 58%)], Train Loss: 0.49128\n",
      "Epoch: 00 [11524/19954 ( 58%)], Train Loss: 0.49110\n",
      "Epoch: 00 [11564/19954 ( 58%)], Train Loss: 0.49050\n",
      "Epoch: 00 [11604/19954 ( 58%)], Train Loss: 0.49081\n",
      "Epoch: 00 [11644/19954 ( 58%)], Train Loss: 0.49021\n",
      "Epoch: 00 [11684/19954 ( 59%)], Train Loss: 0.48991\n",
      "Epoch: 00 [11724/19954 ( 59%)], Train Loss: 0.48962\n",
      "Epoch: 00 [11764/19954 ( 59%)], Train Loss: 0.48906\n",
      "Epoch: 00 [11804/19954 ( 59%)], Train Loss: 0.48819\n",
      "Epoch: 00 [11844/19954 ( 59%)], Train Loss: 0.48717\n",
      "Epoch: 00 [11884/19954 ( 60%)], Train Loss: 0.48717\n",
      "Epoch: 00 [11924/19954 ( 60%)], Train Loss: 0.48684\n",
      "Epoch: 00 [11964/19954 ( 60%)], Train Loss: 0.48621\n",
      "Epoch: 00 [12004/19954 ( 60%)], Train Loss: 0.48584\n",
      "Epoch: 00 [12044/19954 ( 60%)], Train Loss: 0.48570\n",
      "Epoch: 00 [12084/19954 ( 61%)], Train Loss: 0.48554\n",
      "Epoch: 00 [12124/19954 ( 61%)], Train Loss: 0.48571\n",
      "Epoch: 00 [12164/19954 ( 61%)], Train Loss: 0.48495\n",
      "Epoch: 00 [12204/19954 ( 61%)], Train Loss: 0.48485\n",
      "Epoch: 00 [12244/19954 ( 61%)], Train Loss: 0.48441\n",
      "Epoch: 00 [12284/19954 ( 62%)], Train Loss: 0.48422\n",
      "Epoch: 00 [12324/19954 ( 62%)], Train Loss: 0.48347\n",
      "Epoch: 00 [12364/19954 ( 62%)], Train Loss: 0.48336\n",
      "Epoch: 00 [12404/19954 ( 62%)], Train Loss: 0.48257\n",
      "Epoch: 00 [12444/19954 ( 62%)], Train Loss: 0.48291\n",
      "Epoch: 00 [12484/19954 ( 63%)], Train Loss: 0.48271\n",
      "Epoch: 00 [12524/19954 ( 63%)], Train Loss: 0.48242\n",
      "Epoch: 00 [12564/19954 ( 63%)], Train Loss: 0.48180\n",
      "Epoch: 00 [12604/19954 ( 63%)], Train Loss: 0.48098\n",
      "Epoch: 00 [12644/19954 ( 63%)], Train Loss: 0.48033\n",
      "Epoch: 00 [12684/19954 ( 64%)], Train Loss: 0.47954\n",
      "Epoch: 00 [12724/19954 ( 64%)], Train Loss: 0.47917\n",
      "Epoch: 00 [12764/19954 ( 64%)], Train Loss: 0.47959\n",
      "Epoch: 00 [12804/19954 ( 64%)], Train Loss: 0.47964\n",
      "Epoch: 00 [12844/19954 ( 64%)], Train Loss: 0.47883\n",
      "Epoch: 00 [12884/19954 ( 65%)], Train Loss: 0.47813\n",
      "Epoch: 00 [12924/19954 ( 65%)], Train Loss: 0.47736\n",
      "Epoch: 00 [12964/19954 ( 65%)], Train Loss: 0.47715\n",
      "Epoch: 00 [13004/19954 ( 65%)], Train Loss: 0.47643\n",
      "Epoch: 00 [13044/19954 ( 65%)], Train Loss: 0.47568\n",
      "Epoch: 00 [13084/19954 ( 66%)], Train Loss: 0.47520\n",
      "Epoch: 00 [13124/19954 ( 66%)], Train Loss: 0.47514\n",
      "Epoch: 00 [13164/19954 ( 66%)], Train Loss: 0.47475\n",
      "Epoch: 00 [13204/19954 ( 66%)], Train Loss: 0.47455\n",
      "Epoch: 00 [13244/19954 ( 66%)], Train Loss: 0.47381\n",
      "Epoch: 00 [13284/19954 ( 67%)], Train Loss: 0.47344\n",
      "Epoch: 00 [13324/19954 ( 67%)], Train Loss: 0.47294\n",
      "Epoch: 00 [13364/19954 ( 67%)], Train Loss: 0.47260\n",
      "Epoch: 00 [13404/19954 ( 67%)], Train Loss: 0.47222\n",
      "Epoch: 00 [13444/19954 ( 67%)], Train Loss: 0.47209\n",
      "Epoch: 00 [13484/19954 ( 68%)], Train Loss: 0.47163\n",
      "Epoch: 00 [13524/19954 ( 68%)], Train Loss: 0.47133\n",
      "Epoch: 00 [13564/19954 ( 68%)], Train Loss: 0.47186\n",
      "Epoch: 00 [13604/19954 ( 68%)], Train Loss: 0.47240\n",
      "Epoch: 00 [13644/19954 ( 68%)], Train Loss: 0.47205\n",
      "Epoch: 00 [13684/19954 ( 69%)], Train Loss: 0.47173\n",
      "Epoch: 00 [13724/19954 ( 69%)], Train Loss: 0.47107\n",
      "Epoch: 00 [13764/19954 ( 69%)], Train Loss: 0.47076\n",
      "Epoch: 00 [13804/19954 ( 69%)], Train Loss: 0.47034\n",
      "Epoch: 00 [13844/19954 ( 69%)], Train Loss: 0.46959\n",
      "Epoch: 00 [13884/19954 ( 70%)], Train Loss: 0.46961\n",
      "Epoch: 00 [13924/19954 ( 70%)], Train Loss: 0.46902\n",
      "Epoch: 00 [13964/19954 ( 70%)], Train Loss: 0.46868\n",
      "Epoch: 00 [14004/19954 ( 70%)], Train Loss: 0.46902\n",
      "Epoch: 00 [14044/19954 ( 70%)], Train Loss: 0.46899\n",
      "Epoch: 00 [14084/19954 ( 71%)], Train Loss: 0.46912\n",
      "Epoch: 00 [14124/19954 ( 71%)], Train Loss: 0.46896\n",
      "Epoch: 00 [14164/19954 ( 71%)], Train Loss: 0.46807\n",
      "Epoch: 00 [14204/19954 ( 71%)], Train Loss: 0.46776\n",
      "Epoch: 00 [14244/19954 ( 71%)], Train Loss: 0.46753\n",
      "Epoch: 00 [14284/19954 ( 72%)], Train Loss: 0.46724\n",
      "Epoch: 00 [14324/19954 ( 72%)], Train Loss: 0.46661\n",
      "Epoch: 00 [14364/19954 ( 72%)], Train Loss: 0.46647\n",
      "Epoch: 00 [14404/19954 ( 72%)], Train Loss: 0.46599\n",
      "Epoch: 00 [14444/19954 ( 72%)], Train Loss: 0.46538\n",
      "Epoch: 00 [14484/19954 ( 73%)], Train Loss: 0.46519\n",
      "Epoch: 00 [14524/19954 ( 73%)], Train Loss: 0.46458\n",
      "Epoch: 00 [14564/19954 ( 73%)], Train Loss: 0.46460\n",
      "Epoch: 00 [14604/19954 ( 73%)], Train Loss: 0.46415\n",
      "Epoch: 00 [14644/19954 ( 73%)], Train Loss: 0.46433\n",
      "Epoch: 00 [14684/19954 ( 74%)], Train Loss: 0.46374\n",
      "Epoch: 00 [14724/19954 ( 74%)], Train Loss: 0.46296\n",
      "Epoch: 00 [14764/19954 ( 74%)], Train Loss: 0.46247\n",
      "Epoch: 00 [14804/19954 ( 74%)], Train Loss: 0.46179\n",
      "Epoch: 00 [14844/19954 ( 74%)], Train Loss: 0.46134\n",
      "Epoch: 00 [14884/19954 ( 75%)], Train Loss: 0.46084\n",
      "Epoch: 00 [14924/19954 ( 75%)], Train Loss: 0.46086\n",
      "Epoch: 00 [14964/19954 ( 75%)], Train Loss: 0.46027\n",
      "Epoch: 00 [15004/19954 ( 75%)], Train Loss: 0.46023\n",
      "Epoch: 00 [15044/19954 ( 75%)], Train Loss: 0.45971\n",
      "Epoch: 00 [15084/19954 ( 76%)], Train Loss: 0.45940\n",
      "Epoch: 00 [15124/19954 ( 76%)], Train Loss: 0.45923\n",
      "Epoch: 00 [15164/19954 ( 76%)], Train Loss: 0.45864\n",
      "Epoch: 00 [15204/19954 ( 76%)], Train Loss: 0.45842\n",
      "Epoch: 00 [15244/19954 ( 76%)], Train Loss: 0.45850\n",
      "Epoch: 00 [15284/19954 ( 77%)], Train Loss: 0.45826\n",
      "Epoch: 00 [15324/19954 ( 77%)], Train Loss: 0.45789\n",
      "Epoch: 00 [15364/19954 ( 77%)], Train Loss: 0.45732\n",
      "Epoch: 00 [15404/19954 ( 77%)], Train Loss: 0.45673\n",
      "Epoch: 00 [15444/19954 ( 77%)], Train Loss: 0.45684\n",
      "Epoch: 00 [15484/19954 ( 78%)], Train Loss: 0.45699\n",
      "Epoch: 00 [15524/19954 ( 78%)], Train Loss: 0.45646\n",
      "Epoch: 00 [15564/19954 ( 78%)], Train Loss: 0.45600\n",
      "Epoch: 00 [15604/19954 ( 78%)], Train Loss: 0.45585\n",
      "Epoch: 00 [15644/19954 ( 78%)], Train Loss: 0.45509\n",
      "Epoch: 00 [15684/19954 ( 79%)], Train Loss: 0.45501\n",
      "Epoch: 00 [15724/19954 ( 79%)], Train Loss: 0.45480\n",
      "Epoch: 00 [15764/19954 ( 79%)], Train Loss: 0.45414\n",
      "Epoch: 00 [15804/19954 ( 79%)], Train Loss: 0.45374\n",
      "Epoch: 00 [15844/19954 ( 79%)], Train Loss: 0.45313\n",
      "Epoch: 00 [15884/19954 ( 80%)], Train Loss: 0.45279\n",
      "Epoch: 00 [15924/19954 ( 80%)], Train Loss: 0.45218\n",
      "Epoch: 00 [15964/19954 ( 80%)], Train Loss: 0.45173\n",
      "Epoch: 00 [16004/19954 ( 80%)], Train Loss: 0.45164\n",
      "Epoch: 00 [16044/19954 ( 80%)], Train Loss: 0.45185\n",
      "Epoch: 00 [16084/19954 ( 81%)], Train Loss: 0.45118\n",
      "Epoch: 00 [16124/19954 ( 81%)], Train Loss: 0.45121\n",
      "Epoch: 00 [16164/19954 ( 81%)], Train Loss: 0.45128\n",
      "Epoch: 00 [16204/19954 ( 81%)], Train Loss: 0.45126\n",
      "Epoch: 00 [16244/19954 ( 81%)], Train Loss: 0.45092\n",
      "Epoch: 00 [16284/19954 ( 82%)], Train Loss: 0.45048\n",
      "Epoch: 00 [16324/19954 ( 82%)], Train Loss: 0.45052\n",
      "Epoch: 00 [16364/19954 ( 82%)], Train Loss: 0.45021\n",
      "Epoch: 00 [16404/19954 ( 82%)], Train Loss: 0.44992\n",
      "Epoch: 00 [16444/19954 ( 82%)], Train Loss: 0.44922\n",
      "Epoch: 00 [16484/19954 ( 83%)], Train Loss: 0.44924\n",
      "Epoch: 00 [16524/19954 ( 83%)], Train Loss: 0.44904\n",
      "Epoch: 00 [16564/19954 ( 83%)], Train Loss: 0.44826\n",
      "Epoch: 00 [16604/19954 ( 83%)], Train Loss: 0.44807\n",
      "Epoch: 00 [16644/19954 ( 83%)], Train Loss: 0.44760\n",
      "Epoch: 00 [16684/19954 ( 84%)], Train Loss: 0.44710\n",
      "Epoch: 00 [16724/19954 ( 84%)], Train Loss: 0.44692\n",
      "Epoch: 00 [16764/19954 ( 84%)], Train Loss: 0.44625\n",
      "Epoch: 00 [16804/19954 ( 84%)], Train Loss: 0.44610\n",
      "Epoch: 00 [16844/19954 ( 84%)], Train Loss: 0.44537\n",
      "Epoch: 00 [16884/19954 ( 85%)], Train Loss: 0.44532\n",
      "Epoch: 00 [16924/19954 ( 85%)], Train Loss: 0.44526\n",
      "Epoch: 00 [16964/19954 ( 85%)], Train Loss: 0.44509\n",
      "Epoch: 00 [17004/19954 ( 85%)], Train Loss: 0.44477\n",
      "Epoch: 00 [17044/19954 ( 85%)], Train Loss: 0.44452\n",
      "Epoch: 00 [17084/19954 ( 86%)], Train Loss: 0.44403\n",
      "Epoch: 00 [17124/19954 ( 86%)], Train Loss: 0.44365\n",
      "Epoch: 00 [17164/19954 ( 86%)], Train Loss: 0.44318\n",
      "Epoch: 00 [17204/19954 ( 86%)], Train Loss: 0.44262\n",
      "Epoch: 00 [17244/19954 ( 86%)], Train Loss: 0.44209\n",
      "Epoch: 00 [17284/19954 ( 87%)], Train Loss: 0.44198\n",
      "Epoch: 00 [17324/19954 ( 87%)], Train Loss: 0.44199\n",
      "Epoch: 00 [17364/19954 ( 87%)], Train Loss: 0.44163\n",
      "Epoch: 00 [17404/19954 ( 87%)], Train Loss: 0.44125\n",
      "Epoch: 00 [17444/19954 ( 87%)], Train Loss: 0.44080\n",
      "Epoch: 00 [17484/19954 ( 88%)], Train Loss: 0.44021\n",
      "Epoch: 00 [17524/19954 ( 88%)], Train Loss: 0.44013\n",
      "Epoch: 00 [17564/19954 ( 88%)], Train Loss: 0.43969\n",
      "Epoch: 00 [17604/19954 ( 88%)], Train Loss: 0.43946\n",
      "Epoch: 00 [17644/19954 ( 88%)], Train Loss: 0.43935\n",
      "Epoch: 00 [17684/19954 ( 89%)], Train Loss: 0.43953\n",
      "Epoch: 00 [17724/19954 ( 89%)], Train Loss: 0.43918\n",
      "Epoch: 00 [17764/19954 ( 89%)], Train Loss: 0.43913\n",
      "Epoch: 00 [17804/19954 ( 89%)], Train Loss: 0.43887\n",
      "Epoch: 00 [17844/19954 ( 89%)], Train Loss: 0.43874\n",
      "Epoch: 00 [17884/19954 ( 90%)], Train Loss: 0.43895\n",
      "Epoch: 00 [17924/19954 ( 90%)], Train Loss: 0.43900\n",
      "Epoch: 00 [17964/19954 ( 90%)], Train Loss: 0.43879\n",
      "Epoch: 00 [18004/19954 ( 90%)], Train Loss: 0.43860\n",
      "Epoch: 00 [18044/19954 ( 90%)], Train Loss: 0.43843\n",
      "Epoch: 00 [18084/19954 ( 91%)], Train Loss: 0.43807\n",
      "Epoch: 00 [18124/19954 ( 91%)], Train Loss: 0.43754\n",
      "Epoch: 00 [18164/19954 ( 91%)], Train Loss: 0.43711\n",
      "Epoch: 00 [18204/19954 ( 91%)], Train Loss: 0.43687\n",
      "Epoch: 00 [18244/19954 ( 91%)], Train Loss: 0.43657\n",
      "Epoch: 00 [18284/19954 ( 92%)], Train Loss: 0.43634\n",
      "Epoch: 00 [18324/19954 ( 92%)], Train Loss: 0.43636\n",
      "Epoch: 00 [18364/19954 ( 92%)], Train Loss: 0.43577\n",
      "Epoch: 00 [18404/19954 ( 92%)], Train Loss: 0.43546\n",
      "Epoch: 00 [18444/19954 ( 92%)], Train Loss: 0.43531\n",
      "Epoch: 00 [18484/19954 ( 93%)], Train Loss: 0.43536\n",
      "Epoch: 00 [18524/19954 ( 93%)], Train Loss: 0.43522\n",
      "Epoch: 00 [18564/19954 ( 93%)], Train Loss: 0.43484\n",
      "Epoch: 00 [18604/19954 ( 93%)], Train Loss: 0.43458\n",
      "Epoch: 00 [18644/19954 ( 93%)], Train Loss: 0.43433\n",
      "Epoch: 00 [18684/19954 ( 94%)], Train Loss: 0.43408\n",
      "Epoch: 00 [18724/19954 ( 94%)], Train Loss: 0.43390\n",
      "Epoch: 00 [18764/19954 ( 94%)], Train Loss: 0.43347\n",
      "Epoch: 00 [18804/19954 ( 94%)], Train Loss: 0.43348\n",
      "Epoch: 00 [18844/19954 ( 94%)], Train Loss: 0.43288\n",
      "Epoch: 00 [18884/19954 ( 95%)], Train Loss: 0.43270\n",
      "Epoch: 00 [18924/19954 ( 95%)], Train Loss: 0.43225\n",
      "Epoch: 00 [18964/19954 ( 95%)], Train Loss: 0.43189\n",
      "Epoch: 00 [19004/19954 ( 95%)], Train Loss: 0.43157\n",
      "Epoch: 00 [19044/19954 ( 95%)], Train Loss: 0.43133\n",
      "Epoch: 00 [19084/19954 ( 96%)], Train Loss: 0.43129\n",
      "Epoch: 00 [19124/19954 ( 96%)], Train Loss: 0.43157\n",
      "Epoch: 00 [19164/19954 ( 96%)], Train Loss: 0.43127\n",
      "Epoch: 00 [19204/19954 ( 96%)], Train Loss: 0.43063\n",
      "Epoch: 00 [19244/19954 ( 96%)], Train Loss: 0.43045\n",
      "Epoch: 00 [19284/19954 ( 97%)], Train Loss: 0.43016\n",
      "Epoch: 00 [19324/19954 ( 97%)], Train Loss: 0.43002\n",
      "Epoch: 00 [19364/19954 ( 97%)], Train Loss: 0.43004\n",
      "Epoch: 00 [19404/19954 ( 97%)], Train Loss: 0.42964\n",
      "Epoch: 00 [19444/19954 ( 97%)], Train Loss: 0.42934\n",
      "Epoch: 00 [19484/19954 ( 98%)], Train Loss: 0.42905\n",
      "Epoch: 00 [19524/19954 ( 98%)], Train Loss: 0.42919\n",
      "Epoch: 00 [19564/19954 ( 98%)], Train Loss: 0.42895\n",
      "Epoch: 00 [19604/19954 ( 98%)], Train Loss: 0.42898\n",
      "Epoch: 00 [19644/19954 ( 98%)], Train Loss: 0.42865\n",
      "Epoch: 00 [19684/19954 ( 99%)], Train Loss: 0.42821\n",
      "Epoch: 00 [19724/19954 ( 99%)], Train Loss: 0.42777\n",
      "Epoch: 00 [19764/19954 ( 99%)], Train Loss: 0.42776\n",
      "Epoch: 00 [19804/19954 ( 99%)], Train Loss: 0.42764\n",
      "Epoch: 00 [19844/19954 ( 99%)], Train Loss: 0.42733\n",
      "Epoch: 00 [19884/19954 (100%)], Train Loss: 0.42706\n",
      "Epoch: 00 [19924/19954 (100%)], Train Loss: 0.42669\n",
      "Epoch: 00 [19954/19954 (100%)], Train Loss: 0.42684\n",
      "----Validation Results Summary----\n",
      "Epoch: [0] Valid Loss: 0.19375\n",
      "Post-processing 223 example predictions split into 3115 features.\n",
      "valid jaccard:  0.6956278026905831\n",
      "0 Epoch, Best epoch was updated! Valid Loss: 0.19375\n",
      "Saving model checkpoint to output/checkpoint-fold-2-epoch-0.\n",
      "\n",
      "Total Training Time: 2847.860485315323secs, Average Training Time per Epoch: 2847.860485315323secs.\n",
      "Total Validation Time: 189.12860655784607secs, Average Validation Time per Epoch: 189.12860655784607secs.\n",
      "\n",
      "\n",
      "--------------------------------------------------\n",
      "FOLD: 3\n",
      "--------------------------------------------------\n",
      "Model pushed to 1 GPU(s), type Tesla P100-PCIE-16GB.\n",
      "Num examples Train= 20158, Num examples Valid=2911\n",
      "Total Training Steps: 2520, Total Warmup Steps: 252\n",
      "Epoch: 00 [    4/20158 (  0%)], Train Loss: 2.98554\n",
      "Epoch: 00 [   44/20158 (  0%)], Train Loss: 3.08577\n",
      "Epoch: 00 [   84/20158 (  0%)], Train Loss: 3.05020\n",
      "Epoch: 00 [  124/20158 (  1%)], Train Loss: 2.99881\n",
      "Epoch: 00 [  164/20158 (  1%)], Train Loss: 2.92767\n",
      "Epoch: 00 [  204/20158 (  1%)], Train Loss: 2.85243\n",
      "Epoch: 00 [  244/20158 (  1%)], Train Loss: 2.74538\n",
      "Epoch: 00 [  284/20158 (  1%)], Train Loss: 2.63688\n",
      "Epoch: 00 [  324/20158 (  2%)], Train Loss: 2.47520\n",
      "Epoch: 00 [  364/20158 (  2%)], Train Loss: 2.32125\n",
      "Epoch: 00 [  404/20158 (  2%)], Train Loss: 2.16641\n",
      "Epoch: 00 [  444/20158 (  2%)], Train Loss: 2.01418\n",
      "Epoch: 00 [  484/20158 (  2%)], Train Loss: 1.92596\n",
      "Epoch: 00 [  524/20158 (  3%)], Train Loss: 1.83158\n",
      "Epoch: 00 [  564/20158 (  3%)], Train Loss: 1.75694\n",
      "Epoch: 00 [  604/20158 (  3%)], Train Loss: 1.68101\n",
      "Epoch: 00 [  644/20158 (  3%)], Train Loss: 1.59860\n",
      "Epoch: 00 [  684/20158 (  3%)], Train Loss: 1.55572\n",
      "Epoch: 00 [  724/20158 (  4%)], Train Loss: 1.48625\n",
      "Epoch: 00 [  764/20158 (  4%)], Train Loss: 1.43712\n",
      "Epoch: 00 [  804/20158 (  4%)], Train Loss: 1.38672\n",
      "Epoch: 00 [  844/20158 (  4%)], Train Loss: 1.34370\n",
      "Epoch: 00 [  884/20158 (  4%)], Train Loss: 1.31174\n",
      "Epoch: 00 [  924/20158 (  5%)], Train Loss: 1.27130\n",
      "Epoch: 00 [  964/20158 (  5%)], Train Loss: 1.22545\n",
      "Epoch: 00 [ 1004/20158 (  5%)], Train Loss: 1.19502\n",
      "Epoch: 00 [ 1044/20158 (  5%)], Train Loss: 1.17161\n",
      "Epoch: 00 [ 1084/20158 (  5%)], Train Loss: 1.15475\n",
      "Epoch: 00 [ 1124/20158 (  6%)], Train Loss: 1.13448\n",
      "Epoch: 00 [ 1164/20158 (  6%)], Train Loss: 1.12124\n",
      "Epoch: 00 [ 1204/20158 (  6%)], Train Loss: 1.09346\n",
      "Epoch: 00 [ 1244/20158 (  6%)], Train Loss: 1.07509\n",
      "Epoch: 00 [ 1284/20158 (  6%)], Train Loss: 1.05322\n",
      "Epoch: 00 [ 1324/20158 (  7%)], Train Loss: 1.03363\n",
      "Epoch: 00 [ 1364/20158 (  7%)], Train Loss: 1.01754\n",
      "Epoch: 00 [ 1404/20158 (  7%)], Train Loss: 0.99935\n",
      "Epoch: 00 [ 1444/20158 (  7%)], Train Loss: 0.98769\n",
      "Epoch: 00 [ 1484/20158 (  7%)], Train Loss: 0.97715\n",
      "Epoch: 00 [ 1524/20158 (  8%)], Train Loss: 0.96455\n",
      "Epoch: 00 [ 1564/20158 (  8%)], Train Loss: 0.95198\n",
      "Epoch: 00 [ 1604/20158 (  8%)], Train Loss: 0.94297\n",
      "Epoch: 00 [ 1644/20158 (  8%)], Train Loss: 0.92821\n",
      "Epoch: 00 [ 1684/20158 (  8%)], Train Loss: 0.91727\n",
      "Epoch: 00 [ 1724/20158 (  9%)], Train Loss: 0.90223\n",
      "Epoch: 00 [ 1764/20158 (  9%)], Train Loss: 0.89312\n",
      "Epoch: 00 [ 1804/20158 (  9%)], Train Loss: 0.88088\n",
      "Epoch: 00 [ 1844/20158 (  9%)], Train Loss: 0.87265\n",
      "Epoch: 00 [ 1884/20158 (  9%)], Train Loss: 0.86386\n",
      "Epoch: 00 [ 1924/20158 ( 10%)], Train Loss: 0.85719\n",
      "Epoch: 00 [ 1964/20158 ( 10%)], Train Loss: 0.84623\n",
      "Epoch: 00 [ 2004/20158 ( 10%)], Train Loss: 0.83825\n",
      "Epoch: 00 [ 2044/20158 ( 10%)], Train Loss: 0.83212\n",
      "Epoch: 00 [ 2084/20158 ( 10%)], Train Loss: 0.83048\n",
      "Epoch: 00 [ 2124/20158 ( 11%)], Train Loss: 0.82582\n",
      "Epoch: 00 [ 2164/20158 ( 11%)], Train Loss: 0.82126\n",
      "Epoch: 00 [ 2204/20158 ( 11%)], Train Loss: 0.81337\n",
      "Epoch: 00 [ 2244/20158 ( 11%)], Train Loss: 0.80517\n",
      "Epoch: 00 [ 2284/20158 ( 11%)], Train Loss: 0.80340\n",
      "Epoch: 00 [ 2324/20158 ( 12%)], Train Loss: 0.79940\n",
      "Epoch: 00 [ 2364/20158 ( 12%)], Train Loss: 0.79056\n",
      "Epoch: 00 [ 2404/20158 ( 12%)], Train Loss: 0.78656\n",
      "Epoch: 00 [ 2444/20158 ( 12%)], Train Loss: 0.77972\n",
      "Epoch: 00 [ 2484/20158 ( 12%)], Train Loss: 0.77252\n",
      "Epoch: 00 [ 2524/20158 ( 13%)], Train Loss: 0.77095\n",
      "Epoch: 00 [ 2564/20158 ( 13%)], Train Loss: 0.76755\n",
      "Epoch: 00 [ 2604/20158 ( 13%)], Train Loss: 0.76477\n",
      "Epoch: 00 [ 2644/20158 ( 13%)], Train Loss: 0.76165\n",
      "Epoch: 00 [ 2684/20158 ( 13%)], Train Loss: 0.75588\n",
      "Epoch: 00 [ 2724/20158 ( 14%)], Train Loss: 0.75237\n",
      "Epoch: 00 [ 2764/20158 ( 14%)], Train Loss: 0.74712\n",
      "Epoch: 00 [ 2804/20158 ( 14%)], Train Loss: 0.74464\n",
      "Epoch: 00 [ 2844/20158 ( 14%)], Train Loss: 0.73890\n",
      "Epoch: 00 [ 2884/20158 ( 14%)], Train Loss: 0.73567\n",
      "Epoch: 00 [ 2924/20158 ( 15%)], Train Loss: 0.73150\n",
      "Epoch: 00 [ 2964/20158 ( 15%)], Train Loss: 0.73021\n",
      "Epoch: 00 [ 3004/20158 ( 15%)], Train Loss: 0.72489\n",
      "Epoch: 00 [ 3044/20158 ( 15%)], Train Loss: 0.72137\n",
      "Epoch: 00 [ 3084/20158 ( 15%)], Train Loss: 0.71926\n",
      "Epoch: 00 [ 3124/20158 ( 15%)], Train Loss: 0.71662\n",
      "Epoch: 00 [ 3164/20158 ( 16%)], Train Loss: 0.71185\n",
      "Epoch: 00 [ 3204/20158 ( 16%)], Train Loss: 0.70680\n",
      "Epoch: 00 [ 3244/20158 ( 16%)], Train Loss: 0.70545\n",
      "Epoch: 00 [ 3284/20158 ( 16%)], Train Loss: 0.70206\n",
      "Epoch: 00 [ 3324/20158 ( 16%)], Train Loss: 0.69978\n",
      "Epoch: 00 [ 3364/20158 ( 17%)], Train Loss: 0.69524\n",
      "Epoch: 00 [ 3404/20158 ( 17%)], Train Loss: 0.69384\n",
      "Epoch: 00 [ 3444/20158 ( 17%)], Train Loss: 0.68901\n",
      "Epoch: 00 [ 3484/20158 ( 17%)], Train Loss: 0.68698\n",
      "Epoch: 00 [ 3524/20158 ( 17%)], Train Loss: 0.68608\n",
      "Epoch: 00 [ 3564/20158 ( 18%)], Train Loss: 0.68346\n",
      "Epoch: 00 [ 3604/20158 ( 18%)], Train Loss: 0.68549\n",
      "Epoch: 00 [ 3644/20158 ( 18%)], Train Loss: 0.68414\n",
      "Epoch: 00 [ 3684/20158 ( 18%)], Train Loss: 0.68177\n",
      "Epoch: 00 [ 3724/20158 ( 18%)], Train Loss: 0.68011\n",
      "Epoch: 00 [ 3764/20158 ( 19%)], Train Loss: 0.67705\n",
      "Epoch: 00 [ 3804/20158 ( 19%)], Train Loss: 0.67431\n",
      "Epoch: 00 [ 3844/20158 ( 19%)], Train Loss: 0.67232\n",
      "Epoch: 00 [ 3884/20158 ( 19%)], Train Loss: 0.66995\n",
      "Epoch: 00 [ 3924/20158 ( 19%)], Train Loss: 0.66950\n",
      "Epoch: 00 [ 3964/20158 ( 20%)], Train Loss: 0.66614\n",
      "Epoch: 00 [ 4004/20158 ( 20%)], Train Loss: 0.66387\n",
      "Epoch: 00 [ 4044/20158 ( 20%)], Train Loss: 0.66235\n",
      "Epoch: 00 [ 4084/20158 ( 20%)], Train Loss: 0.66101\n",
      "Epoch: 00 [ 4124/20158 ( 20%)], Train Loss: 0.65799\n",
      "Epoch: 00 [ 4164/20158 ( 21%)], Train Loss: 0.65603\n",
      "Epoch: 00 [ 4204/20158 ( 21%)], Train Loss: 0.65332\n",
      "Epoch: 00 [ 4244/20158 ( 21%)], Train Loss: 0.64951\n",
      "Epoch: 00 [ 4284/20158 ( 21%)], Train Loss: 0.64778\n",
      "Epoch: 00 [ 4324/20158 ( 21%)], Train Loss: 0.64420\n",
      "Epoch: 00 [ 4364/20158 ( 22%)], Train Loss: 0.64370\n",
      "Epoch: 00 [ 4404/20158 ( 22%)], Train Loss: 0.64176\n",
      "Epoch: 00 [ 4444/20158 ( 22%)], Train Loss: 0.64152\n",
      "Epoch: 00 [ 4484/20158 ( 22%)], Train Loss: 0.64057\n",
      "Epoch: 00 [ 4524/20158 ( 22%)], Train Loss: 0.63703\n",
      "Epoch: 00 [ 4564/20158 ( 23%)], Train Loss: 0.63584\n",
      "Epoch: 00 [ 4604/20158 ( 23%)], Train Loss: 0.63335\n",
      "Epoch: 00 [ 4644/20158 ( 23%)], Train Loss: 0.63072\n",
      "Epoch: 00 [ 4684/20158 ( 23%)], Train Loss: 0.62765\n",
      "Epoch: 00 [ 4724/20158 ( 23%)], Train Loss: 0.62411\n",
      "Epoch: 00 [ 4764/20158 ( 24%)], Train Loss: 0.62369\n",
      "Epoch: 00 [ 4804/20158 ( 24%)], Train Loss: 0.62154\n",
      "Epoch: 00 [ 4844/20158 ( 24%)], Train Loss: 0.62148\n",
      "Epoch: 00 [ 4884/20158 ( 24%)], Train Loss: 0.62068\n",
      "Epoch: 00 [ 4924/20158 ( 24%)], Train Loss: 0.62032\n",
      "Epoch: 00 [ 4964/20158 ( 25%)], Train Loss: 0.61848\n",
      "Epoch: 00 [ 5004/20158 ( 25%)], Train Loss: 0.62033\n",
      "Epoch: 00 [ 5044/20158 ( 25%)], Train Loss: 0.61798\n",
      "Epoch: 00 [ 5084/20158 ( 25%)], Train Loss: 0.61633\n",
      "Epoch: 00 [ 5124/20158 ( 25%)], Train Loss: 0.61377\n",
      "Epoch: 00 [ 5164/20158 ( 26%)], Train Loss: 0.61231\n",
      "Epoch: 00 [ 5204/20158 ( 26%)], Train Loss: 0.61189\n",
      "Epoch: 00 [ 5244/20158 ( 26%)], Train Loss: 0.61004\n",
      "Epoch: 00 [ 5284/20158 ( 26%)], Train Loss: 0.60818\n",
      "Epoch: 00 [ 5324/20158 ( 26%)], Train Loss: 0.60722\n",
      "Epoch: 00 [ 5364/20158 ( 27%)], Train Loss: 0.60539\n",
      "Epoch: 00 [ 5404/20158 ( 27%)], Train Loss: 0.60496\n",
      "Epoch: 00 [ 5444/20158 ( 27%)], Train Loss: 0.60411\n",
      "Epoch: 00 [ 5484/20158 ( 27%)], Train Loss: 0.60296\n",
      "Epoch: 00 [ 5524/20158 ( 27%)], Train Loss: 0.60273\n",
      "Epoch: 00 [ 5564/20158 ( 28%)], Train Loss: 0.60102\n",
      "Epoch: 00 [ 5604/20158 ( 28%)], Train Loss: 0.60046\n",
      "Epoch: 00 [ 5644/20158 ( 28%)], Train Loss: 0.59736\n",
      "Epoch: 00 [ 5684/20158 ( 28%)], Train Loss: 0.59629\n",
      "Epoch: 00 [ 5724/20158 ( 28%)], Train Loss: 0.59428\n",
      "Epoch: 00 [ 5764/20158 ( 29%)], Train Loss: 0.59279\n",
      "Epoch: 00 [ 5804/20158 ( 29%)], Train Loss: 0.59185\n",
      "Epoch: 00 [ 5844/20158 ( 29%)], Train Loss: 0.59011\n",
      "Epoch: 00 [ 5884/20158 ( 29%)], Train Loss: 0.58831\n",
      "Epoch: 00 [ 5924/20158 ( 29%)], Train Loss: 0.58642\n",
      "Epoch: 00 [ 5964/20158 ( 30%)], Train Loss: 0.58409\n",
      "Epoch: 00 [ 6004/20158 ( 30%)], Train Loss: 0.58343\n",
      "Epoch: 00 [ 6044/20158 ( 30%)], Train Loss: 0.58234\n",
      "Epoch: 00 [ 6084/20158 ( 30%)], Train Loss: 0.58119\n",
      "Epoch: 00 [ 6124/20158 ( 30%)], Train Loss: 0.57945\n",
      "Epoch: 00 [ 6164/20158 ( 31%)], Train Loss: 0.57895\n",
      "Epoch: 00 [ 6204/20158 ( 31%)], Train Loss: 0.57704\n",
      "Epoch: 00 [ 6244/20158 ( 31%)], Train Loss: 0.57632\n",
      "Epoch: 00 [ 6284/20158 ( 31%)], Train Loss: 0.57596\n",
      "Epoch: 00 [ 6324/20158 ( 31%)], Train Loss: 0.57473\n",
      "Epoch: 00 [ 6364/20158 ( 32%)], Train Loss: 0.57351\n",
      "Epoch: 00 [ 6404/20158 ( 32%)], Train Loss: 0.57161\n",
      "Epoch: 00 [ 6444/20158 ( 32%)], Train Loss: 0.56979\n",
      "Epoch: 00 [ 6484/20158 ( 32%)], Train Loss: 0.56990\n",
      "Epoch: 00 [ 6524/20158 ( 32%)], Train Loss: 0.56828\n",
      "Epoch: 00 [ 6564/20158 ( 33%)], Train Loss: 0.56755\n",
      "Epoch: 00 [ 6604/20158 ( 33%)], Train Loss: 0.56570\n",
      "Epoch: 00 [ 6644/20158 ( 33%)], Train Loss: 0.56381\n",
      "Epoch: 00 [ 6684/20158 ( 33%)], Train Loss: 0.56165\n",
      "Epoch: 00 [ 6724/20158 ( 33%)], Train Loss: 0.56072\n",
      "Epoch: 00 [ 6764/20158 ( 34%)], Train Loss: 0.56006\n",
      "Epoch: 00 [ 6804/20158 ( 34%)], Train Loss: 0.55968\n",
      "Epoch: 00 [ 6844/20158 ( 34%)], Train Loss: 0.55874\n",
      "Epoch: 00 [ 6884/20158 ( 34%)], Train Loss: 0.55788\n",
      "Epoch: 00 [ 6924/20158 ( 34%)], Train Loss: 0.55805\n",
      "Epoch: 00 [ 6964/20158 ( 35%)], Train Loss: 0.55823\n",
      "Epoch: 00 [ 7004/20158 ( 35%)], Train Loss: 0.55716\n",
      "Epoch: 00 [ 7044/20158 ( 35%)], Train Loss: 0.55554\n",
      "Epoch: 00 [ 7084/20158 ( 35%)], Train Loss: 0.55435\n",
      "Epoch: 00 [ 7124/20158 ( 35%)], Train Loss: 0.55302\n",
      "Epoch: 00 [ 7164/20158 ( 36%)], Train Loss: 0.55182\n",
      "Epoch: 00 [ 7204/20158 ( 36%)], Train Loss: 0.55071\n",
      "Epoch: 00 [ 7244/20158 ( 36%)], Train Loss: 0.54964\n",
      "Epoch: 00 [ 7284/20158 ( 36%)], Train Loss: 0.54762\n",
      "Epoch: 00 [ 7324/20158 ( 36%)], Train Loss: 0.54738\n",
      "Epoch: 00 [ 7364/20158 ( 37%)], Train Loss: 0.54651\n",
      "Epoch: 00 [ 7404/20158 ( 37%)], Train Loss: 0.54511\n",
      "Epoch: 00 [ 7444/20158 ( 37%)], Train Loss: 0.54410\n",
      "Epoch: 00 [ 7484/20158 ( 37%)], Train Loss: 0.54335\n",
      "Epoch: 00 [ 7524/20158 ( 37%)], Train Loss: 0.54254\n",
      "Epoch: 00 [ 7564/20158 ( 38%)], Train Loss: 0.54144\n",
      "Epoch: 00 [ 7604/20158 ( 38%)], Train Loss: 0.54047\n",
      "Epoch: 00 [ 7644/20158 ( 38%)], Train Loss: 0.53883\n",
      "Epoch: 00 [ 7684/20158 ( 38%)], Train Loss: 0.53829\n",
      "Epoch: 00 [ 7724/20158 ( 38%)], Train Loss: 0.53741\n",
      "Epoch: 00 [ 7764/20158 ( 39%)], Train Loss: 0.53697\n",
      "Epoch: 00 [ 7804/20158 ( 39%)], Train Loss: 0.53663\n",
      "Epoch: 00 [ 7844/20158 ( 39%)], Train Loss: 0.53492\n",
      "Epoch: 00 [ 7884/20158 ( 39%)], Train Loss: 0.53378\n",
      "Epoch: 00 [ 7924/20158 ( 39%)], Train Loss: 0.53320\n",
      "Epoch: 00 [ 7964/20158 ( 40%)], Train Loss: 0.53235\n",
      "Epoch: 00 [ 8004/20158 ( 40%)], Train Loss: 0.53153\n",
      "Epoch: 00 [ 8044/20158 ( 40%)], Train Loss: 0.53035\n",
      "Epoch: 00 [ 8084/20158 ( 40%)], Train Loss: 0.53038\n",
      "Epoch: 00 [ 8124/20158 ( 40%)], Train Loss: 0.52960\n",
      "Epoch: 00 [ 8164/20158 ( 41%)], Train Loss: 0.52883\n",
      "Epoch: 00 [ 8204/20158 ( 41%)], Train Loss: 0.52773\n",
      "Epoch: 00 [ 8244/20158 ( 41%)], Train Loss: 0.52739\n",
      "Epoch: 00 [ 8284/20158 ( 41%)], Train Loss: 0.52652\n",
      "Epoch: 00 [ 8324/20158 ( 41%)], Train Loss: 0.52558\n",
      "Epoch: 00 [ 8364/20158 ( 41%)], Train Loss: 0.52511\n",
      "Epoch: 00 [ 8404/20158 ( 42%)], Train Loss: 0.52433\n",
      "Epoch: 00 [ 8444/20158 ( 42%)], Train Loss: 0.52393\n",
      "Epoch: 00 [ 8484/20158 ( 42%)], Train Loss: 0.52305\n",
      "Epoch: 00 [ 8524/20158 ( 42%)], Train Loss: 0.52189\n",
      "Epoch: 00 [ 8564/20158 ( 42%)], Train Loss: 0.52092\n",
      "Epoch: 00 [ 8604/20158 ( 43%)], Train Loss: 0.52060\n",
      "Epoch: 00 [ 8644/20158 ( 43%)], Train Loss: 0.51986\n",
      "Epoch: 00 [ 8684/20158 ( 43%)], Train Loss: 0.51940\n",
      "Epoch: 00 [ 8724/20158 ( 43%)], Train Loss: 0.51866\n",
      "Epoch: 00 [ 8764/20158 ( 43%)], Train Loss: 0.51852\n",
      "Epoch: 00 [ 8804/20158 ( 44%)], Train Loss: 0.51791\n",
      "Epoch: 00 [ 8844/20158 ( 44%)], Train Loss: 0.51832\n",
      "Epoch: 00 [ 8884/20158 ( 44%)], Train Loss: 0.51681\n",
      "Epoch: 00 [ 8924/20158 ( 44%)], Train Loss: 0.51647\n",
      "Epoch: 00 [ 8964/20158 ( 44%)], Train Loss: 0.51542\n",
      "Epoch: 00 [ 9004/20158 ( 45%)], Train Loss: 0.51464\n",
      "Epoch: 00 [ 9044/20158 ( 45%)], Train Loss: 0.51420\n",
      "Epoch: 00 [ 9084/20158 ( 45%)], Train Loss: 0.51363\n",
      "Epoch: 00 [ 9124/20158 ( 45%)], Train Loss: 0.51295\n",
      "Epoch: 00 [ 9164/20158 ( 45%)], Train Loss: 0.51155\n",
      "Epoch: 00 [ 9204/20158 ( 46%)], Train Loss: 0.51146\n",
      "Epoch: 00 [ 9244/20158 ( 46%)], Train Loss: 0.51132\n",
      "Epoch: 00 [ 9284/20158 ( 46%)], Train Loss: 0.51087\n",
      "Epoch: 00 [ 9324/20158 ( 46%)], Train Loss: 0.50975\n",
      "Epoch: 00 [ 9364/20158 ( 46%)], Train Loss: 0.50895\n",
      "Epoch: 00 [ 9404/20158 ( 47%)], Train Loss: 0.50830\n",
      "Epoch: 00 [ 9444/20158 ( 47%)], Train Loss: 0.50759\n",
      "Epoch: 00 [ 9484/20158 ( 47%)], Train Loss: 0.50677\n",
      "Epoch: 00 [ 9524/20158 ( 47%)], Train Loss: 0.50652\n",
      "Epoch: 00 [ 9564/20158 ( 47%)], Train Loss: 0.50656\n",
      "Epoch: 00 [ 9604/20158 ( 48%)], Train Loss: 0.50564\n",
      "Epoch: 00 [ 9644/20158 ( 48%)], Train Loss: 0.50574\n",
      "Epoch: 00 [ 9684/20158 ( 48%)], Train Loss: 0.50463\n",
      "Epoch: 00 [ 9724/20158 ( 48%)], Train Loss: 0.50365\n",
      "Epoch: 00 [ 9764/20158 ( 48%)], Train Loss: 0.50285\n",
      "Epoch: 00 [ 9804/20158 ( 49%)], Train Loss: 0.50250\n",
      "Epoch: 00 [ 9844/20158 ( 49%)], Train Loss: 0.50169\n",
      "Epoch: 00 [ 9884/20158 ( 49%)], Train Loss: 0.50103\n",
      "Epoch: 00 [ 9924/20158 ( 49%)], Train Loss: 0.49987\n",
      "Epoch: 00 [ 9964/20158 ( 49%)], Train Loss: 0.49985\n",
      "Epoch: 00 [10004/20158 ( 50%)], Train Loss: 0.50027\n",
      "Epoch: 00 [10044/20158 ( 50%)], Train Loss: 0.49893\n",
      "Epoch: 00 [10084/20158 ( 50%)], Train Loss: 0.49801\n",
      "Epoch: 00 [10124/20158 ( 50%)], Train Loss: 0.49759\n",
      "Epoch: 00 [10164/20158 ( 50%)], Train Loss: 0.49691\n",
      "Epoch: 00 [10204/20158 ( 51%)], Train Loss: 0.49673\n",
      "Epoch: 00 [10244/20158 ( 51%)], Train Loss: 0.49682\n",
      "Epoch: 00 [10284/20158 ( 51%)], Train Loss: 0.49666\n",
      "Epoch: 00 [10324/20158 ( 51%)], Train Loss: 0.49587\n",
      "Epoch: 00 [10364/20158 ( 51%)], Train Loss: 0.49575\n",
      "Epoch: 00 [10404/20158 ( 52%)], Train Loss: 0.49478\n",
      "Epoch: 00 [10444/20158 ( 52%)], Train Loss: 0.49454\n",
      "Epoch: 00 [10484/20158 ( 52%)], Train Loss: 0.49431\n",
      "Epoch: 00 [10524/20158 ( 52%)], Train Loss: 0.49396\n",
      "Epoch: 00 [10564/20158 ( 52%)], Train Loss: 0.49323\n",
      "Epoch: 00 [10604/20158 ( 53%)], Train Loss: 0.49301\n",
      "Epoch: 00 [10644/20158 ( 53%)], Train Loss: 0.49278\n",
      "Epoch: 00 [10684/20158 ( 53%)], Train Loss: 0.49324\n",
      "Epoch: 00 [10724/20158 ( 53%)], Train Loss: 0.49339\n",
      "Epoch: 00 [10764/20158 ( 53%)], Train Loss: 0.49255\n",
      "Epoch: 00 [10804/20158 ( 54%)], Train Loss: 0.49258\n",
      "Epoch: 00 [10844/20158 ( 54%)], Train Loss: 0.49128\n",
      "Epoch: 00 [10884/20158 ( 54%)], Train Loss: 0.49025\n",
      "Epoch: 00 [10924/20158 ( 54%)], Train Loss: 0.48945\n",
      "Epoch: 00 [10964/20158 ( 54%)], Train Loss: 0.48899\n",
      "Epoch: 00 [11004/20158 ( 55%)], Train Loss: 0.48811\n",
      "Epoch: 00 [11044/20158 ( 55%)], Train Loss: 0.48711\n",
      "Epoch: 00 [11084/20158 ( 55%)], Train Loss: 0.48660\n",
      "Epoch: 00 [11124/20158 ( 55%)], Train Loss: 0.48598\n",
      "Epoch: 00 [11164/20158 ( 55%)], Train Loss: 0.48570\n",
      "Epoch: 00 [11204/20158 ( 56%)], Train Loss: 0.48535\n",
      "Epoch: 00 [11244/20158 ( 56%)], Train Loss: 0.48442\n",
      "Epoch: 00 [11284/20158 ( 56%)], Train Loss: 0.48404\n",
      "Epoch: 00 [11324/20158 ( 56%)], Train Loss: 0.48336\n",
      "Epoch: 00 [11364/20158 ( 56%)], Train Loss: 0.48307\n",
      "Epoch: 00 [11404/20158 ( 57%)], Train Loss: 0.48282\n",
      "Epoch: 00 [11444/20158 ( 57%)], Train Loss: 0.48250\n",
      "Epoch: 00 [11484/20158 ( 57%)], Train Loss: 0.48244\n",
      "Epoch: 00 [11524/20158 ( 57%)], Train Loss: 0.48218\n",
      "Epoch: 00 [11564/20158 ( 57%)], Train Loss: 0.48148\n",
      "Epoch: 00 [11604/20158 ( 58%)], Train Loss: 0.48101\n",
      "Epoch: 00 [11644/20158 ( 58%)], Train Loss: 0.48030\n",
      "Epoch: 00 [11684/20158 ( 58%)], Train Loss: 0.47976\n",
      "Epoch: 00 [11724/20158 ( 58%)], Train Loss: 0.47892\n",
      "Epoch: 00 [11764/20158 ( 58%)], Train Loss: 0.47873\n",
      "Epoch: 00 [11804/20158 ( 59%)], Train Loss: 0.47883\n",
      "Epoch: 00 [11844/20158 ( 59%)], Train Loss: 0.47823\n",
      "Epoch: 00 [11884/20158 ( 59%)], Train Loss: 0.47772\n",
      "Epoch: 00 [11924/20158 ( 59%)], Train Loss: 0.47728\n",
      "Epoch: 00 [11964/20158 ( 59%)], Train Loss: 0.47725\n",
      "Epoch: 00 [12004/20158 ( 60%)], Train Loss: 0.47655\n",
      "Epoch: 00 [12044/20158 ( 60%)], Train Loss: 0.47607\n",
      "Epoch: 00 [12084/20158 ( 60%)], Train Loss: 0.47637\n",
      "Epoch: 00 [12124/20158 ( 60%)], Train Loss: 0.47545\n",
      "Epoch: 00 [12164/20158 ( 60%)], Train Loss: 0.47500\n",
      "Epoch: 00 [12204/20158 ( 61%)], Train Loss: 0.47427\n",
      "Epoch: 00 [12244/20158 ( 61%)], Train Loss: 0.47429\n",
      "Epoch: 00 [12284/20158 ( 61%)], Train Loss: 0.47355\n",
      "Epoch: 00 [12324/20158 ( 61%)], Train Loss: 0.47315\n",
      "Epoch: 00 [12364/20158 ( 61%)], Train Loss: 0.47314\n",
      "Epoch: 00 [12404/20158 ( 62%)], Train Loss: 0.47344\n",
      "Epoch: 00 [12444/20158 ( 62%)], Train Loss: 0.47272\n",
      "Epoch: 00 [12484/20158 ( 62%)], Train Loss: 0.47305\n",
      "Epoch: 00 [12524/20158 ( 62%)], Train Loss: 0.47261\n",
      "Epoch: 00 [12564/20158 ( 62%)], Train Loss: 0.47266\n",
      "Epoch: 00 [12604/20158 ( 63%)], Train Loss: 0.47264\n",
      "Epoch: 00 [12644/20158 ( 63%)], Train Loss: 0.47232\n",
      "Epoch: 00 [12684/20158 ( 63%)], Train Loss: 0.47218\n",
      "Epoch: 00 [12724/20158 ( 63%)], Train Loss: 0.47137\n",
      "Epoch: 00 [12764/20158 ( 63%)], Train Loss: 0.47064\n",
      "Epoch: 00 [12804/20158 ( 64%)], Train Loss: 0.47062\n",
      "Epoch: 00 [12844/20158 ( 64%)], Train Loss: 0.47035\n",
      "Epoch: 00 [12884/20158 ( 64%)], Train Loss: 0.47040\n",
      "Epoch: 00 [12924/20158 ( 64%)], Train Loss: 0.47068\n",
      "Epoch: 00 [12964/20158 ( 64%)], Train Loss: 0.47046\n",
      "Epoch: 00 [13004/20158 ( 65%)], Train Loss: 0.47085\n",
      "Epoch: 00 [13044/20158 ( 65%)], Train Loss: 0.47043\n",
      "Epoch: 00 [13084/20158 ( 65%)], Train Loss: 0.46986\n",
      "Epoch: 00 [13124/20158 ( 65%)], Train Loss: 0.46968\n",
      "Epoch: 00 [13164/20158 ( 65%)], Train Loss: 0.46938\n",
      "Epoch: 00 [13204/20158 ( 66%)], Train Loss: 0.46955\n",
      "Epoch: 00 [13244/20158 ( 66%)], Train Loss: 0.46919\n",
      "Epoch: 00 [13284/20158 ( 66%)], Train Loss: 0.46830\n",
      "Epoch: 00 [13324/20158 ( 66%)], Train Loss: 0.46851\n",
      "Epoch: 00 [13364/20158 ( 66%)], Train Loss: 0.46845\n",
      "Epoch: 00 [13404/20158 ( 66%)], Train Loss: 0.46787\n",
      "Epoch: 00 [13444/20158 ( 67%)], Train Loss: 0.46766\n",
      "Epoch: 00 [13484/20158 ( 67%)], Train Loss: 0.46771\n",
      "Epoch: 00 [13524/20158 ( 67%)], Train Loss: 0.46786\n",
      "Epoch: 00 [13564/20158 ( 67%)], Train Loss: 0.46769\n",
      "Epoch: 00 [13604/20158 ( 67%)], Train Loss: 0.46742\n",
      "Epoch: 00 [13644/20158 ( 68%)], Train Loss: 0.46750\n",
      "Epoch: 00 [13684/20158 ( 68%)], Train Loss: 0.46691\n",
      "Epoch: 00 [13724/20158 ( 68%)], Train Loss: 0.46670\n",
      "Epoch: 00 [13764/20158 ( 68%)], Train Loss: 0.46614\n",
      "Epoch: 00 [13804/20158 ( 68%)], Train Loss: 0.46554\n",
      "Epoch: 00 [13844/20158 ( 69%)], Train Loss: 0.46556\n",
      "Epoch: 00 [13884/20158 ( 69%)], Train Loss: 0.46541\n",
      "Epoch: 00 [13924/20158 ( 69%)], Train Loss: 0.46520\n",
      "Epoch: 00 [13964/20158 ( 69%)], Train Loss: 0.46487\n",
      "Epoch: 00 [14004/20158 ( 69%)], Train Loss: 0.46513\n",
      "Epoch: 00 [14044/20158 ( 70%)], Train Loss: 0.46452\n",
      "Epoch: 00 [14084/20158 ( 70%)], Train Loss: 0.46415\n",
      "Epoch: 00 [14124/20158 ( 70%)], Train Loss: 0.46346\n",
      "Epoch: 00 [14164/20158 ( 70%)], Train Loss: 0.46299\n",
      "Epoch: 00 [14204/20158 ( 70%)], Train Loss: 0.46252\n",
      "Epoch: 00 [14244/20158 ( 71%)], Train Loss: 0.46166\n",
      "Epoch: 00 [14284/20158 ( 71%)], Train Loss: 0.46075\n",
      "Epoch: 00 [14324/20158 ( 71%)], Train Loss: 0.46040\n",
      "Epoch: 00 [14364/20158 ( 71%)], Train Loss: 0.46008\n",
      "Epoch: 00 [14404/20158 ( 71%)], Train Loss: 0.46014\n",
      "Epoch: 00 [14444/20158 ( 72%)], Train Loss: 0.45950\n",
      "Epoch: 00 [14484/20158 ( 72%)], Train Loss: 0.45924\n",
      "Epoch: 00 [14524/20158 ( 72%)], Train Loss: 0.45924\n",
      "Epoch: 00 [14564/20158 ( 72%)], Train Loss: 0.45906\n",
      "Epoch: 00 [14604/20158 ( 72%)], Train Loss: 0.45874\n",
      "Epoch: 00 [14644/20158 ( 73%)], Train Loss: 0.45810\n",
      "Epoch: 00 [14684/20158 ( 73%)], Train Loss: 0.45728\n",
      "Epoch: 00 [14724/20158 ( 73%)], Train Loss: 0.45661\n",
      "Epoch: 00 [14764/20158 ( 73%)], Train Loss: 0.45669\n",
      "Epoch: 00 [14804/20158 ( 73%)], Train Loss: 0.45641\n",
      "Epoch: 00 [14844/20158 ( 74%)], Train Loss: 0.45597\n",
      "Epoch: 00 [14884/20158 ( 74%)], Train Loss: 0.45614\n",
      "Epoch: 00 [14924/20158 ( 74%)], Train Loss: 0.45595\n",
      "Epoch: 00 [14964/20158 ( 74%)], Train Loss: 0.45520\n",
      "Epoch: 00 [15004/20158 ( 74%)], Train Loss: 0.45481\n",
      "Epoch: 00 [15044/20158 ( 75%)], Train Loss: 0.45452\n",
      "Epoch: 00 [15084/20158 ( 75%)], Train Loss: 0.45435\n",
      "Epoch: 00 [15124/20158 ( 75%)], Train Loss: 0.45436\n",
      "Epoch: 00 [15164/20158 ( 75%)], Train Loss: 0.45410\n",
      "Epoch: 00 [15204/20158 ( 75%)], Train Loss: 0.45339\n",
      "Epoch: 00 [15244/20158 ( 76%)], Train Loss: 0.45325\n",
      "Epoch: 00 [15284/20158 ( 76%)], Train Loss: 0.45318\n",
      "Epoch: 00 [15324/20158 ( 76%)], Train Loss: 0.45278\n",
      "Epoch: 00 [15364/20158 ( 76%)], Train Loss: 0.45242\n",
      "Epoch: 00 [15404/20158 ( 76%)], Train Loss: 0.45258\n",
      "Epoch: 00 [15444/20158 ( 77%)], Train Loss: 0.45208\n",
      "Epoch: 00 [15484/20158 ( 77%)], Train Loss: 0.45167\n",
      "Epoch: 00 [15524/20158 ( 77%)], Train Loss: 0.45131\n",
      "Epoch: 00 [15564/20158 ( 77%)], Train Loss: 0.45068\n",
      "Epoch: 00 [15604/20158 ( 77%)], Train Loss: 0.45043\n",
      "Epoch: 00 [15644/20158 ( 78%)], Train Loss: 0.44977\n",
      "Epoch: 00 [15684/20158 ( 78%)], Train Loss: 0.44919\n",
      "Epoch: 00 [15724/20158 ( 78%)], Train Loss: 0.44899\n",
      "Epoch: 00 [15764/20158 ( 78%)], Train Loss: 0.44824\n",
      "Epoch: 00 [15804/20158 ( 78%)], Train Loss: 0.44843\n",
      "Epoch: 00 [15844/20158 ( 79%)], Train Loss: 0.44845\n",
      "Epoch: 00 [15884/20158 ( 79%)], Train Loss: 0.44826\n",
      "Epoch: 00 [15924/20158 ( 79%)], Train Loss: 0.44816\n",
      "Epoch: 00 [15964/20158 ( 79%)], Train Loss: 0.44826\n",
      "Epoch: 00 [16004/20158 ( 79%)], Train Loss: 0.44806\n",
      "Epoch: 00 [16044/20158 ( 80%)], Train Loss: 0.44790\n",
      "Epoch: 00 [16084/20158 ( 80%)], Train Loss: 0.44762\n",
      "Epoch: 00 [16124/20158 ( 80%)], Train Loss: 0.44729\n",
      "Epoch: 00 [16164/20158 ( 80%)], Train Loss: 0.44704\n",
      "Epoch: 00 [16204/20158 ( 80%)], Train Loss: 0.44666\n",
      "Epoch: 00 [16244/20158 ( 81%)], Train Loss: 0.44644\n",
      "Epoch: 00 [16284/20158 ( 81%)], Train Loss: 0.44609\n",
      "Epoch: 00 [16324/20158 ( 81%)], Train Loss: 0.44580\n",
      "Epoch: 00 [16364/20158 ( 81%)], Train Loss: 0.44600\n",
      "Epoch: 00 [16404/20158 ( 81%)], Train Loss: 0.44570\n",
      "Epoch: 00 [16444/20158 ( 82%)], Train Loss: 0.44525\n",
      "Epoch: 00 [16484/20158 ( 82%)], Train Loss: 0.44539\n",
      "Epoch: 00 [16524/20158 ( 82%)], Train Loss: 0.44530\n",
      "Epoch: 00 [16564/20158 ( 82%)], Train Loss: 0.44463\n",
      "Epoch: 00 [16604/20158 ( 82%)], Train Loss: 0.44464\n",
      "Epoch: 00 [16644/20158 ( 83%)], Train Loss: 0.44395\n",
      "Epoch: 00 [16684/20158 ( 83%)], Train Loss: 0.44344\n",
      "Epoch: 00 [16724/20158 ( 83%)], Train Loss: 0.44289\n",
      "Epoch: 00 [16764/20158 ( 83%)], Train Loss: 0.44253\n",
      "Epoch: 00 [16804/20158 ( 83%)], Train Loss: 0.44265\n",
      "Epoch: 00 [16844/20158 ( 84%)], Train Loss: 0.44250\n",
      "Epoch: 00 [16884/20158 ( 84%)], Train Loss: 0.44189\n",
      "Epoch: 00 [16924/20158 ( 84%)], Train Loss: 0.44183\n",
      "Epoch: 00 [16964/20158 ( 84%)], Train Loss: 0.44125\n",
      "Epoch: 00 [17004/20158 ( 84%)], Train Loss: 0.44183\n",
      "Epoch: 00 [17044/20158 ( 85%)], Train Loss: 0.44134\n",
      "Epoch: 00 [17084/20158 ( 85%)], Train Loss: 0.44105\n",
      "Epoch: 00 [17124/20158 ( 85%)], Train Loss: 0.44061\n",
      "Epoch: 00 [17164/20158 ( 85%)], Train Loss: 0.44025\n",
      "Epoch: 00 [17204/20158 ( 85%)], Train Loss: 0.43983\n",
      "Epoch: 00 [17244/20158 ( 86%)], Train Loss: 0.43966\n",
      "Epoch: 00 [17284/20158 ( 86%)], Train Loss: 0.43922\n",
      "Epoch: 00 [17324/20158 ( 86%)], Train Loss: 0.43906\n",
      "Epoch: 00 [17364/20158 ( 86%)], Train Loss: 0.43877\n",
      "Epoch: 00 [17404/20158 ( 86%)], Train Loss: 0.43871\n",
      "Epoch: 00 [17444/20158 ( 87%)], Train Loss: 0.43855\n",
      "Epoch: 00 [17484/20158 ( 87%)], Train Loss: 0.43815\n",
      "Epoch: 00 [17524/20158 ( 87%)], Train Loss: 0.43808\n",
      "Epoch: 00 [17564/20158 ( 87%)], Train Loss: 0.43739\n",
      "Epoch: 00 [17604/20158 ( 87%)], Train Loss: 0.43702\n",
      "Epoch: 00 [17644/20158 ( 88%)], Train Loss: 0.43658\n",
      "Epoch: 00 [17684/20158 ( 88%)], Train Loss: 0.43631\n",
      "Epoch: 00 [17724/20158 ( 88%)], Train Loss: 0.43627\n",
      "Epoch: 00 [17764/20158 ( 88%)], Train Loss: 0.43621\n",
      "Epoch: 00 [17804/20158 ( 88%)], Train Loss: 0.43619\n",
      "Epoch: 00 [17844/20158 ( 89%)], Train Loss: 0.43615\n",
      "Epoch: 00 [17884/20158 ( 89%)], Train Loss: 0.43607\n",
      "Epoch: 00 [17924/20158 ( 89%)], Train Loss: 0.43567\n",
      "Epoch: 00 [17964/20158 ( 89%)], Train Loss: 0.43548\n",
      "Epoch: 00 [18004/20158 ( 89%)], Train Loss: 0.43504\n",
      "Epoch: 00 [18044/20158 ( 90%)], Train Loss: 0.43468\n",
      "Epoch: 00 [18084/20158 ( 90%)], Train Loss: 0.43460\n",
      "Epoch: 00 [18124/20158 ( 90%)], Train Loss: 0.43428\n",
      "Epoch: 00 [18164/20158 ( 90%)], Train Loss: 0.43362\n",
      "Epoch: 00 [18204/20158 ( 90%)], Train Loss: 0.43336\n",
      "Epoch: 00 [18244/20158 ( 91%)], Train Loss: 0.43327\n",
      "Epoch: 00 [18284/20158 ( 91%)], Train Loss: 0.43313\n",
      "Epoch: 00 [18324/20158 ( 91%)], Train Loss: 0.43274\n",
      "Epoch: 00 [18364/20158 ( 91%)], Train Loss: 0.43282\n",
      "Epoch: 00 [18404/20158 ( 91%)], Train Loss: 0.43265\n",
      "Epoch: 00 [18444/20158 ( 91%)], Train Loss: 0.43232\n",
      "Epoch: 00 [18484/20158 ( 92%)], Train Loss: 0.43209\n",
      "Epoch: 00 [18524/20158 ( 92%)], Train Loss: 0.43194\n",
      "Epoch: 00 [18564/20158 ( 92%)], Train Loss: 0.43194\n",
      "Epoch: 00 [18604/20158 ( 92%)], Train Loss: 0.43183\n",
      "Epoch: 00 [18644/20158 ( 92%)], Train Loss: 0.43170\n",
      "Epoch: 00 [18684/20158 ( 93%)], Train Loss: 0.43114\n",
      "Epoch: 00 [18724/20158 ( 93%)], Train Loss: 0.43098\n",
      "Epoch: 00 [18764/20158 ( 93%)], Train Loss: 0.43087\n",
      "Epoch: 00 [18804/20158 ( 93%)], Train Loss: 0.43070\n",
      "Epoch: 00 [18844/20158 ( 93%)], Train Loss: 0.43018\n",
      "Epoch: 00 [18884/20158 ( 94%)], Train Loss: 0.42963\n",
      "Epoch: 00 [18924/20158 ( 94%)], Train Loss: 0.42918\n",
      "Epoch: 00 [18964/20158 ( 94%)], Train Loss: 0.42889\n",
      "Epoch: 00 [19004/20158 ( 94%)], Train Loss: 0.42839\n",
      "Epoch: 00 [19044/20158 ( 94%)], Train Loss: 0.42790\n",
      "Epoch: 00 [19084/20158 ( 95%)], Train Loss: 0.42782\n",
      "Epoch: 00 [19124/20158 ( 95%)], Train Loss: 0.42746\n",
      "Epoch: 00 [19164/20158 ( 95%)], Train Loss: 0.42709\n",
      "Epoch: 00 [19204/20158 ( 95%)], Train Loss: 0.42683\n",
      "Epoch: 00 [19244/20158 ( 95%)], Train Loss: 0.42692\n",
      "Epoch: 00 [19284/20158 ( 96%)], Train Loss: 0.42668\n",
      "Epoch: 00 [19324/20158 ( 96%)], Train Loss: 0.42619\n",
      "Epoch: 00 [19364/20158 ( 96%)], Train Loss: 0.42616\n",
      "Epoch: 00 [19404/20158 ( 96%)], Train Loss: 0.42596\n",
      "Epoch: 00 [19444/20158 ( 96%)], Train Loss: 0.42557\n",
      "Epoch: 00 [19484/20158 ( 97%)], Train Loss: 0.42543\n",
      "Epoch: 00 [19524/20158 ( 97%)], Train Loss: 0.42488\n",
      "Epoch: 00 [19564/20158 ( 97%)], Train Loss: 0.42456\n",
      "Epoch: 00 [19604/20158 ( 97%)], Train Loss: 0.42474\n",
      "Epoch: 00 [19644/20158 ( 97%)], Train Loss: 0.42430\n",
      "Epoch: 00 [19684/20158 ( 98%)], Train Loss: 0.42421\n",
      "Epoch: 00 [19724/20158 ( 98%)], Train Loss: 0.42371\n",
      "Epoch: 00 [19764/20158 ( 98%)], Train Loss: 0.42320\n",
      "Epoch: 00 [19804/20158 ( 98%)], Train Loss: 0.42291\n",
      "Epoch: 00 [19844/20158 ( 98%)], Train Loss: 0.42298\n",
      "Epoch: 00 [19884/20158 ( 99%)], Train Loss: 0.42264\n",
      "Epoch: 00 [19924/20158 ( 99%)], Train Loss: 0.42257\n",
      "Epoch: 00 [19964/20158 ( 99%)], Train Loss: 0.42247\n",
      "Epoch: 00 [20004/20158 ( 99%)], Train Loss: 0.42257\n",
      "Epoch: 00 [20044/20158 ( 99%)], Train Loss: 0.42235\n",
      "Epoch: 00 [20084/20158 (100%)], Train Loss: 0.42183\n",
      "Epoch: 00 [20124/20158 (100%)], Train Loss: 0.42147\n",
      "Epoch: 00 [20158/20158 (100%)], Train Loss: 0.42142\n",
      "----Validation Results Summary----\n",
      "Epoch: [0] Valid Loss: 0.25900\n",
      "Post-processing 223 example predictions split into 2911 features.\n",
      "valid jaccard:  0.644335895793295\n",
      "0 Epoch, Best epoch was updated! Valid Loss: 0.25900\n",
      "Saving model checkpoint to output/checkpoint-fold-3-epoch-0.\n",
      "\n",
      "Total Training Time: 2881.1969010829926secs, Average Training Time per Epoch: 2881.1969010829926secs.\n",
      "Total Validation Time: 175.38269352912903secs, Average Validation Time per Epoch: 175.38269352912903secs.\n",
      "\n",
      "\n",
      "--------------------------------------------------\n",
      "FOLD: 4\n",
      "--------------------------------------------------\n",
      "Model pushed to 1 GPU(s), type Tesla P100-PCIE-16GB.\n",
      "Num examples Train= 20209, Num examples Valid=2860\n",
      "Total Training Steps: 2527, Total Warmup Steps: 252\n",
      "Epoch: 00 [    4/20209 (  0%)], Train Loss: 3.12538\n",
      "Epoch: 00 [   44/20209 (  0%)], Train Loss: 3.05727\n",
      "Epoch: 00 [   84/20209 (  0%)], Train Loss: 3.03219\n",
      "Epoch: 00 [  124/20209 (  1%)], Train Loss: 3.00419\n",
      "Epoch: 00 [  164/20209 (  1%)], Train Loss: 2.94420\n",
      "Epoch: 00 [  204/20209 (  1%)], Train Loss: 2.86709\n",
      "Epoch: 00 [  244/20209 (  1%)], Train Loss: 2.76264\n",
      "Epoch: 00 [  284/20209 (  1%)], Train Loss: 2.61573\n",
      "Epoch: 00 [  324/20209 (  2%)], Train Loss: 2.47863\n",
      "Epoch: 00 [  364/20209 (  2%)], Train Loss: 2.32883\n",
      "Epoch: 00 [  404/20209 (  2%)], Train Loss: 2.20338\n",
      "Epoch: 00 [  444/20209 (  2%)], Train Loss: 2.06687\n",
      "Epoch: 00 [  484/20209 (  2%)], Train Loss: 1.94108\n",
      "Epoch: 00 [  524/20209 (  3%)], Train Loss: 1.84206\n",
      "Epoch: 00 [  564/20209 (  3%)], Train Loss: 1.75829\n",
      "Epoch: 00 [  604/20209 (  3%)], Train Loss: 1.68351\n",
      "Epoch: 00 [  644/20209 (  3%)], Train Loss: 1.61744\n",
      "Epoch: 00 [  684/20209 (  3%)], Train Loss: 1.54374\n",
      "Epoch: 00 [  724/20209 (  4%)], Train Loss: 1.50779\n",
      "Epoch: 00 [  764/20209 (  4%)], Train Loss: 1.44139\n",
      "Epoch: 00 [  804/20209 (  4%)], Train Loss: 1.39982\n",
      "Epoch: 00 [  844/20209 (  4%)], Train Loss: 1.34865\n",
      "Epoch: 00 [  884/20209 (  4%)], Train Loss: 1.31584\n",
      "Epoch: 00 [  924/20209 (  5%)], Train Loss: 1.28156\n",
      "Epoch: 00 [  964/20209 (  5%)], Train Loss: 1.24740\n",
      "Epoch: 00 [ 1004/20209 (  5%)], Train Loss: 1.21439\n",
      "Epoch: 00 [ 1044/20209 (  5%)], Train Loss: 1.18204\n",
      "Epoch: 00 [ 1084/20209 (  5%)], Train Loss: 1.14954\n",
      "Epoch: 00 [ 1124/20209 (  6%)], Train Loss: 1.12703\n",
      "Epoch: 00 [ 1164/20209 (  6%)], Train Loss: 1.10117\n",
      "Epoch: 00 [ 1204/20209 (  6%)], Train Loss: 1.07934\n",
      "Epoch: 00 [ 1244/20209 (  6%)], Train Loss: 1.05169\n",
      "Epoch: 00 [ 1284/20209 (  6%)], Train Loss: 1.02930\n",
      "Epoch: 00 [ 1324/20209 (  7%)], Train Loss: 1.01048\n",
      "Epoch: 00 [ 1364/20209 (  7%)], Train Loss: 0.99677\n",
      "Epoch: 00 [ 1404/20209 (  7%)], Train Loss: 0.98428\n",
      "Epoch: 00 [ 1444/20209 (  7%)], Train Loss: 0.97020\n",
      "Epoch: 00 [ 1484/20209 (  7%)], Train Loss: 0.95170\n",
      "Epoch: 00 [ 1524/20209 (  8%)], Train Loss: 0.94223\n",
      "Epoch: 00 [ 1564/20209 (  8%)], Train Loss: 0.92729\n",
      "Epoch: 00 [ 1604/20209 (  8%)], Train Loss: 0.91739\n",
      "Epoch: 00 [ 1644/20209 (  8%)], Train Loss: 0.90362\n",
      "Epoch: 00 [ 1684/20209 (  8%)], Train Loss: 0.89795\n",
      "Epoch: 00 [ 1724/20209 (  9%)], Train Loss: 0.88542\n",
      "Epoch: 00 [ 1764/20209 (  9%)], Train Loss: 0.87140\n",
      "Epoch: 00 [ 1804/20209 (  9%)], Train Loss: 0.86748\n",
      "Epoch: 00 [ 1844/20209 (  9%)], Train Loss: 0.85812\n",
      "Epoch: 00 [ 1884/20209 (  9%)], Train Loss: 0.85081\n",
      "Epoch: 00 [ 1924/20209 ( 10%)], Train Loss: 0.84154\n",
      "Epoch: 00 [ 1964/20209 ( 10%)], Train Loss: 0.84007\n",
      "Epoch: 00 [ 2004/20209 ( 10%)], Train Loss: 0.83096\n",
      "Epoch: 00 [ 2044/20209 ( 10%)], Train Loss: 0.82330\n",
      "Epoch: 00 [ 2084/20209 ( 10%)], Train Loss: 0.81951\n",
      "Epoch: 00 [ 2124/20209 ( 11%)], Train Loss: 0.81405\n",
      "Epoch: 00 [ 2164/20209 ( 11%)], Train Loss: 0.80914\n",
      "Epoch: 00 [ 2204/20209 ( 11%)], Train Loss: 0.80221\n",
      "Epoch: 00 [ 2244/20209 ( 11%)], Train Loss: 0.79645\n",
      "Epoch: 00 [ 2284/20209 ( 11%)], Train Loss: 0.79390\n",
      "Epoch: 00 [ 2324/20209 ( 11%)], Train Loss: 0.78773\n",
      "Epoch: 00 [ 2364/20209 ( 12%)], Train Loss: 0.78236\n",
      "Epoch: 00 [ 2404/20209 ( 12%)], Train Loss: 0.77805\n",
      "Epoch: 00 [ 2444/20209 ( 12%)], Train Loss: 0.77421\n",
      "Epoch: 00 [ 2484/20209 ( 12%)], Train Loss: 0.77135\n",
      "Epoch: 00 [ 2524/20209 ( 12%)], Train Loss: 0.76497\n",
      "Epoch: 00 [ 2564/20209 ( 13%)], Train Loss: 0.75971\n",
      "Epoch: 00 [ 2604/20209 ( 13%)], Train Loss: 0.75097\n",
      "Epoch: 00 [ 2644/20209 ( 13%)], Train Loss: 0.74303\n",
      "Epoch: 00 [ 2684/20209 ( 13%)], Train Loss: 0.74074\n",
      "Epoch: 00 [ 2724/20209 ( 13%)], Train Loss: 0.73531\n",
      "Epoch: 00 [ 2764/20209 ( 14%)], Train Loss: 0.73288\n",
      "Epoch: 00 [ 2804/20209 ( 14%)], Train Loss: 0.73015\n",
      "Epoch: 00 [ 2844/20209 ( 14%)], Train Loss: 0.72694\n",
      "Epoch: 00 [ 2884/20209 ( 14%)], Train Loss: 0.72673\n",
      "Epoch: 00 [ 2924/20209 ( 14%)], Train Loss: 0.72458\n",
      "Epoch: 00 [ 2964/20209 ( 15%)], Train Loss: 0.72075\n",
      "Epoch: 00 [ 3004/20209 ( 15%)], Train Loss: 0.72015\n",
      "Epoch: 00 [ 3044/20209 ( 15%)], Train Loss: 0.71721\n",
      "Epoch: 00 [ 3084/20209 ( 15%)], Train Loss: 0.71389\n",
      "Epoch: 00 [ 3124/20209 ( 15%)], Train Loss: 0.70971\n",
      "Epoch: 00 [ 3164/20209 ( 16%)], Train Loss: 0.70617\n",
      "Epoch: 00 [ 3204/20209 ( 16%)], Train Loss: 0.70231\n",
      "Epoch: 00 [ 3244/20209 ( 16%)], Train Loss: 0.70230\n",
      "Epoch: 00 [ 3284/20209 ( 16%)], Train Loss: 0.69876\n",
      "Epoch: 00 [ 3324/20209 ( 16%)], Train Loss: 0.69355\n",
      "Epoch: 00 [ 3364/20209 ( 17%)], Train Loss: 0.68980\n",
      "Epoch: 00 [ 3404/20209 ( 17%)], Train Loss: 0.68566\n",
      "Epoch: 00 [ 3444/20209 ( 17%)], Train Loss: 0.68306\n",
      "Epoch: 00 [ 3484/20209 ( 17%)], Train Loss: 0.68260\n",
      "Epoch: 00 [ 3524/20209 ( 17%)], Train Loss: 0.67986\n",
      "Epoch: 00 [ 3564/20209 ( 18%)], Train Loss: 0.67743\n",
      "Epoch: 00 [ 3604/20209 ( 18%)], Train Loss: 0.67419\n",
      "Epoch: 00 [ 3644/20209 ( 18%)], Train Loss: 0.67060\n",
      "Epoch: 00 [ 3684/20209 ( 18%)], Train Loss: 0.66965\n",
      "Epoch: 00 [ 3724/20209 ( 18%)], Train Loss: 0.66589\n",
      "Epoch: 00 [ 3764/20209 ( 19%)], Train Loss: 0.66434\n",
      "Epoch: 00 [ 3804/20209 ( 19%)], Train Loss: 0.66293\n",
      "Epoch: 00 [ 3844/20209 ( 19%)], Train Loss: 0.66067\n",
      "Epoch: 00 [ 3884/20209 ( 19%)], Train Loss: 0.65896\n",
      "Epoch: 00 [ 3924/20209 ( 19%)], Train Loss: 0.65616\n",
      "Epoch: 00 [ 3964/20209 ( 20%)], Train Loss: 0.65529\n",
      "Epoch: 00 [ 4004/20209 ( 20%)], Train Loss: 0.65116\n",
      "Epoch: 00 [ 4044/20209 ( 20%)], Train Loss: 0.64840\n",
      "Epoch: 00 [ 4084/20209 ( 20%)], Train Loss: 0.64740\n",
      "Epoch: 00 [ 4124/20209 ( 20%)], Train Loss: 0.64345\n",
      "Epoch: 00 [ 4164/20209 ( 21%)], Train Loss: 0.64218\n",
      "Epoch: 00 [ 4204/20209 ( 21%)], Train Loss: 0.64100\n",
      "Epoch: 00 [ 4244/20209 ( 21%)], Train Loss: 0.63698\n",
      "Epoch: 00 [ 4284/20209 ( 21%)], Train Loss: 0.63320\n",
      "Epoch: 00 [ 4324/20209 ( 21%)], Train Loss: 0.62956\n",
      "Epoch: 00 [ 4364/20209 ( 22%)], Train Loss: 0.62644\n",
      "Epoch: 00 [ 4404/20209 ( 22%)], Train Loss: 0.62498\n",
      "Epoch: 00 [ 4444/20209 ( 22%)], Train Loss: 0.62194\n",
      "Epoch: 00 [ 4484/20209 ( 22%)], Train Loss: 0.61977\n",
      "Epoch: 00 [ 4524/20209 ( 22%)], Train Loss: 0.61681\n",
      "Epoch: 00 [ 4564/20209 ( 23%)], Train Loss: 0.61594\n",
      "Epoch: 00 [ 4604/20209 ( 23%)], Train Loss: 0.61507\n",
      "Epoch: 00 [ 4644/20209 ( 23%)], Train Loss: 0.61379\n",
      "Epoch: 00 [ 4684/20209 ( 23%)], Train Loss: 0.61127\n",
      "Epoch: 00 [ 4724/20209 ( 23%)], Train Loss: 0.61055\n",
      "Epoch: 00 [ 4764/20209 ( 24%)], Train Loss: 0.60825\n",
      "Epoch: 00 [ 4804/20209 ( 24%)], Train Loss: 0.60603\n",
      "Epoch: 00 [ 4844/20209 ( 24%)], Train Loss: 0.60515\n",
      "Epoch: 00 [ 4884/20209 ( 24%)], Train Loss: 0.60559\n",
      "Epoch: 00 [ 4924/20209 ( 24%)], Train Loss: 0.60422\n",
      "Epoch: 00 [ 4964/20209 ( 25%)], Train Loss: 0.60320\n",
      "Epoch: 00 [ 5004/20209 ( 25%)], Train Loss: 0.60241\n",
      "Epoch: 00 [ 5044/20209 ( 25%)], Train Loss: 0.60254\n",
      "Epoch: 00 [ 5084/20209 ( 25%)], Train Loss: 0.60117\n",
      "Epoch: 00 [ 5124/20209 ( 25%)], Train Loss: 0.59875\n",
      "Epoch: 00 [ 5164/20209 ( 26%)], Train Loss: 0.59786\n",
      "Epoch: 00 [ 5204/20209 ( 26%)], Train Loss: 0.59754\n",
      "Epoch: 00 [ 5244/20209 ( 26%)], Train Loss: 0.59456\n",
      "Epoch: 00 [ 5284/20209 ( 26%)], Train Loss: 0.59263\n",
      "Epoch: 00 [ 5324/20209 ( 26%)], Train Loss: 0.59090\n",
      "Epoch: 00 [ 5364/20209 ( 27%)], Train Loss: 0.59019\n",
      "Epoch: 00 [ 5404/20209 ( 27%)], Train Loss: 0.58903\n",
      "Epoch: 00 [ 5444/20209 ( 27%)], Train Loss: 0.58742\n",
      "Epoch: 00 [ 5484/20209 ( 27%)], Train Loss: 0.58426\n",
      "Epoch: 00 [ 5524/20209 ( 27%)], Train Loss: 0.58205\n",
      "Epoch: 00 [ 5564/20209 ( 28%)], Train Loss: 0.58084\n",
      "Epoch: 00 [ 5604/20209 ( 28%)], Train Loss: 0.57836\n",
      "Epoch: 00 [ 5644/20209 ( 28%)], Train Loss: 0.57670\n",
      "Epoch: 00 [ 5684/20209 ( 28%)], Train Loss: 0.57535\n",
      "Epoch: 00 [ 5724/20209 ( 28%)], Train Loss: 0.57418\n",
      "Epoch: 00 [ 5764/20209 ( 29%)], Train Loss: 0.57171\n",
      "Epoch: 00 [ 5804/20209 ( 29%)], Train Loss: 0.57147\n",
      "Epoch: 00 [ 5844/20209 ( 29%)], Train Loss: 0.57040\n",
      "Epoch: 00 [ 5884/20209 ( 29%)], Train Loss: 0.57078\n",
      "Epoch: 00 [ 5924/20209 ( 29%)], Train Loss: 0.57013\n",
      "Epoch: 00 [ 5964/20209 ( 30%)], Train Loss: 0.56983\n",
      "Epoch: 00 [ 6004/20209 ( 30%)], Train Loss: 0.56909\n",
      "Epoch: 00 [ 6044/20209 ( 30%)], Train Loss: 0.56661\n",
      "Epoch: 00 [ 6084/20209 ( 30%)], Train Loss: 0.56551\n",
      "Epoch: 00 [ 6124/20209 ( 30%)], Train Loss: 0.56426\n",
      "Epoch: 00 [ 6164/20209 ( 31%)], Train Loss: 0.56356\n",
      "Epoch: 00 [ 6204/20209 ( 31%)], Train Loss: 0.56367\n",
      "Epoch: 00 [ 6244/20209 ( 31%)], Train Loss: 0.56275\n",
      "Epoch: 00 [ 6284/20209 ( 31%)], Train Loss: 0.56091\n",
      "Epoch: 00 [ 6324/20209 ( 31%)], Train Loss: 0.55974\n",
      "Epoch: 00 [ 6364/20209 ( 31%)], Train Loss: 0.56013\n",
      "Epoch: 00 [ 6404/20209 ( 32%)], Train Loss: 0.55938\n",
      "Epoch: 00 [ 6444/20209 ( 32%)], Train Loss: 0.55838\n",
      "Epoch: 00 [ 6484/20209 ( 32%)], Train Loss: 0.55624\n",
      "Epoch: 00 [ 6524/20209 ( 32%)], Train Loss: 0.55694\n",
      "Epoch: 00 [ 6564/20209 ( 32%)], Train Loss: 0.55752\n",
      "Epoch: 00 [ 6604/20209 ( 33%)], Train Loss: 0.55696\n",
      "Epoch: 00 [ 6644/20209 ( 33%)], Train Loss: 0.55647\n",
      "Epoch: 00 [ 6684/20209 ( 33%)], Train Loss: 0.55509\n",
      "Epoch: 00 [ 6724/20209 ( 33%)], Train Loss: 0.55387\n",
      "Epoch: 00 [ 6764/20209 ( 33%)], Train Loss: 0.55346\n",
      "Epoch: 00 [ 6804/20209 ( 34%)], Train Loss: 0.55251\n",
      "Epoch: 00 [ 6844/20209 ( 34%)], Train Loss: 0.55298\n",
      "Epoch: 00 [ 6884/20209 ( 34%)], Train Loss: 0.55197\n",
      "Epoch: 00 [ 6924/20209 ( 34%)], Train Loss: 0.55033\n",
      "Epoch: 00 [ 6964/20209 ( 34%)], Train Loss: 0.54950\n",
      "Epoch: 00 [ 7004/20209 ( 35%)], Train Loss: 0.54936\n",
      "Epoch: 00 [ 7044/20209 ( 35%)], Train Loss: 0.54767\n",
      "Epoch: 00 [ 7084/20209 ( 35%)], Train Loss: 0.54586\n",
      "Epoch: 00 [ 7124/20209 ( 35%)], Train Loss: 0.54382\n",
      "Epoch: 00 [ 7164/20209 ( 35%)], Train Loss: 0.54345\n",
      "Epoch: 00 [ 7204/20209 ( 36%)], Train Loss: 0.54298\n",
      "Epoch: 00 [ 7244/20209 ( 36%)], Train Loss: 0.54169\n",
      "Epoch: 00 [ 7284/20209 ( 36%)], Train Loss: 0.54044\n",
      "Epoch: 00 [ 7324/20209 ( 36%)], Train Loss: 0.53874\n",
      "Epoch: 00 [ 7364/20209 ( 36%)], Train Loss: 0.53740\n",
      "Epoch: 00 [ 7404/20209 ( 37%)], Train Loss: 0.53585\n",
      "Epoch: 00 [ 7444/20209 ( 37%)], Train Loss: 0.53480\n",
      "Epoch: 00 [ 7484/20209 ( 37%)], Train Loss: 0.53354\n",
      "Epoch: 00 [ 7524/20209 ( 37%)], Train Loss: 0.53153\n",
      "Epoch: 00 [ 7564/20209 ( 37%)], Train Loss: 0.53053\n",
      "Epoch: 00 [ 7604/20209 ( 38%)], Train Loss: 0.52974\n",
      "Epoch: 00 [ 7644/20209 ( 38%)], Train Loss: 0.53004\n",
      "Epoch: 00 [ 7684/20209 ( 38%)], Train Loss: 0.52968\n",
      "Epoch: 00 [ 7724/20209 ( 38%)], Train Loss: 0.52901\n",
      "Epoch: 00 [ 7764/20209 ( 38%)], Train Loss: 0.52829\n",
      "Epoch: 00 [ 7804/20209 ( 39%)], Train Loss: 0.52693\n",
      "Epoch: 00 [ 7844/20209 ( 39%)], Train Loss: 0.52548\n",
      "Epoch: 00 [ 7884/20209 ( 39%)], Train Loss: 0.52463\n",
      "Epoch: 00 [ 7924/20209 ( 39%)], Train Loss: 0.52445\n",
      "Epoch: 00 [ 7964/20209 ( 39%)], Train Loss: 0.52373\n",
      "Epoch: 00 [ 8004/20209 ( 40%)], Train Loss: 0.52291\n",
      "Epoch: 00 [ 8044/20209 ( 40%)], Train Loss: 0.52195\n",
      "Epoch: 00 [ 8084/20209 ( 40%)], Train Loss: 0.52227\n",
      "Epoch: 00 [ 8124/20209 ( 40%)], Train Loss: 0.52261\n",
      "Epoch: 00 [ 8164/20209 ( 40%)], Train Loss: 0.52316\n",
      "Epoch: 00 [ 8204/20209 ( 41%)], Train Loss: 0.52341\n",
      "Epoch: 00 [ 8244/20209 ( 41%)], Train Loss: 0.52253\n",
      "Epoch: 00 [ 8284/20209 ( 41%)], Train Loss: 0.52213\n",
      "Epoch: 00 [ 8324/20209 ( 41%)], Train Loss: 0.52242\n",
      "Epoch: 00 [ 8364/20209 ( 41%)], Train Loss: 0.52179\n",
      "Epoch: 00 [ 8404/20209 ( 42%)], Train Loss: 0.52195\n",
      "Epoch: 00 [ 8444/20209 ( 42%)], Train Loss: 0.52079\n",
      "Epoch: 00 [ 8484/20209 ( 42%)], Train Loss: 0.51932\n",
      "Epoch: 00 [ 8524/20209 ( 42%)], Train Loss: 0.51819\n",
      "Epoch: 00 [ 8564/20209 ( 42%)], Train Loss: 0.51869\n",
      "Epoch: 00 [ 8604/20209 ( 43%)], Train Loss: 0.51883\n",
      "Epoch: 00 [ 8644/20209 ( 43%)], Train Loss: 0.51834\n",
      "Epoch: 00 [ 8684/20209 ( 43%)], Train Loss: 0.51825\n",
      "Epoch: 00 [ 8724/20209 ( 43%)], Train Loss: 0.51843\n",
      "Epoch: 00 [ 8764/20209 ( 43%)], Train Loss: 0.51830\n",
      "Epoch: 00 [ 8804/20209 ( 44%)], Train Loss: 0.51791\n",
      "Epoch: 00 [ 8844/20209 ( 44%)], Train Loss: 0.51744\n",
      "Epoch: 00 [ 8884/20209 ( 44%)], Train Loss: 0.51698\n",
      "Epoch: 00 [ 8924/20209 ( 44%)], Train Loss: 0.51736\n",
      "Epoch: 00 [ 8964/20209 ( 44%)], Train Loss: 0.51661\n",
      "Epoch: 00 [ 9004/20209 ( 45%)], Train Loss: 0.51623\n",
      "Epoch: 00 [ 9044/20209 ( 45%)], Train Loss: 0.51541\n",
      "Epoch: 00 [ 9084/20209 ( 45%)], Train Loss: 0.51455\n",
      "Epoch: 00 [ 9124/20209 ( 45%)], Train Loss: 0.51363\n",
      "Epoch: 00 [ 9164/20209 ( 45%)], Train Loss: 0.51290\n",
      "Epoch: 00 [ 9204/20209 ( 46%)], Train Loss: 0.51198\n",
      "Epoch: 00 [ 9244/20209 ( 46%)], Train Loss: 0.51234\n",
      "Epoch: 00 [ 9284/20209 ( 46%)], Train Loss: 0.51176\n",
      "Epoch: 00 [ 9324/20209 ( 46%)], Train Loss: 0.51156\n",
      "Epoch: 00 [ 9364/20209 ( 46%)], Train Loss: 0.51072\n",
      "Epoch: 00 [ 9404/20209 ( 47%)], Train Loss: 0.50997\n",
      "Epoch: 00 [ 9444/20209 ( 47%)], Train Loss: 0.51048\n",
      "Epoch: 00 [ 9484/20209 ( 47%)], Train Loss: 0.50903\n",
      "Epoch: 00 [ 9524/20209 ( 47%)], Train Loss: 0.50853\n",
      "Epoch: 00 [ 9564/20209 ( 47%)], Train Loss: 0.50746\n",
      "Epoch: 00 [ 9604/20209 ( 48%)], Train Loss: 0.50724\n",
      "Epoch: 00 [ 9644/20209 ( 48%)], Train Loss: 0.50686\n",
      "Epoch: 00 [ 9684/20209 ( 48%)], Train Loss: 0.50655\n",
      "Epoch: 00 [ 9724/20209 ( 48%)], Train Loss: 0.50588\n",
      "Epoch: 00 [ 9764/20209 ( 48%)], Train Loss: 0.50555\n",
      "Epoch: 00 [ 9804/20209 ( 49%)], Train Loss: 0.50513\n",
      "Epoch: 00 [ 9844/20209 ( 49%)], Train Loss: 0.50454\n",
      "Epoch: 00 [ 9884/20209 ( 49%)], Train Loss: 0.50420\n",
      "Epoch: 00 [ 9924/20209 ( 49%)], Train Loss: 0.50362\n",
      "Epoch: 00 [ 9964/20209 ( 49%)], Train Loss: 0.50341\n",
      "Epoch: 00 [10004/20209 ( 50%)], Train Loss: 0.50323\n",
      "Epoch: 00 [10044/20209 ( 50%)], Train Loss: 0.50232\n",
      "Epoch: 00 [10084/20209 ( 50%)], Train Loss: 0.50188\n",
      "Epoch: 00 [10124/20209 ( 50%)], Train Loss: 0.50160\n",
      "Epoch: 00 [10164/20209 ( 50%)], Train Loss: 0.50080\n",
      "Epoch: 00 [10204/20209 ( 50%)], Train Loss: 0.50009\n",
      "Epoch: 00 [10244/20209 ( 51%)], Train Loss: 0.49966\n",
      "Epoch: 00 [10284/20209 ( 51%)], Train Loss: 0.49866\n",
      "Epoch: 00 [10324/20209 ( 51%)], Train Loss: 0.49815\n",
      "Epoch: 00 [10364/20209 ( 51%)], Train Loss: 0.49814\n",
      "Epoch: 00 [10404/20209 ( 51%)], Train Loss: 0.49836\n",
      "Epoch: 00 [10444/20209 ( 52%)], Train Loss: 0.49788\n",
      "Epoch: 00 [10484/20209 ( 52%)], Train Loss: 0.49723\n",
      "Epoch: 00 [10524/20209 ( 52%)], Train Loss: 0.49686\n",
      "Epoch: 00 [10564/20209 ( 52%)], Train Loss: 0.49648\n",
      "Epoch: 00 [10604/20209 ( 52%)], Train Loss: 0.49571\n",
      "Epoch: 00 [10644/20209 ( 53%)], Train Loss: 0.49550\n",
      "Epoch: 00 [10684/20209 ( 53%)], Train Loss: 0.49474\n",
      "Epoch: 00 [10724/20209 ( 53%)], Train Loss: 0.49485\n",
      "Epoch: 00 [10764/20209 ( 53%)], Train Loss: 0.49503\n",
      "Epoch: 00 [10804/20209 ( 53%)], Train Loss: 0.49418\n",
      "Epoch: 00 [10844/20209 ( 54%)], Train Loss: 0.49372\n",
      "Epoch: 00 [10884/20209 ( 54%)], Train Loss: 0.49295\n",
      "Epoch: 00 [10924/20209 ( 54%)], Train Loss: 0.49275\n",
      "Epoch: 00 [10964/20209 ( 54%)], Train Loss: 0.49231\n",
      "Epoch: 00 [11004/20209 ( 54%)], Train Loss: 0.49116\n",
      "Epoch: 00 [11044/20209 ( 55%)], Train Loss: 0.49139\n",
      "Epoch: 00 [11084/20209 ( 55%)], Train Loss: 0.49098\n",
      "Epoch: 00 [11124/20209 ( 55%)], Train Loss: 0.49058\n",
      "Epoch: 00 [11164/20209 ( 55%)], Train Loss: 0.49019\n",
      "Epoch: 00 [11204/20209 ( 55%)], Train Loss: 0.49040\n",
      "Epoch: 00 [11244/20209 ( 56%)], Train Loss: 0.49034\n",
      "Epoch: 00 [11284/20209 ( 56%)], Train Loss: 0.49063\n",
      "Epoch: 00 [11324/20209 ( 56%)], Train Loss: 0.49012\n",
      "Epoch: 00 [11364/20209 ( 56%)], Train Loss: 0.49005\n",
      "Epoch: 00 [11404/20209 ( 56%)], Train Loss: 0.48937\n",
      "Epoch: 00 [11444/20209 ( 57%)], Train Loss: 0.48901\n",
      "Epoch: 00 [11484/20209 ( 57%)], Train Loss: 0.48915\n",
      "Epoch: 00 [11524/20209 ( 57%)], Train Loss: 0.48916\n",
      "Epoch: 00 [11564/20209 ( 57%)], Train Loss: 0.48812\n",
      "Epoch: 00 [11604/20209 ( 57%)], Train Loss: 0.48826\n",
      "Epoch: 00 [11644/20209 ( 58%)], Train Loss: 0.48724\n",
      "Epoch: 00 [11684/20209 ( 58%)], Train Loss: 0.48625\n",
      "Epoch: 00 [11724/20209 ( 58%)], Train Loss: 0.48578\n",
      "Epoch: 00 [11764/20209 ( 58%)], Train Loss: 0.48479\n",
      "Epoch: 00 [11804/20209 ( 58%)], Train Loss: 0.48406\n",
      "Epoch: 00 [11844/20209 ( 59%)], Train Loss: 0.48383\n",
      "Epoch: 00 [11884/20209 ( 59%)], Train Loss: 0.48316\n",
      "Epoch: 00 [11924/20209 ( 59%)], Train Loss: 0.48224\n",
      "Epoch: 00 [11964/20209 ( 59%)], Train Loss: 0.48205\n",
      "Epoch: 00 [12004/20209 ( 59%)], Train Loss: 0.48189\n",
      "Epoch: 00 [12044/20209 ( 60%)], Train Loss: 0.48180\n",
      "Epoch: 00 [12084/20209 ( 60%)], Train Loss: 0.48116\n",
      "Epoch: 00 [12124/20209 ( 60%)], Train Loss: 0.48094\n",
      "Epoch: 00 [12164/20209 ( 60%)], Train Loss: 0.47996\n",
      "Epoch: 00 [12204/20209 ( 60%)], Train Loss: 0.47930\n",
      "Epoch: 00 [12244/20209 ( 61%)], Train Loss: 0.47917\n",
      "Epoch: 00 [12284/20209 ( 61%)], Train Loss: 0.47833\n",
      "Epoch: 00 [12324/20209 ( 61%)], Train Loss: 0.47759\n",
      "Epoch: 00 [12364/20209 ( 61%)], Train Loss: 0.47733\n",
      "Epoch: 00 [12404/20209 ( 61%)], Train Loss: 0.47681\n",
      "Epoch: 00 [12444/20209 ( 62%)], Train Loss: 0.47692\n",
      "Epoch: 00 [12484/20209 ( 62%)], Train Loss: 0.47604\n",
      "Epoch: 00 [12524/20209 ( 62%)], Train Loss: 0.47537\n",
      "Epoch: 00 [12564/20209 ( 62%)], Train Loss: 0.47446\n",
      "Epoch: 00 [12604/20209 ( 62%)], Train Loss: 0.47459\n",
      "Epoch: 00 [12644/20209 ( 63%)], Train Loss: 0.47446\n",
      "Epoch: 00 [12684/20209 ( 63%)], Train Loss: 0.47387\n",
      "Epoch: 00 [12724/20209 ( 63%)], Train Loss: 0.47320\n",
      "Epoch: 00 [12764/20209 ( 63%)], Train Loss: 0.47352\n",
      "Epoch: 00 [12804/20209 ( 63%)], Train Loss: 0.47316\n",
      "Epoch: 00 [12844/20209 ( 64%)], Train Loss: 0.47300\n",
      "Epoch: 00 [12884/20209 ( 64%)], Train Loss: 0.47232\n",
      "Epoch: 00 [12924/20209 ( 64%)], Train Loss: 0.47156\n",
      "Epoch: 00 [12964/20209 ( 64%)], Train Loss: 0.47191\n",
      "Epoch: 00 [13004/20209 ( 64%)], Train Loss: 0.47122\n",
      "Epoch: 00 [13044/20209 ( 65%)], Train Loss: 0.47091\n",
      "Epoch: 00 [13084/20209 ( 65%)], Train Loss: 0.47062\n",
      "Epoch: 00 [13124/20209 ( 65%)], Train Loss: 0.47012\n",
      "Epoch: 00 [13164/20209 ( 65%)], Train Loss: 0.47019\n",
      "Epoch: 00 [13204/20209 ( 65%)], Train Loss: 0.46961\n",
      "Epoch: 00 [13244/20209 ( 66%)], Train Loss: 0.46935\n",
      "Epoch: 00 [13284/20209 ( 66%)], Train Loss: 0.46927\n",
      "Epoch: 00 [13324/20209 ( 66%)], Train Loss: 0.46839\n",
      "Epoch: 00 [13364/20209 ( 66%)], Train Loss: 0.46858\n",
      "Epoch: 00 [13404/20209 ( 66%)], Train Loss: 0.46814\n",
      "Epoch: 00 [13444/20209 ( 67%)], Train Loss: 0.46780\n",
      "Epoch: 00 [13484/20209 ( 67%)], Train Loss: 0.46754\n",
      "Epoch: 00 [13524/20209 ( 67%)], Train Loss: 0.46728\n",
      "Epoch: 00 [13564/20209 ( 67%)], Train Loss: 0.46722\n",
      "Epoch: 00 [13604/20209 ( 67%)], Train Loss: 0.46674\n",
      "Epoch: 00 [13644/20209 ( 68%)], Train Loss: 0.46637\n",
      "Epoch: 00 [13684/20209 ( 68%)], Train Loss: 0.46613\n",
      "Epoch: 00 [13724/20209 ( 68%)], Train Loss: 0.46540\n",
      "Epoch: 00 [13764/20209 ( 68%)], Train Loss: 0.46579\n",
      "Epoch: 00 [13804/20209 ( 68%)], Train Loss: 0.46511\n",
      "Epoch: 00 [13844/20209 ( 69%)], Train Loss: 0.46483\n",
      "Epoch: 00 [13884/20209 ( 69%)], Train Loss: 0.46466\n",
      "Epoch: 00 [13924/20209 ( 69%)], Train Loss: 0.46453\n",
      "Epoch: 00 [13964/20209 ( 69%)], Train Loss: 0.46432\n",
      "Epoch: 00 [14004/20209 ( 69%)], Train Loss: 0.46428\n",
      "Epoch: 00 [14044/20209 ( 69%)], Train Loss: 0.46358\n",
      "Epoch: 00 [14084/20209 ( 70%)], Train Loss: 0.46339\n",
      "Epoch: 00 [14124/20209 ( 70%)], Train Loss: 0.46304\n",
      "Epoch: 00 [14164/20209 ( 70%)], Train Loss: 0.46289\n",
      "Epoch: 00 [14204/20209 ( 70%)], Train Loss: 0.46251\n",
      "Epoch: 00 [14244/20209 ( 70%)], Train Loss: 0.46230\n",
      "Epoch: 00 [14284/20209 ( 71%)], Train Loss: 0.46184\n",
      "Epoch: 00 [14324/20209 ( 71%)], Train Loss: 0.46170\n",
      "Epoch: 00 [14364/20209 ( 71%)], Train Loss: 0.46106\n",
      "Epoch: 00 [14404/20209 ( 71%)], Train Loss: 0.46088\n",
      "Epoch: 00 [14444/20209 ( 71%)], Train Loss: 0.46077\n",
      "Epoch: 00 [14484/20209 ( 72%)], Train Loss: 0.46089\n",
      "Epoch: 00 [14524/20209 ( 72%)], Train Loss: 0.46038\n",
      "Epoch: 00 [14564/20209 ( 72%)], Train Loss: 0.46082\n",
      "Epoch: 00 [14604/20209 ( 72%)], Train Loss: 0.46045\n",
      "Epoch: 00 [14644/20209 ( 72%)], Train Loss: 0.46033\n",
      "Epoch: 00 [14684/20209 ( 73%)], Train Loss: 0.45944\n",
      "Epoch: 00 [14724/20209 ( 73%)], Train Loss: 0.45914\n",
      "Epoch: 00 [14764/20209 ( 73%)], Train Loss: 0.45880\n",
      "Epoch: 00 [14804/20209 ( 73%)], Train Loss: 0.45887\n",
      "Epoch: 00 [14844/20209 ( 73%)], Train Loss: 0.45862\n",
      "Epoch: 00 [14884/20209 ( 74%)], Train Loss: 0.45822\n",
      "Epoch: 00 [14924/20209 ( 74%)], Train Loss: 0.45782\n",
      "Epoch: 00 [14964/20209 ( 74%)], Train Loss: 0.45713\n",
      "Epoch: 00 [15004/20209 ( 74%)], Train Loss: 0.45676\n",
      "Epoch: 00 [15044/20209 ( 74%)], Train Loss: 0.45659\n",
      "Epoch: 00 [15084/20209 ( 75%)], Train Loss: 0.45619\n",
      "Epoch: 00 [15124/20209 ( 75%)], Train Loss: 0.45579\n",
      "Epoch: 00 [15164/20209 ( 75%)], Train Loss: 0.45565\n",
      "Epoch: 00 [15204/20209 ( 75%)], Train Loss: 0.45555\n",
      "Epoch: 00 [15244/20209 ( 75%)], Train Loss: 0.45532\n",
      "Epoch: 00 [15284/20209 ( 76%)], Train Loss: 0.45468\n",
      "Epoch: 00 [15324/20209 ( 76%)], Train Loss: 0.45433\n",
      "Epoch: 00 [15364/20209 ( 76%)], Train Loss: 0.45404\n",
      "Epoch: 00 [15404/20209 ( 76%)], Train Loss: 0.45360\n",
      "Epoch: 00 [15444/20209 ( 76%)], Train Loss: 0.45311\n",
      "Epoch: 00 [15484/20209 ( 77%)], Train Loss: 0.45237\n",
      "Epoch: 00 [15524/20209 ( 77%)], Train Loss: 0.45216\n",
      "Epoch: 00 [15564/20209 ( 77%)], Train Loss: 0.45165\n",
      "Epoch: 00 [15604/20209 ( 77%)], Train Loss: 0.45156\n",
      "Epoch: 00 [15644/20209 ( 77%)], Train Loss: 0.45140\n",
      "Epoch: 00 [15684/20209 ( 78%)], Train Loss: 0.45089\n",
      "Epoch: 00 [15724/20209 ( 78%)], Train Loss: 0.45083\n",
      "Epoch: 00 [15764/20209 ( 78%)], Train Loss: 0.45086\n",
      "Epoch: 00 [15804/20209 ( 78%)], Train Loss: 0.45099\n",
      "Epoch: 00 [15844/20209 ( 78%)], Train Loss: 0.45065\n",
      "Epoch: 00 [15884/20209 ( 79%)], Train Loss: 0.45040\n",
      "Epoch: 00 [15924/20209 ( 79%)], Train Loss: 0.45006\n",
      "Epoch: 00 [15964/20209 ( 79%)], Train Loss: 0.44978\n",
      "Epoch: 00 [16004/20209 ( 79%)], Train Loss: 0.44926\n",
      "Epoch: 00 [16044/20209 ( 79%)], Train Loss: 0.44911\n",
      "Epoch: 00 [16084/20209 ( 80%)], Train Loss: 0.44851\n",
      "Epoch: 00 [16124/20209 ( 80%)], Train Loss: 0.44793\n",
      "Epoch: 00 [16164/20209 ( 80%)], Train Loss: 0.44736\n",
      "Epoch: 00 [16204/20209 ( 80%)], Train Loss: 0.44676\n",
      "Epoch: 00 [16244/20209 ( 80%)], Train Loss: 0.44677\n",
      "Epoch: 00 [16284/20209 ( 81%)], Train Loss: 0.44677\n",
      "Epoch: 00 [16324/20209 ( 81%)], Train Loss: 0.44663\n",
      "Epoch: 00 [16364/20209 ( 81%)], Train Loss: 0.44673\n",
      "Epoch: 00 [16404/20209 ( 81%)], Train Loss: 0.44638\n",
      "Epoch: 00 [16444/20209 ( 81%)], Train Loss: 0.44588\n",
      "Epoch: 00 [16484/20209 ( 82%)], Train Loss: 0.44556\n",
      "Epoch: 00 [16524/20209 ( 82%)], Train Loss: 0.44534\n",
      "Epoch: 00 [16564/20209 ( 82%)], Train Loss: 0.44481\n",
      "Epoch: 00 [16604/20209 ( 82%)], Train Loss: 0.44449\n",
      "Epoch: 00 [16644/20209 ( 82%)], Train Loss: 0.44425\n",
      "Epoch: 00 [16684/20209 ( 83%)], Train Loss: 0.44401\n",
      "Epoch: 00 [16724/20209 ( 83%)], Train Loss: 0.44346\n",
      "Epoch: 00 [16764/20209 ( 83%)], Train Loss: 0.44365\n",
      "Epoch: 00 [16804/20209 ( 83%)], Train Loss: 0.44331\n",
      "Epoch: 00 [16844/20209 ( 83%)], Train Loss: 0.44371\n",
      "Epoch: 00 [16884/20209 ( 84%)], Train Loss: 0.44310\n",
      "Epoch: 00 [16924/20209 ( 84%)], Train Loss: 0.44279\n",
      "Epoch: 00 [16964/20209 ( 84%)], Train Loss: 0.44227\n",
      "Epoch: 00 [17004/20209 ( 84%)], Train Loss: 0.44203\n",
      "Epoch: 00 [17044/20209 ( 84%)], Train Loss: 0.44192\n",
      "Epoch: 00 [17084/20209 ( 85%)], Train Loss: 0.44177\n",
      "Epoch: 00 [17124/20209 ( 85%)], Train Loss: 0.44139\n",
      "Epoch: 00 [17164/20209 ( 85%)], Train Loss: 0.44135\n",
      "Epoch: 00 [17204/20209 ( 85%)], Train Loss: 0.44089\n",
      "Epoch: 00 [17244/20209 ( 85%)], Train Loss: 0.44107\n",
      "Epoch: 00 [17284/20209 ( 86%)], Train Loss: 0.44063\n",
      "Epoch: 00 [17324/20209 ( 86%)], Train Loss: 0.44031\n",
      "Epoch: 00 [17364/20209 ( 86%)], Train Loss: 0.44021\n",
      "Epoch: 00 [17404/20209 ( 86%)], Train Loss: 0.43977\n",
      "Epoch: 00 [17444/20209 ( 86%)], Train Loss: 0.43955\n",
      "Epoch: 00 [17484/20209 ( 87%)], Train Loss: 0.43923\n",
      "Epoch: 00 [17524/20209 ( 87%)], Train Loss: 0.43888\n",
      "Epoch: 00 [17564/20209 ( 87%)], Train Loss: 0.43833\n",
      "Epoch: 00 [17604/20209 ( 87%)], Train Loss: 0.43796\n",
      "Epoch: 00 [17644/20209 ( 87%)], Train Loss: 0.43735\n",
      "Epoch: 00 [17684/20209 ( 88%)], Train Loss: 0.43690\n",
      "Epoch: 00 [17724/20209 ( 88%)], Train Loss: 0.43652\n",
      "Epoch: 00 [17764/20209 ( 88%)], Train Loss: 0.43611\n",
      "Epoch: 00 [17804/20209 ( 88%)], Train Loss: 0.43628\n",
      "Epoch: 00 [17844/20209 ( 88%)], Train Loss: 0.43608\n",
      "Epoch: 00 [17884/20209 ( 88%)], Train Loss: 0.43586\n",
      "Epoch: 00 [17924/20209 ( 89%)], Train Loss: 0.43544\n",
      "Epoch: 00 [17964/20209 ( 89%)], Train Loss: 0.43522\n",
      "Epoch: 00 [18004/20209 ( 89%)], Train Loss: 0.43494\n",
      "Epoch: 00 [18044/20209 ( 89%)], Train Loss: 0.43470\n",
      "Epoch: 00 [18084/20209 ( 89%)], Train Loss: 0.43482\n",
      "Epoch: 00 [18124/20209 ( 90%)], Train Loss: 0.43466\n",
      "Epoch: 00 [18164/20209 ( 90%)], Train Loss: 0.43478\n",
      "Epoch: 00 [18204/20209 ( 90%)], Train Loss: 0.43439\n",
      "Epoch: 00 [18244/20209 ( 90%)], Train Loss: 0.43461\n",
      "Epoch: 00 [18284/20209 ( 90%)], Train Loss: 0.43441\n",
      "Epoch: 00 [18324/20209 ( 91%)], Train Loss: 0.43426\n",
      "Epoch: 00 [18364/20209 ( 91%)], Train Loss: 0.43415\n",
      "Epoch: 00 [18404/20209 ( 91%)], Train Loss: 0.43399\n",
      "Epoch: 00 [18444/20209 ( 91%)], Train Loss: 0.43357\n",
      "Epoch: 00 [18484/20209 ( 91%)], Train Loss: 0.43312\n",
      "Epoch: 00 [18524/20209 ( 92%)], Train Loss: 0.43292\n",
      "Epoch: 00 [18564/20209 ( 92%)], Train Loss: 0.43267\n",
      "Epoch: 00 [18604/20209 ( 92%)], Train Loss: 0.43312\n",
      "Epoch: 00 [18644/20209 ( 92%)], Train Loss: 0.43262\n",
      "Epoch: 00 [18684/20209 ( 92%)], Train Loss: 0.43250\n",
      "Epoch: 00 [18724/20209 ( 93%)], Train Loss: 0.43263\n",
      "Epoch: 00 [18764/20209 ( 93%)], Train Loss: 0.43231\n",
      "Epoch: 00 [18804/20209 ( 93%)], Train Loss: 0.43217\n",
      "Epoch: 00 [18844/20209 ( 93%)], Train Loss: 0.43175\n",
      "Epoch: 00 [18884/20209 ( 93%)], Train Loss: 0.43140\n",
      "Epoch: 00 [18924/20209 ( 94%)], Train Loss: 0.43074\n",
      "Epoch: 00 [18964/20209 ( 94%)], Train Loss: 0.43051\n",
      "Epoch: 00 [19004/20209 ( 94%)], Train Loss: 0.43006\n",
      "Epoch: 00 [19044/20209 ( 94%)], Train Loss: 0.42970\n",
      "Epoch: 00 [19084/20209 ( 94%)], Train Loss: 0.42929\n",
      "Epoch: 00 [19124/20209 ( 95%)], Train Loss: 0.42903\n",
      "Epoch: 00 [19164/20209 ( 95%)], Train Loss: 0.42897\n",
      "Epoch: 00 [19204/20209 ( 95%)], Train Loss: 0.42874\n",
      "Epoch: 00 [19244/20209 ( 95%)], Train Loss: 0.42828\n",
      "Epoch: 00 [19284/20209 ( 95%)], Train Loss: 0.42805\n",
      "Epoch: 00 [19324/20209 ( 96%)], Train Loss: 0.42817\n",
      "Epoch: 00 [19364/20209 ( 96%)], Train Loss: 0.42806\n",
      "Epoch: 00 [19404/20209 ( 96%)], Train Loss: 0.42778\n",
      "Epoch: 00 [19444/20209 ( 96%)], Train Loss: 0.42747\n",
      "Epoch: 00 [19484/20209 ( 96%)], Train Loss: 0.42761\n",
      "Epoch: 00 [19524/20209 ( 97%)], Train Loss: 0.42740\n",
      "Epoch: 00 [19564/20209 ( 97%)], Train Loss: 0.42709\n",
      "Epoch: 00 [19604/20209 ( 97%)], Train Loss: 0.42695\n",
      "Epoch: 00 [19644/20209 ( 97%)], Train Loss: 0.42639\n",
      "Epoch: 00 [19684/20209 ( 97%)], Train Loss: 0.42623\n",
      "Epoch: 00 [19724/20209 ( 98%)], Train Loss: 0.42643\n",
      "Epoch: 00 [19764/20209 ( 98%)], Train Loss: 0.42624\n",
      "Epoch: 00 [19804/20209 ( 98%)], Train Loss: 0.42624\n",
      "Epoch: 00 [19844/20209 ( 98%)], Train Loss: 0.42594\n",
      "Epoch: 00 [19884/20209 ( 98%)], Train Loss: 0.42575\n",
      "Epoch: 00 [19924/20209 ( 99%)], Train Loss: 0.42567\n",
      "Epoch: 00 [19964/20209 ( 99%)], Train Loss: 0.42512\n",
      "Epoch: 00 [20004/20209 ( 99%)], Train Loss: 0.42493\n",
      "Epoch: 00 [20044/20209 ( 99%)], Train Loss: 0.42499\n",
      "Epoch: 00 [20084/20209 ( 99%)], Train Loss: 0.42478\n",
      "Epoch: 00 [20124/20209 (100%)], Train Loss: 0.42457\n",
      "Epoch: 00 [20164/20209 (100%)], Train Loss: 0.42437\n",
      "Epoch: 00 [20204/20209 (100%)], Train Loss: 0.42383\n",
      "Epoch: 00 [20209/20209 (100%)], Train Loss: 0.42376\n",
      "----Validation Results Summary----\n",
      "Epoch: [0] Valid Loss: 0.18992\n",
      "Post-processing 222 example predictions split into 2860 features.\n",
      "valid jaccard:  0.7201951951951953\n",
      "0 Epoch, Best epoch was updated! Valid Loss: 0.18992\n",
      "Saving model checkpoint to output/checkpoint-fold-4-epoch-0.\n",
      "\n",
      "Total Training Time: 2888.429162979126secs, Average Training Time per Epoch: 2888.429162979126secs.\n",
      "Total Validation Time: 171.62748408317566secs, Average Validation Time per Epoch: 171.62748408317566secs.\n",
      "**************************************************\n",
      "Final jacard scores, 5-fold:  [0.67704 0.67085 0.69563 0.64434 0.7202 ]\n",
      "Average jacard: 0.6816091483019735\n",
      "**************************************************\n"
     ]
    }
   ],
   "source": [
    "for fold in range(5):\n",
    "  print();print()\n",
    "  print('-'*50)\n",
    "  print(f'FOLD: {fold}')\n",
    "  print('-'*50)\n",
    "  run(train, fold)\n",
    "\n",
    "print(\"*\"*50)\n",
    "print(\"Final jacard scores, 5-fold: \", np.round(all_jacard_scores,5))\n",
    "print(\"Average jacard:\",np.mean(all_jacard_scores))\n",
    "print(\"*\"*50)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.10"
  },
  "papermill": {
   "default_parameters": {},
   "duration": 15906.703645,
   "end_time": "2021-11-06T16:11:23.133837",
   "environment_variables": {},
   "exception": null,
   "input_path": "__notebook__.ipynb",
   "output_path": "__notebook__.ipynb",
   "parameters": {},
   "start_time": "2021-11-06T11:46:16.430192",
   "version": "2.3.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
