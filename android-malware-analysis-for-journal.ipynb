{"cells":[{"metadata":{},"cell_type":"markdown","source":"# Android Malware Analysis"},{"metadata":{},"cell_type":"markdown","source":"## Android\n\n\nAndroid is one of the most used mobile operating systems worldwide. Due to its technological impact, its open-source code and the possibility of installing applications from third parties without any central control, Android has recently become a malware target. Even if it includes security mechanisms, the last news about malicious activities and Android´s vulnerabilities point to the importance of continuing the development of methods and frameworks to improve its security.\n\nTo prevent malware attacks, researches and developers have proposed different security solutions, applying static analysis, dynamic analysis, and artificial intelligence. Indeed, data science has become a promising area in cybersecurity, since analytical models based on data allow for the discovery of insights that can help to predict malicious activities.\n\nWe can analyze cyber threats using two techniques, static analysis, and dynamic analysis, the most important thing is that these are the approaches to get the features that we are going to use in data science.\n\n+ **Static analysis**: it includes the methods that allow us to get information about the software that we want to analyze without executing it, one example of them is the study of the code, their callings, resources, etc.\n+ **Dynamic analysis**: it is another approach where the idea is to analyze the cyber threat during its execution, in other words, get information about its behavior, some of their features are the netflows.\n"},{"metadata":{},"cell_type":"markdown","source":"# State of the Art\n\nIn 2016 we published an article [2] about the state of the art of frameworks and results about Android malware detection. This work reflects different static analysis tools (TaintDroid, Stowaway, Crowdroid y Airmid), dynamic analysis systems (Paranoid and DroidMOSS), frameworks (MobSafe, SAAF, and ASEF) and some research results about using machine learning. From this article we concluded that the idea is using both static and dynamic analysis in order to get spectra of features, moreover, some works have been working to use virtual devices in the cloud.\n"},{"metadata":{},"cell_type":"markdown","source":"# Datasets\n\nIn 2016 we explored [3] Android Genome Project (MalGenome), it is a dataset which was active from 2012 until the end of the year 2015, this set of malware has a size of 1260 applications, grouped into a total of 49 families. Today, we can find other jobs such as: Drebin, a research project offering a total of 5560 applications consisting of 179 malware families; AndrooZoo, which includes a collection of 5669661 applications Android from different sources (including Google Play); VirusShare, another repository that provides samples of malware for cybersecurity researchers; and DroidCollector, this is another set which provides around 8000 benign applications and 5560 malware samples, moreover, it facilitates us samples of network traffic as pcap files."},{"metadata":{},"cell_type":"markdown","source":"# Static Analysis \n\nIn this first step, I'm going to analyze some features in order to answer the next hypothesis, *exist a differential of the permissions used between a set of malware and benign samples*, in other words… \n\n<img src=\"https://pics.me.me/when-the-flashlight-app-wants-access-to-your-call-history-32812256.png\" height=\"250\" width=\"250\">\n\nFor this approach, I developed a code that consisted to extract and make a CSV file which has information about permissions of applications, through this script you can map each APK (Android Application Package) against a list of permissions. You can find more information about the proposed framework at [3]\n\nhttps://github.com/urcuqui/WhiteHat"},{"metadata":{"_uuid":"d629ff2d2480ee46fbb7e2d37f6b5fab8052498a","collapsed":true,"_cell_guid":"79c7e3d0-c299-4dcb-8224-4455121ee9b0","trusted":false},"cell_type":"markdown","source":"### Packages"},{"metadata":{"trusted":true},"cell_type":"code","source":"from sklearn.naive_bayes import GaussianNB, BernoulliNB\nfrom sklearn.metrics import accuracy_score, classification_report\nfrom sklearn.ensemble import BaggingClassifier\nfrom sklearn.neighbors import KNeighborsClassifier\nfrom sklearn.linear_model import SGDClassifier\nfrom sklearn.model_selection import train_test_split\nfrom sklearn.metrics import cohen_kappa_score\nfrom sklearn.metrics import confusion_matrix\nfrom sklearn.ensemble import RandomForestClassifier\n\nfrom sklearn import preprocessing\nimport torch\nfrom sklearn import svm\nfrom sklearn import tree\nimport pandas as pd\nfrom sklearn.externals import joblib\nimport pickle\nimport numpy as np\nimport seaborn as sns\n\nfrom sklearn.metrics import f1_score\nfrom sklearn.metrics import precision_score\nfrom sklearn.metrics import recall_score\nfrom sklearn.metrics import average_precision_score\nfrom sklearn.metrics import precision_recall_curve\nimport matplotlib.pyplot as plt\n\nfrom sklearn.pipeline import make_pipeline\nfrom sklearn.preprocessing import Imputer\nfrom sklearn.model_selection import cross_val_score\n\nfrom tensorflow.keras.models import Sequential\nfrom tensorflow.keras.layers import Dense, Flatten, Conv2D, MaxPooling2D,Dropout\nfrom tensorflow.keras.losses import sparse_categorical_crossentropy\nfrom tensorflow.keras.optimizers import Adam\nfrom sklearn.model_selection import KFold\nfrom sklearn.svm import SVR\nfrom sklearn.preprocessing import MinMaxScaler\nfrom sklearn.metrics import confusion_matrix,accuracy_score,precision_score,f1_score\nfrom collections import Counter\nfrom imblearn.over_sampling import SMOTE\nfrom sklearn.metrics import brier_score_loss\nfrom sklearn.metrics import roc_auc_score\n\nfrom sklearn.feature_selection import SelectKBest \nfrom sklearn.feature_selection import chi2","execution_count":1,"outputs":[{"output_type":"stream","text":"/opt/conda/lib/python3.6/site-packages/sklearn/externals/joblib/__init__.py:15: DeprecationWarning: sklearn.externals.joblib is deprecated in 0.21 and will be removed in 0.23. Please import this functionality directly from joblib, which can be installed with: pip install joblib. If this warning is raised when loading pickled models, you may need to re-serialize those models with scikit-learn 0.21+.\n  warnings.warn(msg, category=DeprecationWarning)\nUsing TensorFlow backend.\n","name":"stderr"}]},{"metadata":{},"cell_type":"markdown","source":"### Exploratory "},{"metadata":{},"cell_type":"markdown","source":"For the next analysis, I'm going to explore the Malgenome dataset, as I said nowadays we can find other sources with a lot of examples and malware families which would be important for future works, the idea of the next experiment and results is to show our first approached.\n\n<img src=\"https://ieee-dataport.org/sites/default/files/styles/large/public/Sin%20t%C3%ADtulo.png?itok=4YoQim00\" /> "},{"metadata":{},"cell_type":"markdown","source":"*It is not neccesary to apply a data cleaning and transformation process*"},{"metadata":{"trusted":true},"cell_type":"code","source":"import pandas as pd\ndf = pd.read_csv(\"../input/datasetandroidpermissions/train.csv\", sep=\";\")","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"df = df.astype(\"int64\")\ndf.type.value_counts()","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"Type is the label that represents if an application is a malware or not, as we can see this dataset is balanced."},{"metadata":{"trusted":true},"cell_type":"code","source":"df.shape","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"*Let's get the top 10 of permissions that are used for our malware samples*"},{"metadata":{},"cell_type":"markdown","source":"*Malicious*"},{"metadata":{"trusted":true},"cell_type":"code","source":"pd.Series.sort_values(df[df.type==1].sum(axis=0), ascending=False)[1:11]","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"*Benign*"},{"metadata":{"trusted":true},"cell_type":"code","source":"pd.Series.sort_values(df[df.type==0].sum(axis=0), ascending=False)[:10]","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"import matplotlib.pyplot as plt\nfig, axs =  plt.subplots(nrows=2, sharex=True)\n\npd.Series.sort_values(df[df.type==0].sum(axis=0), ascending=False)[:10].plot.bar(ax=axs[0])\npd.Series.sort_values(df[df.type==1].sum(axis=0), ascending=False)[1:11].plot.bar(ax=axs[1], color=\"red\")","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"The last outputs allow us to get insights about a difference between the permissions used by the malware and the benign applications."},{"metadata":{},"cell_type":"markdown","source":"### Modeling"},{"metadata":{"trusted":true},"cell_type":"code","source":"X_train, X_test, y_train, y_test = train_test_split(df.iloc[:, 1:330], df['type'], test_size=0.20, random_state=42)","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"*Naive Bayes algorithm*"},{"metadata":{"trusted":true},"cell_type":"code","source":"#Cross validation\nX_train=df.iloc[:, 1:330]\ny_train=df['type']\ngnb = GaussianNB()\n\ndef custom_cross_validation1(model, X_train, y_train, cv):\n    my_pipeline = make_pipeline(Imputer(), model)\n    scores = cross_val_score(my_pipeline, X_train, y_train, cv=cv)\n    print(\"%0.2f accuracy with a standard deviation of %0.2f\" % (scores.mean(), scores.std()))\n    return scores\n\n#sc=custom_cross_validation(gnb, X_train, y_train, cv=10)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"#R square\ndef r2(y_true,y_pred):\n    m_t_v=np.mean(y_true)\n    numerator=0\n    denominator=0\n    for yt,yp in zip(y_true,y_pred):\n        numerator +=(yt-yp)**2\n        denominator +=(yt-m_t_v)**2\n    r=numerator/denominator\n    return 1-r","execution_count":2,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"#MCC\ndef mcc(y_true,y_pred):\n    #tp=true_positive(y_true,y_pred)\n    #tn=true_negative(y_true,y_pred)\n    #fp=false_positive(y_true,y_pred)\n    #fn=false_negative(y_true,y_pred)\n    tn, fp, fn, tp = confusion_matrix(y_true,y_pred).ravel()\n    n=(tp*tn)-(fp*fn)\n    d=((tp+fp)*(fn+tn)*(fp+tn)*(tp+fn))\n    d=d**0.5\n    r=n/d\n    return r","execution_count":3,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"def scoreset(y_true,y_pred):\n    score_set=[]\n    pred=y_pred\n    y_test=y_true\n    print(\"cohen kappa score\",cohen_kappa_score(y_test, pred))\n    score_set.append(cohen_kappa_score(y_test, pred))\n    print(\"cohen kappa score quadratic\",cohen_kappa_score(y_test, pred, weights=\"quadratic\"))\n    score_set.append(cohen_kappa_score(y_test, pred, weights=\"quadratic\"))\n    print(\"R square score\", r2(y_test,pred))\n    score_set.append(r2(y_test,pred))\n    print(\"MCC score\", mcc(y_test,pred))\n    score_set.append(mcc(y_test,pred))\n    print(\"Brier Score Loss\",brier_score_loss(y_test, pred))\n    score_set.append(brier_score_loss(y_test, pred))\n    print(\"AUC ROC Score\",roc_auc_score(y_test, pred))\n    score_set.append(roc_auc_score(y_test, pred))\n    print(\"\")\n    print(\"accuracy:\",accuracy_score(y_test,pred))\n    score_set.append(accuracy_score(y_test,pred))\n    print(\"F1 Score:\",f1_score(y_test,pred))\n    score_set.append(f1_score(y_test,pred))\n    print(\"Precision:\",precision_score(y_test,pred))\n    score_set.append(precision_score(y_test,pred))\n    print(\"Recall:\",recall_score(y_test,pred))\n    score_set.append(recall_score(y_test,pred))\n    return score_set\n    ","execution_count":4,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"# Naive Bayes algorithm\ngnb = GaussianNB()\ngnb.fit(X_train, y_train)\n\n# pred\npred = gnb.predict(X_test)\n\n# accuracy\naccuracy = accuracy_score(pred, y_test)\nprint(\"naive_bayes\")\nprint(classification_report(pred, y_test, labels=None))\n\n\n#print(\"cohen kappa score\",cohen_kappa_score(y_test, pred))\n#print(\"cohen kappa score quadratic\",cohen_kappa_score(y_test, pred, weights=\"quadratic\"))\n#print(\"R square score\", r2(y_test,pred))\n#print(\"MCC score\", mcc(y_test,pred))\n#print(\"Brier Score Loss\",brier_score_loss(y_test, pred))\n#print(\"AUC ROC Score\",roc_auc_score(y_test, pred))\n#print(\"\")\n#print(\"accuracy:\",accuracy)\n#print(\"F1 Score:\",f1_score(pred, y_test))\n#print(\"Precision:\",precision_score(pred, y_test))\n#print(\"Recall:\",recall_score(pred, y_test))\n\ns=scoreset(y_test,pred)\n#print(s)","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"*kneighbors algorithm*"},{"metadata":{"trusted":true},"cell_type":"code","source":"# kneighbors algorithm\n\nfor i in range(3,15,3):\n    \n    neigh = KNeighborsClassifier(n_neighbors=i)\n    neigh.fit(X_train, y_train)\n    pred = neigh.predict(X_test)\n    # accuracy\n    accuracy = accuracy_score(pred, y_test)\n    print(\"kneighbors {}\".format(i))\n    print(accuracy)\n    print(classification_report(pred, y_test, labels=None))\n    s=scoreset(y_test,pred)\n    \n    ","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"*Decision Tree*"},{"metadata":{"trusted":true},"cell_type":"code","source":"clf = tree.DecisionTreeClassifier()\nclf.fit(X_train, y_train)\n\n# Read the csv test file\n\npred = clf.predict(X_test)\n# accuracy\naccuracy = accuracy_score(pred, y_test)\nprint(clf)\nprint(accuracy)\nprint(classification_report(pred, y_test, labels=None))\ns=scoreset(y_test,pred)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"#LinearSVM\nfrom sklearn import svm\n\nlin_clf = svm.LinearSVC()\nlin_clf.fit(X_train, y_train)\npred = lin_clf.predict(X_test)\n# accuracy\naccuracy = accuracy_score(pred, y_test)\nprint(accuracy)\nprint(classification_report(pred, y_test, labels=None))\ns=scoreset(y_test,pred)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"import xgboost as xgb\n\nxgb_clf = xgb.XGBClassifier()\nxgb_clf = xgb_clf.fit(X_train, y_train)\npred=xgb_clf.predict(X_test)\ncm=confusion_matrix(y_test, pred)\n\naccuracy = accuracy_score(y_test,pred)\nprint(accuracy)\nprint(classification_report(y_test,pred, labels=None))\ns=scoreset(y_test,pred)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"from sklearn.linear_model import LogisticRegression\n\nclf = LogisticRegression(random_state=0).fit(X_train, y_train)\npred=clf.predict(X_test)\ncm=confusion_matrix(y_test, pred)\n\naccuracy = accuracy_score(y_test,pred)\nprint(accuracy)\nprint(classification_report(y_test,pred, labels=None))\nprint(\"cohen kappa score\")\nprint(cohen_kappa_score(y_test, pred))\nprint(cm)\npred=pred.astype(\"int64\")\ns=scoreset(y_test,pred)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"from sklearn.ensemble import AdaBoostClassifier\n# Create adaboost classifer object\nabc = AdaBoostClassifier(n_estimators=50,\n                         learning_rate=1.3)#vary the learning rate from 0 to 1.5\n# Train Adaboost Classifer\nmodel = abc.fit(X_train, y_train)\n\n#Predict the response for test dataset\npred = model.predict(X_test)\ncm=confusion_matrix(y_test, pred)\n\naccuracy = accuracy_score(y_test,pred)\nprint(accuracy)\nprint(classification_report(y_test,pred, labels=None))\nprint(\"cohen kappa score\")\nprint(cohen_kappa_score(y_test, pred))\nprint(cm)\npred=pred.astype(\"int64\")\ns=scoreset(y_test,pred)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"rdF=RandomForestClassifier(n_estimators=250, max_depth=50,random_state=45)\nrdF.fit(X_train,y_train)\npred=rdF.predict(X_test)\ncm=confusion_matrix(y_test, pred)\n\naccuracy = accuracy_score(y_test,pred)\nprint(rdF)\nprint(accuracy)\nprint(classification_report(y_test,pred, labels=None))\nprint(\"cohen kappa score\")\nprint(cohen_kappa_score(y_test, pred))\nprint(cm)\npred=pred.astype(\"int64\")\ns=scoreset(y_test,pred)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"\n\n# Two features with highest chi-squared statistics are selected \nchi2_features = SelectKBest(chi2, k = 300) \nX_kbest_features = chi2_features.fit_transform(X_train.astype(), y_train) \n  \n# Reduced features \nprint('Original feature number:', X_train.shape[1]) \nprint('Reduced feature number:', X_kbest_features.shape[1]) \nprint(X_kbest_features)","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"Through the last results we can see how we trained different classifiers to detect malware using its permissions, but as I said this is only a first approximation, I didn't analyze the hyperparameters and others things to improve the results."},{"metadata":{},"cell_type":"markdown","source":"# Dynamic Analysis"},{"metadata":{},"cell_type":"markdown","source":"For this approach, we used a set of pcap files from the DroidCollector project integrated by 4705 benign and 7846 malicious applications. All of the files were processed by our feature extractor script (a result from [4]), the idea of this analysis is to answer the next question, according to the static analysis previously seen a lot of applications use a network connection, in other words, they are trying to communicate or transmit information, so.. is it possible to distinguish between malware and benign application using network traffic?"},{"metadata":{"trusted":true},"cell_type":"code","source":"import pandas as pd\ndata = pd.read_csv(\"../input/network-traffic-android-malware/android_traffic.csv\", sep=\";\")\ndata.head()","execution_count":5,"outputs":[{"output_type":"execute_result","execution_count":5,"data":{"text/plain":"        name  tcp_packets  dist_port_tcp  external_ips  vulume_bytes  \\\n0  AntiVirus           36              6             3          3911   \n1  AntiVirus          117              0             9         23514   \n2  AntiVirus          196              0             6         24151   \n3  AntiVirus            6              0             1           889   \n4  AntiVirus            6              0             1           882   \n\n   udp_packets  tcp_urg_packet  source_app_packets  remote_app_packets  \\\n0            0               0                  39                  33   \n1            0               0                 128                 107   \n2            0               0                 205                 214   \n3            0               0                   7                   6   \n4            0               0                   7                   6   \n\n   source_app_bytes  remote_app_bytes  duracion  avg_local_pkt_rate  \\\n0              5100              4140       NaN                 NaN   \n1             26248             24358       NaN                 NaN   \n2            163887             24867       NaN                 NaN   \n3               819               975       NaN                 NaN   \n4               819               968       NaN                 NaN   \n\n   avg_remote_pkt_rate  source_app_packets.1  dns_query_times    type  \n0                  NaN                    39                3  benign  \n1                  NaN                   128               11  benign  \n2                  NaN                   205                9  benign  \n3                  NaN                     7                1  benign  \n4                  NaN                     7                1  benign  ","text/html":"<div>\n<style scoped>\n    .dataframe tbody tr th:only-of-type {\n        vertical-align: middle;\n    }\n\n    .dataframe tbody tr th {\n        vertical-align: top;\n    }\n\n    .dataframe thead th {\n        text-align: right;\n    }\n</style>\n<table border=\"1\" class=\"dataframe\">\n  <thead>\n    <tr style=\"text-align: right;\">\n      <th></th>\n      <th>name</th>\n      <th>tcp_packets</th>\n      <th>dist_port_tcp</th>\n      <th>external_ips</th>\n      <th>vulume_bytes</th>\n      <th>udp_packets</th>\n      <th>tcp_urg_packet</th>\n      <th>source_app_packets</th>\n      <th>remote_app_packets</th>\n      <th>source_app_bytes</th>\n      <th>remote_app_bytes</th>\n      <th>duracion</th>\n      <th>avg_local_pkt_rate</th>\n      <th>avg_remote_pkt_rate</th>\n      <th>source_app_packets.1</th>\n      <th>dns_query_times</th>\n      <th>type</th>\n    </tr>\n  </thead>\n  <tbody>\n    <tr>\n      <th>0</th>\n      <td>AntiVirus</td>\n      <td>36</td>\n      <td>6</td>\n      <td>3</td>\n      <td>3911</td>\n      <td>0</td>\n      <td>0</td>\n      <td>39</td>\n      <td>33</td>\n      <td>5100</td>\n      <td>4140</td>\n      <td>NaN</td>\n      <td>NaN</td>\n      <td>NaN</td>\n      <td>39</td>\n      <td>3</td>\n      <td>benign</td>\n    </tr>\n    <tr>\n      <th>1</th>\n      <td>AntiVirus</td>\n      <td>117</td>\n      <td>0</td>\n      <td>9</td>\n      <td>23514</td>\n      <td>0</td>\n      <td>0</td>\n      <td>128</td>\n      <td>107</td>\n      <td>26248</td>\n      <td>24358</td>\n      <td>NaN</td>\n      <td>NaN</td>\n      <td>NaN</td>\n      <td>128</td>\n      <td>11</td>\n      <td>benign</td>\n    </tr>\n    <tr>\n      <th>2</th>\n      <td>AntiVirus</td>\n      <td>196</td>\n      <td>0</td>\n      <td>6</td>\n      <td>24151</td>\n      <td>0</td>\n      <td>0</td>\n      <td>205</td>\n      <td>214</td>\n      <td>163887</td>\n      <td>24867</td>\n      <td>NaN</td>\n      <td>NaN</td>\n      <td>NaN</td>\n      <td>205</td>\n      <td>9</td>\n      <td>benign</td>\n    </tr>\n    <tr>\n      <th>3</th>\n      <td>AntiVirus</td>\n      <td>6</td>\n      <td>0</td>\n      <td>1</td>\n      <td>889</td>\n      <td>0</td>\n      <td>0</td>\n      <td>7</td>\n      <td>6</td>\n      <td>819</td>\n      <td>975</td>\n      <td>NaN</td>\n      <td>NaN</td>\n      <td>NaN</td>\n      <td>7</td>\n      <td>1</td>\n      <td>benign</td>\n    </tr>\n    <tr>\n      <th>4</th>\n      <td>AntiVirus</td>\n      <td>6</td>\n      <td>0</td>\n      <td>1</td>\n      <td>882</td>\n      <td>0</td>\n      <td>0</td>\n      <td>7</td>\n      <td>6</td>\n      <td>819</td>\n      <td>968</td>\n      <td>NaN</td>\n      <td>NaN</td>\n      <td>NaN</td>\n      <td>7</td>\n      <td>1</td>\n      <td>benign</td>\n    </tr>\n  </tbody>\n</table>\n</div>"},"metadata":{}}]},{"metadata":{"trusted":true},"cell_type":"code","source":"#replacing null with 0\ndata.fillna(0)","execution_count":6,"outputs":[{"output_type":"execute_result","execution_count":6,"data":{"text/plain":"           name  tcp_packets  dist_port_tcp  external_ips  vulume_bytes  \\\n0     AntiVirus           36              6             3          3911   \n1     AntiVirus          117              0             9         23514   \n2     AntiVirus          196              0             6         24151   \n3     AntiVirus            6              0             1           889   \n4     AntiVirus            6              0             1           882   \n...         ...          ...            ...           ...           ...   \n7840      Zsone            0              0             0             0   \n7841      Zsone            4              4             1           296   \n7842      Zsone            0              0             0             0   \n7843      Zsone            0              0             0             0   \n7844      Zsone            0              0             0             0   \n\n      udp_packets  tcp_urg_packet  source_app_packets  remote_app_packets  \\\n0               0               0                  39                  33   \n1               0               0                 128                 107   \n2               0               0                 205                 214   \n3               0               0                   7                   6   \n4               0               0                   7                   6   \n...           ...             ...                 ...                 ...   \n7840            0               0                   2                   2   \n7841            0               0                   5                   1   \n7842            0               0                   2                   2   \n7843            0               0                   2                   2   \n7844            0               0                   2                   2   \n\n      source_app_bytes  remote_app_bytes  duracion  avg_local_pkt_rate  \\\n0                 5100              4140       0.0                 0.0   \n1                26248             24358       0.0                 0.0   \n2               163887             24867       0.0                 0.0   \n3                  819               975       0.0                 0.0   \n4                  819               968       0.0                 0.0   \n...                ...               ...       ...                 ...   \n7840               257               143       0.0                 0.0   \n7841                86               382       0.0                 0.0   \n7842               257               143       0.0                 0.0   \n7843               257               143       0.0                 0.0   \n7844               257               143       0.0                 0.0   \n\n      avg_remote_pkt_rate  source_app_packets.1  dns_query_times       type  \n0                     0.0                    39                3     benign  \n1                     0.0                   128               11     benign  \n2                     0.0                   205                9     benign  \n3                     0.0                     7                1     benign  \n4                     0.0                     7                1     benign  \n...                   ...                   ...              ...        ...  \n7840                  0.0                     2                2  malicious  \n7841                  0.0                     5                1  malicious  \n7842                  0.0                     2                2  malicious  \n7843                  0.0                     2                2  malicious  \n7844                  0.0                     2                2  malicious  \n\n[7845 rows x 17 columns]","text/html":"<div>\n<style scoped>\n    .dataframe tbody tr th:only-of-type {\n        vertical-align: middle;\n    }\n\n    .dataframe tbody tr th {\n        vertical-align: top;\n    }\n\n    .dataframe thead th {\n        text-align: right;\n    }\n</style>\n<table border=\"1\" class=\"dataframe\">\n  <thead>\n    <tr style=\"text-align: right;\">\n      <th></th>\n      <th>name</th>\n      <th>tcp_packets</th>\n      <th>dist_port_tcp</th>\n      <th>external_ips</th>\n      <th>vulume_bytes</th>\n      <th>udp_packets</th>\n      <th>tcp_urg_packet</th>\n      <th>source_app_packets</th>\n      <th>remote_app_packets</th>\n      <th>source_app_bytes</th>\n      <th>remote_app_bytes</th>\n      <th>duracion</th>\n      <th>avg_local_pkt_rate</th>\n      <th>avg_remote_pkt_rate</th>\n      <th>source_app_packets.1</th>\n      <th>dns_query_times</th>\n      <th>type</th>\n    </tr>\n  </thead>\n  <tbody>\n    <tr>\n      <th>0</th>\n      <td>AntiVirus</td>\n      <td>36</td>\n      <td>6</td>\n      <td>3</td>\n      <td>3911</td>\n      <td>0</td>\n      <td>0</td>\n      <td>39</td>\n      <td>33</td>\n      <td>5100</td>\n      <td>4140</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>39</td>\n      <td>3</td>\n      <td>benign</td>\n    </tr>\n    <tr>\n      <th>1</th>\n      <td>AntiVirus</td>\n      <td>117</td>\n      <td>0</td>\n      <td>9</td>\n      <td>23514</td>\n      <td>0</td>\n      <td>0</td>\n      <td>128</td>\n      <td>107</td>\n      <td>26248</td>\n      <td>24358</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>128</td>\n      <td>11</td>\n      <td>benign</td>\n    </tr>\n    <tr>\n      <th>2</th>\n      <td>AntiVirus</td>\n      <td>196</td>\n      <td>0</td>\n      <td>6</td>\n      <td>24151</td>\n      <td>0</td>\n      <td>0</td>\n      <td>205</td>\n      <td>214</td>\n      <td>163887</td>\n      <td>24867</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>205</td>\n      <td>9</td>\n      <td>benign</td>\n    </tr>\n    <tr>\n      <th>3</th>\n      <td>AntiVirus</td>\n      <td>6</td>\n      <td>0</td>\n      <td>1</td>\n      <td>889</td>\n      <td>0</td>\n      <td>0</td>\n      <td>7</td>\n      <td>6</td>\n      <td>819</td>\n      <td>975</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>7</td>\n      <td>1</td>\n      <td>benign</td>\n    </tr>\n    <tr>\n      <th>4</th>\n      <td>AntiVirus</td>\n      <td>6</td>\n      <td>0</td>\n      <td>1</td>\n      <td>882</td>\n      <td>0</td>\n      <td>0</td>\n      <td>7</td>\n      <td>6</td>\n      <td>819</td>\n      <td>968</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>7</td>\n      <td>1</td>\n      <td>benign</td>\n    </tr>\n    <tr>\n      <th>...</th>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n    </tr>\n    <tr>\n      <th>7840</th>\n      <td>Zsone</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>2</td>\n      <td>2</td>\n      <td>257</td>\n      <td>143</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>2</td>\n      <td>2</td>\n      <td>malicious</td>\n    </tr>\n    <tr>\n      <th>7841</th>\n      <td>Zsone</td>\n      <td>4</td>\n      <td>4</td>\n      <td>1</td>\n      <td>296</td>\n      <td>0</td>\n      <td>0</td>\n      <td>5</td>\n      <td>1</td>\n      <td>86</td>\n      <td>382</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>5</td>\n      <td>1</td>\n      <td>malicious</td>\n    </tr>\n    <tr>\n      <th>7842</th>\n      <td>Zsone</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>2</td>\n      <td>2</td>\n      <td>257</td>\n      <td>143</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>2</td>\n      <td>2</td>\n      <td>malicious</td>\n    </tr>\n    <tr>\n      <th>7843</th>\n      <td>Zsone</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>2</td>\n      <td>2</td>\n      <td>257</td>\n      <td>143</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>2</td>\n      <td>2</td>\n      <td>malicious</td>\n    </tr>\n    <tr>\n      <th>7844</th>\n      <td>Zsone</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>2</td>\n      <td>2</td>\n      <td>257</td>\n      <td>143</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>2</td>\n      <td>2</td>\n      <td>malicious</td>\n    </tr>\n  </tbody>\n</table>\n<p>7845 rows × 17 columns</p>\n</div>"},"metadata":{}}]},{"metadata":{"trusted":true},"cell_type":"code","source":"data.columns","execution_count":7,"outputs":[{"output_type":"execute_result","execution_count":7,"data":{"text/plain":"Index(['name', 'tcp_packets', 'dist_port_tcp', 'external_ips', 'vulume_bytes',\n       'udp_packets', 'tcp_urg_packet', 'source_app_packets',\n       'remote_app_packets', 'source_app_bytes', 'remote_app_bytes',\n       'duracion', 'avg_local_pkt_rate', 'avg_remote_pkt_rate',\n       'source_app_packets.1', 'dns_query_times', 'type'],\n      dtype='object')"},"metadata":{}}]},{"metadata":{"trusted":true},"cell_type":"code","source":"data.shape","execution_count":8,"outputs":[{"output_type":"execute_result","execution_count":8,"data":{"text/plain":"(7845, 17)"},"metadata":{}}]},{"metadata":{"trusted":true},"cell_type":"code","source":"##encoding\nfrom sklearn import preprocessing\n\nle = preprocessing.LabelEncoder()\nle.fit(data['type'])\ndata['type'] = le.transform(data['type'])\n#print(data['type'])","execution_count":9,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"data.type.value_counts()","execution_count":10,"outputs":[{"output_type":"execute_result","execution_count":10,"data":{"text/plain":"0    4704\n1    3141\nName: type, dtype: int64"},"metadata":{}}]},{"metadata":{},"cell_type":"markdown","source":"In this case, we have an unbalanced dataset, so another model evaluation will be used.(4704,3141)"},{"metadata":{},"cell_type":"markdown","source":"### Data Cleaning and Processing"},{"metadata":{"trusted":true},"cell_type":"code","source":"data.isna().sum()","execution_count":11,"outputs":[{"output_type":"execute_result","execution_count":11,"data":{"text/plain":"name                       0\ntcp_packets                0\ndist_port_tcp              0\nexternal_ips               0\nvulume_bytes               0\nudp_packets                0\ntcp_urg_packet             0\nsource_app_packets         0\nremote_app_packets         0\nsource_app_bytes           0\nremote_app_bytes           0\nduracion                7845\navg_local_pkt_rate      7845\navg_remote_pkt_rate     7845\nsource_app_packets.1       0\ndns_query_times            0\ntype                       0\ndtype: int64"},"metadata":{}}]},{"metadata":{},"cell_type":"markdown","source":"When we processed each pcap we had some problems getting three features (duration, avg remote package rate, avg local package rate) this why got during the feature processing script, we don't have this issue nowadays. "},{"metadata":{"trusted":true},"cell_type":"code","source":"data = data.drop(['duracion','avg_local_pkt_rate','avg_remote_pkt_rate'], axis=1).copy()","execution_count":12,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"data.describe()","execution_count":13,"outputs":[{"output_type":"execute_result","execution_count":13,"data":{"text/plain":"        tcp_packets  dist_port_tcp  external_ips  vulume_bytes  udp_packets  \\\ncount   7845.000000    7845.000000   7845.000000  7.845000e+03  7845.000000   \nmean     147.578713       7.738177      2.748502  1.654375e+04     0.056724   \nstd      777.920084      51.654222      2.923005  8.225650e+04     1.394046   \nmin        0.000000       0.000000      0.000000  0.000000e+00     0.000000   \n25%        6.000000       0.000000      1.000000  8.880000e+02     0.000000   \n50%       25.000000       0.000000      2.000000  3.509000e+03     0.000000   \n75%       93.000000       0.000000      4.000000  1.218900e+04     0.000000   \nmax    37143.000000    2167.000000     43.000000  4.226790e+06    65.000000   \n\n       tcp_urg_packet  source_app_packets  remote_app_packets  \\\ncount     7845.000000         7845.000000         7845.000000   \nmean         0.000255          152.911918          194.706310   \nstd          0.015966          779.034618         1068.112696   \nmin          0.000000            1.000000            0.000000   \n25%          0.000000            7.000000            7.000000   \n50%          0.000000           30.000000           24.000000   \n75%          0.000000           98.000000           92.000000   \nmax          1.000000        37150.000000        45928.000000   \n\n       source_app_bytes  remote_app_bytes  source_app_packets.1  \\\ncount      7.845000e+03      7.845000e+03           7845.000000   \nmean       2.024967e+05      1.692260e+04            152.911918   \nstd        1.401076e+06      8.238182e+04            779.034618   \nmin        0.000000e+00      6.900000e+01              1.000000   \n25%        9.340000e+02      1.046000e+03              7.000000   \n50%        4.090000e+03      3.803000e+03             30.000000   \n75%        2.624400e+04      1.261000e+04             98.000000   \nmax        6.823516e+07      4.227323e+06          37150.000000   \n\n       dns_query_times         type  \ncount      7845.000000  7845.000000  \nmean          4.898917     0.400382  \nstd          18.900478     0.490007  \nmin           0.000000     0.000000  \n25%           1.000000     0.000000  \n50%           3.000000     0.000000  \n75%           5.000000     1.000000  \nmax         913.000000     1.000000  ","text/html":"<div>\n<style scoped>\n    .dataframe tbody tr th:only-of-type {\n        vertical-align: middle;\n    }\n\n    .dataframe tbody tr th {\n        vertical-align: top;\n    }\n\n    .dataframe thead th {\n        text-align: right;\n    }\n</style>\n<table border=\"1\" class=\"dataframe\">\n  <thead>\n    <tr style=\"text-align: right;\">\n      <th></th>\n      <th>tcp_packets</th>\n      <th>dist_port_tcp</th>\n      <th>external_ips</th>\n      <th>vulume_bytes</th>\n      <th>udp_packets</th>\n      <th>tcp_urg_packet</th>\n      <th>source_app_packets</th>\n      <th>remote_app_packets</th>\n      <th>source_app_bytes</th>\n      <th>remote_app_bytes</th>\n      <th>source_app_packets.1</th>\n      <th>dns_query_times</th>\n      <th>type</th>\n    </tr>\n  </thead>\n  <tbody>\n    <tr>\n      <th>count</th>\n      <td>7845.000000</td>\n      <td>7845.000000</td>\n      <td>7845.000000</td>\n      <td>7.845000e+03</td>\n      <td>7845.000000</td>\n      <td>7845.000000</td>\n      <td>7845.000000</td>\n      <td>7845.000000</td>\n      <td>7.845000e+03</td>\n      <td>7.845000e+03</td>\n      <td>7845.000000</td>\n      <td>7845.000000</td>\n      <td>7845.000000</td>\n    </tr>\n    <tr>\n      <th>mean</th>\n      <td>147.578713</td>\n      <td>7.738177</td>\n      <td>2.748502</td>\n      <td>1.654375e+04</td>\n      <td>0.056724</td>\n      <td>0.000255</td>\n      <td>152.911918</td>\n      <td>194.706310</td>\n      <td>2.024967e+05</td>\n      <td>1.692260e+04</td>\n      <td>152.911918</td>\n      <td>4.898917</td>\n      <td>0.400382</td>\n    </tr>\n    <tr>\n      <th>std</th>\n      <td>777.920084</td>\n      <td>51.654222</td>\n      <td>2.923005</td>\n      <td>8.225650e+04</td>\n      <td>1.394046</td>\n      <td>0.015966</td>\n      <td>779.034618</td>\n      <td>1068.112696</td>\n      <td>1.401076e+06</td>\n      <td>8.238182e+04</td>\n      <td>779.034618</td>\n      <td>18.900478</td>\n      <td>0.490007</td>\n    </tr>\n    <tr>\n      <th>min</th>\n      <td>0.000000</td>\n      <td>0.000000</td>\n      <td>0.000000</td>\n      <td>0.000000e+00</td>\n      <td>0.000000</td>\n      <td>0.000000</td>\n      <td>1.000000</td>\n      <td>0.000000</td>\n      <td>0.000000e+00</td>\n      <td>6.900000e+01</td>\n      <td>1.000000</td>\n      <td>0.000000</td>\n      <td>0.000000</td>\n    </tr>\n    <tr>\n      <th>25%</th>\n      <td>6.000000</td>\n      <td>0.000000</td>\n      <td>1.000000</td>\n      <td>8.880000e+02</td>\n      <td>0.000000</td>\n      <td>0.000000</td>\n      <td>7.000000</td>\n      <td>7.000000</td>\n      <td>9.340000e+02</td>\n      <td>1.046000e+03</td>\n      <td>7.000000</td>\n      <td>1.000000</td>\n      <td>0.000000</td>\n    </tr>\n    <tr>\n      <th>50%</th>\n      <td>25.000000</td>\n      <td>0.000000</td>\n      <td>2.000000</td>\n      <td>3.509000e+03</td>\n      <td>0.000000</td>\n      <td>0.000000</td>\n      <td>30.000000</td>\n      <td>24.000000</td>\n      <td>4.090000e+03</td>\n      <td>3.803000e+03</td>\n      <td>30.000000</td>\n      <td>3.000000</td>\n      <td>0.000000</td>\n    </tr>\n    <tr>\n      <th>75%</th>\n      <td>93.000000</td>\n      <td>0.000000</td>\n      <td>4.000000</td>\n      <td>1.218900e+04</td>\n      <td>0.000000</td>\n      <td>0.000000</td>\n      <td>98.000000</td>\n      <td>92.000000</td>\n      <td>2.624400e+04</td>\n      <td>1.261000e+04</td>\n      <td>98.000000</td>\n      <td>5.000000</td>\n      <td>1.000000</td>\n    </tr>\n    <tr>\n      <th>max</th>\n      <td>37143.000000</td>\n      <td>2167.000000</td>\n      <td>43.000000</td>\n      <td>4.226790e+06</td>\n      <td>65.000000</td>\n      <td>1.000000</td>\n      <td>37150.000000</td>\n      <td>45928.000000</td>\n      <td>6.823516e+07</td>\n      <td>4.227323e+06</td>\n      <td>37150.000000</td>\n      <td>913.000000</td>\n      <td>1.000000</td>\n    </tr>\n  </tbody>\n</table>\n</div>"},"metadata":{}}]},{"metadata":{},"cell_type":"markdown","source":"Now, the idea is to see the outliers in the data"},{"metadata":{"trusted":true},"cell_type":"code","source":"sns.boxplot(data.tcp_urg_packet)","execution_count":14,"outputs":[{"output_type":"execute_result","execution_count":14,"data":{"text/plain":"<matplotlib.axes._subplots.AxesSubplot at 0x7f6290ce60f0>"},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"<Figure size 432x288 with 1 Axes>","image/png":"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\n"},"metadata":{"needs_background":"light"}}]},{"metadata":{"trusted":true},"cell_type":"code","source":"data.loc[data.tcp_urg_packet > 0].shape[0]","execution_count":15,"outputs":[{"output_type":"execute_result","execution_count":15,"data":{"text/plain":"2"},"metadata":{}}]},{"metadata":{},"cell_type":"markdown","source":"That column will be no used for the analysis, only two rows are different to zero, maybe they are interesting for future analysis."},{"metadata":{"trusted":true},"cell_type":"code","source":"#data = data.drop(['duracion','avg_local_pkt_rate','avg_remote_pkt_rate'], axis=1).copy()\nX,y=data.iloc[:,1:17].astype(\"int\"), data.type.astype(\"int\")\nchi_scores = chi2(X,y)\np_values = pd.Series(chi_scores[1],index = X.columns)\np_values.sort_values(ascending = False , inplace = True)\np_values.plot.bar()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"data = data.drop(columns=[\"tcp_urg_packet\"], axis=1).copy()\ndata.shape","execution_count":16,"outputs":[{"output_type":"execute_result","execution_count":16,"data":{"text/plain":"(7845, 13)"},"metadata":{}}]},{"metadata":{"trusted":true},"cell_type":"code","source":"sns.pairplot(data)","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"We have many outliers in some features, I will omit the depth analysis and only get the set of the data without the noise. "},{"metadata":{"trusted":true},"cell_type":"code","source":"data=data[data.tcp_packets<20000].copy()\ndata=data[data.dist_port_tcp<1400].copy()\ndata=data[data.external_ips<35].copy()\ndata=data[data.vulume_bytes<2000000].copy()\ndata=data[data.udp_packets<40].copy()\ndata=data[data.remote_app_packets<15000].copy()","execution_count":17,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"data[data.duplicated()].sum()","execution_count":18,"outputs":[{"output_type":"execute_result","execution_count":18,"data":{"text/plain":"name                    AntiVirusAntiVirusAntiVirusAntiVirusAntiVirusA...\ntcp_packets                                                         15038\ndist_port_tcp                                                        3514\nexternal_ips                                                         1434\nvulume_bytes                                                      2061210\nudp_packets                                                            38\nsource_app_packets                                                  21720\nremote_app_packets                                                  18841\nsource_app_bytes                                                  8615120\nremote_app_bytes                                                  2456160\nsource_app_packets.1                                                21720\ndns_query_times                                                      5095\ntype                                                                 1268\ndtype: object"},"metadata":{}}]},{"metadata":{"trusted":true},"cell_type":"code","source":"data=data.drop('source_app_packets.1',axis=1).copy()","execution_count":19,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"scaler = preprocessing.RobustScaler()\nscaledData = scaler.fit_transform(data.iloc[:,1:11])\nscaledData = pd.DataFrame(scaledData, columns=['tcp_packets','dist_port_tcp','external_ips','vulume_bytes','udp_packets','source_app_packets','remote_app_packets',' source_app_bytes','remote_app_bytes','dns_query_times'])","execution_count":20,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"From [6] we concluded that the best network features are:\n\n+ (R1): TCP packets, it has the number of packets TCP sent and got during communication.\n+ (R2): Different TCP packets, it is the total number of packets different from TCP.\n+ (R3): External IP, represents the number the external addresses (IPs) where the application tried to communicated\n+ (R4): Volume of bytes, it is the number of bytes that was sent from the application to the external sites\n+ (R5) UDP packets, the total number of packets UDP transmitted in a communication.\n+ (R6) Packets of the source application, it is the number of packets that were sent from the application to a remote server.\n+ (R7) Remote application packages, number of packages received from external sources.\n+ (R8) Bytes of the application source, this is the volume (in Bytes) of the communication between the application and server.\n+ (R9) Bytes of the application remote, this is the volume (in Bytes) of the data from the server to the emulator.\n+ (R10) DNS queries, number of DNS queries.\n"},{"metadata":{"trusted":true},"cell_type":"code","source":"from sklearn.feature_selection import SelectKBest \nfrom sklearn.feature_selection import chi2\n\nX_train,y_train=scaledData.iloc[:,0:10].astype(\"int\"), data.type.astype(\"int\")\n# Two features with highest chi-squared statistics are selected \nchi2_features = SelectKBest(chi2, k = 8) \nX_kbest_features = chi2_features.fit_transform(X_train, y_train) \n  \n# Reduced features \nprint('Original feature number:', X_train.shape[1], X_train.columns) \nprint('Reduced feature number:', X_kbest_features.shape[1])\n","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"\nX,y=scaledData.iloc[:,0:10].astype(\"int\"), data.type.astype(\"int\")\nchi_scores = chi2(X,y)\np_values = pd.Series(chi_scores[1],index = X.columns)\np_values.sort_values(ascending = False , inplace = True)\np_values.plot.bar()","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"Since dns_query_times has higher the p-value, it says that this variables is independent of the reponse and can not be considered for model training"},{"metadata":{"trusted":true},"cell_type":"code","source":"#optional\nscaledData.shape\nscaledData = scaledData.drop(columns=[\"dns_query_times\"], axis=1).copy()\nscaledData.shape","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"### Modeling"},{"metadata":{"trusted":true},"cell_type":"code","source":"X_train, X_test, y_train, y_test = train_test_split(scaledData.iloc[:,0:10], data.type.astype(\"str\"), test_size=0.25, random_state=45)\n#X_train, X_test, y_train, y_test = train_test_split(scaledData.iloc[:,0:9], data.type.astype(\"str\"), test_size=0.25, random_state=45)","execution_count":34,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"y1_test=y_test.astype(\"int64\")","execution_count":35,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"#LinearSVM\nfrom sklearn import svm\n\nlin_clf = svm.LinearSVC()\nlin_clf.fit(X_train, y_train)\npred = lin_clf.predict(X_test)\npred=pred.astype(\"int64\")\n## accuracy\ns=scoreset(y1_test,pred)\n#print(s)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"gnb = GaussianNB()\ngnb.fit(X_train, y_train)\npred = gnb.predict(X_test)\n\npred=pred.astype(\"int64\")\n## accuracy\ns=scoreset(y1_test,pred)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"# kneighbors algorithm\n\nfor i in range(3,15,3):\n    \n    neigh = KNeighborsClassifier(n_neighbors=i)\n    neigh.fit(X_train, y_train)\n    pred = neigh.predict(X_test)\n    pred=pred.astype(\"int64\")\n    # accuracy\n    accuracy = accuracy_score(pred, y1_test)\n    print(\"kneighbors {}\".format(i))\n    s=scoreset(y1_test,pred)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"rdF=RandomForestClassifier(n_estimators=250, max_depth=50,random_state=45)\nrdF.fit(X_train,y_train)\npred=rdF.predict(X_test)\ncm=confusion_matrix(y_test, pred)\n\naccuracy = accuracy_score(y_test,pred)\nprint(rdF)\nprint(accuracy)\nprint(classification_report(y_test,pred, labels=None))\nprint(\"cohen kappa score\")\nprint(cohen_kappa_score(y_test, pred))\nprint(cm)\npred=pred.astype(\"int64\")\ns=scoreset(y1_test,pred)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"import xgboost as xgb\n\nxgb_clf = xgb.XGBClassifier()\nxgb_clf = xgb_clf.fit(X_train, y_train)\npred=xgb_clf.predict(X_test)\ncm=confusion_matrix(y_test, pred)\n\naccuracy = accuracy_score(y_test,pred)\nprint(accuracy)\nprint(classification_report(y_test,pred, labels=None))\nprint(\"cohen kappa score\")\nprint(cohen_kappa_score(y_test, pred))\nprint(cm)\npred=pred.astype(\"int64\")\ns=scoreset(y1_test,pred)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"from sklearn.ensemble import AdaBoostClassifier\n# Create adaboost classifer object\nabc = AdaBoostClassifier(n_estimators=50,\n                         learning_rate=1.3)#vary the learning rate from 0 to 1.5\n# Train Adaboost Classifer\nmodel = abc.fit(X_train, y_train)\n\n#Predict the response for test dataset\npred = model.predict(X_test)\ncm=confusion_matrix(y_test, pred)\n\naccuracy = accuracy_score(y_test,pred)\nprint(accuracy)\nprint(classification_report(y_test,pred, labels=None))\nprint(\"cohen kappa score\")\nprint(cohen_kappa_score(y_test, pred))\nprint(cm)\npred=pred.astype(\"int64\")\ns=scoreset(y1_test,pred)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"from sklearn.linear_model import LogisticRegression\n\nclf = LogisticRegression(random_state=0).fit(X_train, y_train)\npred=clf.predict(X_test)\n#cm=confusion_matrix(y1_test, pred)\n\n#accuracy = accuracy_score(y1_test,pred)\n#print(accuracy)\n#print(classification_report(y1_test,pred, labels=None))\npred=pred.astype(\"int64\")\ns=scoreset(y1_test,pred)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"from sklearn import preprocessing\nle = preprocessing.LabelEncoder()\ny_train=le.fit_transform(y_train)\ny_test=le.fit_transform(y_test)\nprint(len(y_test))\nprint(y_test[0:100])","execution_count":36,"outputs":[{"output_type":"stream","text":"1958\n[1 0 0 1 0 1 0 1 1 0 0 0 0 0 1 0 0 1 1 1 0 0 0 1 0 0 0 1 0 1 0 1 0 0 1 1 0\n 1 1 0 1 0 0 0 1 0 0 1 1 1 0 0 0 0 1 0 1 0 1 0 1 0 1 0 1 1 0 0 0 0 0 1 0 0\n 0 0 1 0 0 1 0 1 0 0 1 0 0 0 1 1 0 1 1 1 1 0 0 1 1 0]\n","name":"stdout"}]},{"metadata":{"trusted":true},"cell_type":"code","source":"from keras.models import Sequential\nfrom keras.layers import Dense\nimport matplotlib.pyplot as plt\nimport numpy\n\n# create model\nmodel = Sequential()\nmodel.add(Dense(14, input_dim=10, kernel_initializer='uniform', activation='relu'))\nmodel.add(Dense(8, kernel_initializer='uniform', activation='relu'))\nmodel.add(Dense(8, kernel_initializer='uniform', activation='relu'))\nmodel.add(Dense(8, kernel_initializer='uniform', activation='relu'))\nmodel.add(Dense(1, kernel_initializer='uniform', activation='sigmoid'))\n# Compile model\nmodel.compile(loss='binary_crossentropy', optimizer='adam', metrics=['accuracy'])\nmodel.summary()\n","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"from keras.models import Sequential\nfrom keras.layers import Dense\nimport matplotlib.pyplot as plt\nimport numpy\n\n# create model\nmodel = Sequential()\nmodel.add(Dense(14, input_dim=10, kernel_initializer='uniform', activation='relu'))\nmodel.add(Dense(8, kernel_initializer='uniform', activation='relu'))\nmodel.add(Dense(8, kernel_initializer='uniform', activation='relu'))\nmodel.add(Dense(8, kernel_initializer='uniform', activation='relu'))\nmodel.add(Dense(8, kernel_initializer='uniform', activation='relu'))\nmodel.add(Dense(1, kernel_initializer='uniform', activation='sigmoid'))\n# Compile model\nmodel.compile(loss='binary_crossentropy', optimizer='adam', metrics=['accuracy'])\nmodel.summary()\n","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"#Neural network module\nfrom keras.models import Sequential \nfrom keras.layers import Dense,Activation,Dropout \nfrom keras.layers.normalization import BatchNormalization \nfrom keras.utils import np_utils\n\n# create model\nmodel = Sequential()\nmodel.add(Dense(14, input_dim=10, kernel_initializer='uniform', activation='relu'))\nmodel.add(Dense(12, kernel_initializer='uniform', activation='relu'))\nmodel.add(Dense(8, kernel_initializer='uniform', activation='relu'))\nmodel.add(Dense(1, kernel_initializer='uniform', activation='sigmoid'))\n# Compile model\nmodel.compile(loss='binary_crossentropy', optimizer='adam', metrics=['accuracy'])\nmodel.summary()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"######best one with 87% ACC and good graphs\nfrom keras.models import Sequential\nfrom keras.layers import Dense,Activation,Dropout\nimport matplotlib.pyplot as plt\nimport numpy\nmodel=Sequential()\nmodel.add(Dense(1000,input_dim=10,activation='relu'))\nmodel.add(Dropout(0.2))\nmodel.add(Dense(500,activation='relu'))\nmodel.add(Dropout(0.2))\nmodel.add(Dense(300,activation='relu'))\nmodel.add(Dropout(0.2))\nmodel.add(Dense(1,activation='sigmoid'))\n#model.add(Dense(3,activation='softmax'))\nmodel.compile(loss='binary_crossentropy',optimizer='SGD',metrics=['accuracy'])","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"######best one with 87% ACC and good graphs for 500 epochs and 0.10 split\nfrom keras.models import Sequential\nfrom keras.layers import Dense,Activation,Dropout\nimport matplotlib.pyplot as plt\nimport numpy\nmodel=Sequential()\nmodel.add(Dense(1000,input_dim=10,activation='relu'))\nmodel.add(Dropout(0.2))\nmodel.add(Dense(700,activation='relu'))\nmodel.add(Dropout(0.2))\nmodel.add(Dense(500,activation='relu'))\nmodel.add(Dropout(0.2))\nmodel.add(Dense(300,activation='relu'))\nmodel.add(Dropout(0.2))\nmodel.add(Dense(100,activation='relu'))\nmodel.add(Dropout(0.2))\nmodel.add(Dense(1,activation='sigmoid'))\nmodel.compile(loss='binary_crossentropy',optimizer='SGD',metrics=['accuracy'])\n\n# Fit the model\nprint('training')\nhistory = model.fit(X_train, y_train, validation_split=0.10, epochs=500, batch_size=100, verbose=1)\nprint('over')\n\n##test dataset\ny_pred = model.predict(X_test)\n#y_pred=np.argmax(y_pred,axis=-1)\ny_pred=np.where(y_pred> 0.5, 1, 0)\n#print(y_pred[0:5])\ny_true=y_test\nfrom sklearn.metrics import confusion_matrix,accuracy_score,precision_score,f1_score\n#tp, fp, fn, tn = confusion_matrix(y_true,y_pred)\nacc=accuracy_score(y_true, y_pred)\nprecision=precision_score(y_true, y_pred, average='weighted')\nf1=f1_score(y_true, y_pred, average='weighted')\nprint('Accuracy=',acc)\nprint('Precision=',precision)\nprint('F1Score=',f1)\nprint('tn, fp, fn, tp' )\n\nfrom sklearn.metrics import classification_report\n\n# Generate a classification report\nreport = classification_report(y_test, y_pred, target_names=['0','1'])\nprint(report)\nprint(confusion_matrix(y_true,y_pred))\n\ns=scoreset(y_true,y_pred)\nplot_graph_train_val(history)","execution_count":38,"outputs":[{"output_type":"stream","text":"training\nTrain on 5286 samples, validate on 588 samples\nEpoch 1/500\n5286/5286 [==============================] - 1s 104us/step - loss: 0.7007 - accuracy: 0.5885 - val_loss: 0.7078 - val_accuracy: 0.4643\nEpoch 2/500\n5286/5286 [==============================] - 0s 36us/step - loss: 0.7024 - accuracy: 0.5832 - val_loss: 0.6677 - val_accuracy: 0.5850\nEpoch 3/500\n5286/5286 [==============================] - 0s 35us/step - loss: 0.6646 - accuracy: 0.5948 - val_loss: 0.6659 - val_accuracy: 0.5850\nEpoch 4/500\n5286/5286 [==============================] - 0s 35us/step - loss: 0.6682 - accuracy: 0.5959 - val_loss: 0.6616 - val_accuracy: 0.5867\nEpoch 5/500\n5286/5286 [==============================] - 0s 36us/step - loss: 0.6654 - accuracy: 0.5991 - val_loss: 0.6643 - val_accuracy: 0.5850\nEpoch 6/500\n5286/5286 [==============================] - 0s 35us/step - loss: 0.6623 - accuracy: 0.5972 - val_loss: 0.6609 - val_accuracy: 0.5850\nEpoch 7/500\n5286/5286 [==============================] - 0s 35us/step - loss: 0.6555 - accuracy: 0.6018 - val_loss: 0.6594 - val_accuracy: 0.5867\nEpoch 8/500\n5286/5286 [==============================] - 0s 34us/step - loss: 0.6603 - accuracy: 0.6001 - val_loss: 0.6591 - val_accuracy: 0.5884\nEpoch 9/500\n5286/5286 [==============================] - 0s 34us/step - loss: 0.6560 - accuracy: 0.6039 - val_loss: 0.6534 - val_accuracy: 0.5901\nEpoch 10/500\n5286/5286 [==============================] - 0s 34us/step - loss: 0.6583 - accuracy: 0.6040 - val_loss: 0.6529 - val_accuracy: 0.5901\nEpoch 11/500\n5286/5286 [==============================] - 0s 35us/step - loss: 0.6514 - accuracy: 0.6101 - val_loss: 0.6511 - val_accuracy: 0.5901\nEpoch 12/500\n5286/5286 [==============================] - 0s 35us/step - loss: 0.6552 - accuracy: 0.6124 - val_loss: 0.6519 - val_accuracy: 0.5867\nEpoch 13/500\n5286/5286 [==============================] - 0s 35us/step - loss: 0.6462 - accuracy: 0.6148 - val_loss: 0.6469 - val_accuracy: 0.5901\nEpoch 14/500\n5286/5286 [==============================] - 0s 34us/step - loss: 0.6435 - accuracy: 0.6154 - val_loss: 0.6467 - val_accuracy: 0.5901\nEpoch 15/500\n5286/5286 [==============================] - 0s 37us/step - loss: 0.6385 - accuracy: 0.6182 - val_loss: 0.6452 - val_accuracy: 0.5901\nEpoch 16/500\n5286/5286 [==============================] - 0s 36us/step - loss: 0.6440 - accuracy: 0.6201 - val_loss: 0.6429 - val_accuracy: 0.5884\nEpoch 17/500\n5286/5286 [==============================] - 0s 34us/step - loss: 0.6437 - accuracy: 0.6224 - val_loss: 0.6442 - val_accuracy: 0.6139\nEpoch 18/500\n5286/5286 [==============================] - 0s 34us/step - loss: 0.6448 - accuracy: 0.6341 - val_loss: 0.6407 - val_accuracy: 0.6173\nEpoch 19/500\n5286/5286 [==============================] - 0s 34us/step - loss: 0.6369 - accuracy: 0.6396 - val_loss: 0.6394 - val_accuracy: 0.6276\nEpoch 20/500\n5286/5286 [==============================] - 0s 34us/step - loss: 0.6322 - accuracy: 0.6462 - val_loss: 0.6375 - val_accuracy: 0.6276\nEpoch 21/500\n5286/5286 [==============================] - 0s 36us/step - loss: 0.6337 - accuracy: 0.6563 - val_loss: 0.6350 - val_accuracy: 0.6276\nEpoch 22/500\n5286/5286 [==============================] - 0s 58us/step - loss: 0.6328 - accuracy: 0.6595 - val_loss: 0.6321 - val_accuracy: 0.6650\nEpoch 23/500\n5286/5286 [==============================] - 0s 51us/step - loss: 0.6312 - accuracy: 0.6670 - val_loss: 0.6299 - val_accuracy: 0.6667\nEpoch 24/500\n5286/5286 [==============================] - 0s 50us/step - loss: 0.6308 - accuracy: 0.6824 - val_loss: 0.6267 - val_accuracy: 0.6667\nEpoch 25/500\n5286/5286 [==============================] - 0s 34us/step - loss: 0.6273 - accuracy: 0.6831 - val_loss: 0.6264 - val_accuracy: 0.6667\nEpoch 26/500\n5286/5286 [==============================] - 0s 35us/step - loss: 0.6222 - accuracy: 0.6856 - val_loss: 0.6292 - val_accuracy: 0.6684\nEpoch 27/500\n5286/5286 [==============================] - 0s 34us/step - loss: 0.6159 - accuracy: 0.6945 - val_loss: 0.6238 - val_accuracy: 0.6667\nEpoch 28/500\n5286/5286 [==============================] - 0s 35us/step - loss: 0.6116 - accuracy: 0.6981 - val_loss: 0.6246 - val_accuracy: 0.6769\nEpoch 29/500\n5286/5286 [==============================] - 0s 36us/step - loss: 0.6205 - accuracy: 0.6983 - val_loss: 0.6228 - val_accuracy: 0.6752\nEpoch 30/500\n5286/5286 [==============================] - 0s 35us/step - loss: 0.6092 - accuracy: 0.7020 - val_loss: 0.6172 - val_accuracy: 0.6752\nEpoch 31/500\n5286/5286 [==============================] - 0s 40us/step - loss: 0.6090 - accuracy: 0.7037 - val_loss: 0.6170 - val_accuracy: 0.6752\nEpoch 32/500\n5286/5286 [==============================] - 0s 33us/step - loss: 0.6100 - accuracy: 0.7068 - val_loss: 0.6159 - val_accuracy: 0.6786\nEpoch 33/500\n5286/5286 [==============================] - 0s 34us/step - loss: 0.6017 - accuracy: 0.7094 - val_loss: 0.6166 - val_accuracy: 0.6888\nEpoch 34/500\n5286/5286 [==============================] - 0s 34us/step - loss: 0.5962 - accuracy: 0.7107 - val_loss: 0.6094 - val_accuracy: 0.6854\nEpoch 35/500\n5286/5286 [==============================] - 0s 35us/step - loss: 0.5922 - accuracy: 0.7121 - val_loss: 0.6175 - val_accuracy: 0.6905\nEpoch 36/500\n5286/5286 [==============================] - 0s 35us/step - loss: 0.5904 - accuracy: 0.7132 - val_loss: 0.6186 - val_accuracy: 0.6939\nEpoch 37/500\n5286/5286 [==============================] - 0s 36us/step - loss: 0.5870 - accuracy: 0.7119 - val_loss: 0.6186 - val_accuracy: 0.6888\nEpoch 38/500\n5286/5286 [==============================] - 0s 36us/step - loss: 0.5790 - accuracy: 0.7128 - val_loss: 0.6007 - val_accuracy: 0.6837\nEpoch 39/500\n5286/5286 [==============================] - 0s 38us/step - loss: 0.5897 - accuracy: 0.7166 - val_loss: 0.6034 - val_accuracy: 0.6854\nEpoch 40/500\n5286/5286 [==============================] - 0s 36us/step - loss: 0.5830 - accuracy: 0.7147 - val_loss: 0.6044 - val_accuracy: 0.6905\nEpoch 41/500\n5286/5286 [==============================] - 0s 37us/step - loss: 0.5792 - accuracy: 0.7183 - val_loss: 0.5979 - val_accuracy: 0.6888\nEpoch 42/500\n5286/5286 [==============================] - 0s 35us/step - loss: 0.5785 - accuracy: 0.7157 - val_loss: 0.6008 - val_accuracy: 0.6956\nEpoch 43/500\n5286/5286 [==============================] - 0s 34us/step - loss: 0.5726 - accuracy: 0.7162 - val_loss: 0.5917 - val_accuracy: 0.6922\nEpoch 44/500\n5286/5286 [==============================] - 0s 35us/step - loss: 0.5691 - accuracy: 0.7145 - val_loss: 0.5993 - val_accuracy: 0.7007\nEpoch 45/500\n5286/5286 [==============================] - 0s 35us/step - loss: 0.5705 - accuracy: 0.7189 - val_loss: 0.5997 - val_accuracy: 0.6939\nEpoch 46/500\n5286/5286 [==============================] - 0s 48us/step - loss: 0.5610 - accuracy: 0.7181 - val_loss: 0.5860 - val_accuracy: 0.6939\nEpoch 47/500\n5286/5286 [==============================] - 0s 62us/step - loss: 0.5617 - accuracy: 0.7236 - val_loss: 0.5897 - val_accuracy: 0.7024\nEpoch 48/500\n5286/5286 [==============================] - 0s 62us/step - loss: 0.5559 - accuracy: 0.7193 - val_loss: 0.5900 - val_accuracy: 0.7075\nEpoch 49/500\n5286/5286 [==============================] - 0s 59us/step - loss: 0.5604 - accuracy: 0.7227 - val_loss: 0.5832 - val_accuracy: 0.6939\nEpoch 50/500\n5286/5286 [==============================] - 0s 40us/step - loss: 0.5586 - accuracy: 0.7219 - val_loss: 0.5900 - val_accuracy: 0.7075\nEpoch 51/500\n5286/5286 [==============================] - 0s 35us/step - loss: 0.5514 - accuracy: 0.7253 - val_loss: 0.5831 - val_accuracy: 0.7160\nEpoch 52/500\n5286/5286 [==============================] - 0s 36us/step - loss: 0.5511 - accuracy: 0.7272 - val_loss: 0.5775 - val_accuracy: 0.7143\nEpoch 53/500\n5286/5286 [==============================] - 0s 37us/step - loss: 0.5543 - accuracy: 0.7264 - val_loss: 0.5809 - val_accuracy: 0.7075\nEpoch 54/500\n5286/5286 [==============================] - 0s 36us/step - loss: 0.5443 - accuracy: 0.7266 - val_loss: 0.5726 - val_accuracy: 0.7160\nEpoch 55/500\n5286/5286 [==============================] - 0s 36us/step - loss: 0.5529 - accuracy: 0.7268 - val_loss: 0.5799 - val_accuracy: 0.7143\nEpoch 56/500\n","name":"stdout"},{"output_type":"stream","text":"5286/5286 [==============================] - 0s 36us/step - loss: 0.5443 - accuracy: 0.7261 - val_loss: 0.5754 - val_accuracy: 0.7143\nEpoch 57/500\n5286/5286 [==============================] - 0s 36us/step - loss: 0.5412 - accuracy: 0.7281 - val_loss: 0.5763 - val_accuracy: 0.7126\nEpoch 58/500\n5286/5286 [==============================] - 0s 36us/step - loss: 0.5386 - accuracy: 0.7281 - val_loss: 0.5789 - val_accuracy: 0.7109\nEpoch 59/500\n5286/5286 [==============================] - 0s 36us/step - loss: 0.5378 - accuracy: 0.7266 - val_loss: 0.5682 - val_accuracy: 0.7194\nEpoch 60/500\n5286/5286 [==============================] - 0s 36us/step - loss: 0.5367 - accuracy: 0.7304 - val_loss: 0.5720 - val_accuracy: 0.7126\nEpoch 61/500\n5286/5286 [==============================] - 0s 34us/step - loss: 0.5325 - accuracy: 0.7306 - val_loss: 0.5735 - val_accuracy: 0.7109\nEpoch 62/500\n5286/5286 [==============================] - 0s 35us/step - loss: 0.5327 - accuracy: 0.7285 - val_loss: 0.5698 - val_accuracy: 0.7109\nEpoch 63/500\n5286/5286 [==============================] - 0s 34us/step - loss: 0.5296 - accuracy: 0.7323 - val_loss: 0.5671 - val_accuracy: 0.7092\nEpoch 64/500\n5286/5286 [==============================] - 0s 36us/step - loss: 0.5334 - accuracy: 0.7348 - val_loss: 0.5700 - val_accuracy: 0.7160\nEpoch 65/500\n5286/5286 [==============================] - 0s 37us/step - loss: 0.5270 - accuracy: 0.7393 - val_loss: 0.5608 - val_accuracy: 0.7143\nEpoch 66/500\n5286/5286 [==============================] - 0s 36us/step - loss: 0.5347 - accuracy: 0.7378 - val_loss: 0.5637 - val_accuracy: 0.7177\nEpoch 67/500\n5286/5286 [==============================] - 0s 36us/step - loss: 0.5235 - accuracy: 0.7380 - val_loss: 0.5593 - val_accuracy: 0.7143\nEpoch 68/500\n5286/5286 [==============================] - 0s 36us/step - loss: 0.5288 - accuracy: 0.7369 - val_loss: 0.5534 - val_accuracy: 0.7177\nEpoch 69/500\n5286/5286 [==============================] - 0s 36us/step - loss: 0.5272 - accuracy: 0.7372 - val_loss: 0.5518 - val_accuracy: 0.7211\nEpoch 70/500\n5286/5286 [==============================] - 0s 38us/step - loss: 0.5238 - accuracy: 0.7399 - val_loss: 0.5552 - val_accuracy: 0.7143\nEpoch 71/500\n5286/5286 [==============================] - 0s 36us/step - loss: 0.5204 - accuracy: 0.7386 - val_loss: 0.5542 - val_accuracy: 0.7279\nEpoch 72/500\n5286/5286 [==============================] - 0s 36us/step - loss: 0.5231 - accuracy: 0.7465 - val_loss: 0.5523 - val_accuracy: 0.7262\nEpoch 73/500\n5286/5286 [==============================] - 0s 35us/step - loss: 0.5175 - accuracy: 0.7456 - val_loss: 0.5510 - val_accuracy: 0.7228\nEpoch 74/500\n5286/5286 [==============================] - 0s 38us/step - loss: 0.5170 - accuracy: 0.7440 - val_loss: 0.5534 - val_accuracy: 0.7194\nEpoch 75/500\n5286/5286 [==============================] - 0s 58us/step - loss: 0.5207 - accuracy: 0.7353 - val_loss: 0.5461 - val_accuracy: 0.7279\nEpoch 76/500\n5286/5286 [==============================] - 0s 46us/step - loss: 0.5134 - accuracy: 0.7408 - val_loss: 0.5420 - val_accuracy: 0.7483\nEpoch 77/500\n5286/5286 [==============================] - 0s 44us/step - loss: 0.5145 - accuracy: 0.7406 - val_loss: 0.5572 - val_accuracy: 0.7551\nEpoch 78/500\n5286/5286 [==============================] - 0s 46us/step - loss: 0.5143 - accuracy: 0.7408 - val_loss: 0.5359 - val_accuracy: 0.7466\nEpoch 79/500\n5286/5286 [==============================] - 0s 38us/step - loss: 0.5115 - accuracy: 0.7431 - val_loss: 0.5458 - val_accuracy: 0.7381\nEpoch 80/500\n5286/5286 [==============================] - 0s 35us/step - loss: 0.5098 - accuracy: 0.7431 - val_loss: 0.5366 - val_accuracy: 0.7534\nEpoch 81/500\n5286/5286 [==============================] - 0s 35us/step - loss: 0.5109 - accuracy: 0.7471 - val_loss: 0.5348 - val_accuracy: 0.7347\nEpoch 82/500\n5286/5286 [==============================] - 0s 34us/step - loss: 0.5087 - accuracy: 0.7499 - val_loss: 0.5333 - val_accuracy: 0.7568\nEpoch 83/500\n5286/5286 [==============================] - 0s 36us/step - loss: 0.5055 - accuracy: 0.7446 - val_loss: 0.5399 - val_accuracy: 0.7517\nEpoch 84/500\n5286/5286 [==============================] - 0s 37us/step - loss: 0.5067 - accuracy: 0.7465 - val_loss: 0.5289 - val_accuracy: 0.7466\nEpoch 85/500\n5286/5286 [==============================] - 0s 37us/step - loss: 0.5023 - accuracy: 0.7480 - val_loss: 0.5251 - val_accuracy: 0.7534\nEpoch 86/500\n5286/5286 [==============================] - 0s 36us/step - loss: 0.4988 - accuracy: 0.7529 - val_loss: 0.5317 - val_accuracy: 0.7483\nEpoch 87/500\n5286/5286 [==============================] - 0s 35us/step - loss: 0.5043 - accuracy: 0.7533 - val_loss: 0.5289 - val_accuracy: 0.7517\nEpoch 88/500\n5286/5286 [==============================] - 0s 35us/step - loss: 0.5009 - accuracy: 0.7507 - val_loss: 0.5280 - val_accuracy: 0.7602\nEpoch 89/500\n5286/5286 [==============================] - 0s 35us/step - loss: 0.4996 - accuracy: 0.7503 - val_loss: 0.5277 - val_accuracy: 0.7602\nEpoch 90/500\n5286/5286 [==============================] - 0s 34us/step - loss: 0.4966 - accuracy: 0.7501 - val_loss: 0.5229 - val_accuracy: 0.7551\nEpoch 91/500\n5286/5286 [==============================] - 0s 34us/step - loss: 0.4950 - accuracy: 0.7527 - val_loss: 0.5217 - val_accuracy: 0.7585\nEpoch 92/500\n5286/5286 [==============================] - 0s 34us/step - loss: 0.4944 - accuracy: 0.7561 - val_loss: 0.5161 - val_accuracy: 0.7687\nEpoch 93/500\n5286/5286 [==============================] - 0s 34us/step - loss: 0.4922 - accuracy: 0.7597 - val_loss: 0.5145 - val_accuracy: 0.7636\nEpoch 94/500\n5286/5286 [==============================] - 0s 39us/step - loss: 0.4921 - accuracy: 0.7586 - val_loss: 0.5130 - val_accuracy: 0.7619\nEpoch 95/500\n5286/5286 [==============================] - 0s 36us/step - loss: 0.5003 - accuracy: 0.7478 - val_loss: 0.5201 - val_accuracy: 0.7585\nEpoch 96/500\n5286/5286 [==============================] - 0s 36us/step - loss: 0.4917 - accuracy: 0.7592 - val_loss: 0.5192 - val_accuracy: 0.7568\nEpoch 97/500\n5286/5286 [==============================] - 0s 36us/step - loss: 0.4917 - accuracy: 0.7592 - val_loss: 0.5115 - val_accuracy: 0.7619\nEpoch 98/500\n5286/5286 [==============================] - 0s 36us/step - loss: 0.4903 - accuracy: 0.7662 - val_loss: 0.5199 - val_accuracy: 0.7619\nEpoch 99/500\n5286/5286 [==============================] - 0s 36us/step - loss: 0.4927 - accuracy: 0.7662 - val_loss: 0.5198 - val_accuracy: 0.7636\nEpoch 100/500\n5286/5286 [==============================] - 0s 36us/step - loss: 0.4857 - accuracy: 0.7586 - val_loss: 0.5100 - val_accuracy: 0.7585\nEpoch 101/500\n5286/5286 [==============================] - 0s 36us/step - loss: 0.4871 - accuracy: 0.7575 - val_loss: 0.5082 - val_accuracy: 0.7551\nEpoch 102/500\n5286/5286 [==============================] - 0s 35us/step - loss: 0.4920 - accuracy: 0.7597 - val_loss: 0.5064 - val_accuracy: 0.7568\nEpoch 103/500\n5286/5286 [==============================] - 0s 36us/step - loss: 0.4920 - accuracy: 0.7563 - val_loss: 0.5056 - val_accuracy: 0.7653\nEpoch 104/500\n5286/5286 [==============================] - 0s 36us/step - loss: 0.4889 - accuracy: 0.7624 - val_loss: 0.5012 - val_accuracy: 0.7585\nEpoch 105/500\n5286/5286 [==============================] - 0s 36us/step - loss: 0.4814 - accuracy: 0.7658 - val_loss: 0.5093 - val_accuracy: 0.7551\nEpoch 106/500\n5286/5286 [==============================] - 0s 36us/step - loss: 0.4825 - accuracy: 0.7637 - val_loss: 0.5073 - val_accuracy: 0.7602\nEpoch 107/500\n5286/5286 [==============================] - 0s 36us/step - loss: 0.4804 - accuracy: 0.7620 - val_loss: 0.5014 - val_accuracy: 0.7636\nEpoch 108/500\n5286/5286 [==============================] - 0s 36us/step - loss: 0.4797 - accuracy: 0.7686 - val_loss: 0.5038 - val_accuracy: 0.7636\nEpoch 109/500\n5286/5286 [==============================] - 0s 36us/step - loss: 0.4744 - accuracy: 0.7635 - val_loss: 0.4949 - val_accuracy: 0.7653\nEpoch 110/500\n5286/5286 [==============================] - 0s 37us/step - loss: 0.4765 - accuracy: 0.7654 - val_loss: 0.4986 - val_accuracy: 0.7687\nEpoch 111/500\n5286/5286 [==============================] - 0s 35us/step - loss: 0.4835 - accuracy: 0.7673 - val_loss: 0.4945 - val_accuracy: 0.7704\n","name":"stdout"},{"output_type":"stream","text":"Epoch 112/500\n5286/5286 [==============================] - 0s 39us/step - loss: 0.4715 - accuracy: 0.7707 - val_loss: 0.5087 - val_accuracy: 0.7619\nEpoch 113/500\n5286/5286 [==============================] - 0s 36us/step - loss: 0.4748 - accuracy: 0.7719 - val_loss: 0.5002 - val_accuracy: 0.7585\nEpoch 114/500\n5286/5286 [==============================] - 0s 36us/step - loss: 0.4751 - accuracy: 0.7664 - val_loss: 0.4980 - val_accuracy: 0.7602\nEpoch 115/500\n5286/5286 [==============================] - 0s 36us/step - loss: 0.4734 - accuracy: 0.7698 - val_loss: 0.5026 - val_accuracy: 0.7602\nEpoch 116/500\n5286/5286 [==============================] - 0s 36us/step - loss: 0.4711 - accuracy: 0.7705 - val_loss: 0.4864 - val_accuracy: 0.7721\nEpoch 117/500\n5286/5286 [==============================] - 0s 35us/step - loss: 0.4727 - accuracy: 0.7787 - val_loss: 0.5027 - val_accuracy: 0.7670\nEpoch 118/500\n5286/5286 [==============================] - 0s 36us/step - loss: 0.4648 - accuracy: 0.7736 - val_loss: 0.4841 - val_accuracy: 0.7721\nEpoch 119/500\n5286/5286 [==============================] - 0s 37us/step - loss: 0.4722 - accuracy: 0.7719 - val_loss: 0.4903 - val_accuracy: 0.7636\nEpoch 120/500\n5286/5286 [==============================] - 0s 37us/step - loss: 0.4610 - accuracy: 0.7713 - val_loss: 0.4883 - val_accuracy: 0.7721\nEpoch 121/500\n5286/5286 [==============================] - 0s 36us/step - loss: 0.4700 - accuracy: 0.7768 - val_loss: 0.4855 - val_accuracy: 0.7755\nEpoch 122/500\n5286/5286 [==============================] - 0s 36us/step - loss: 0.4816 - accuracy: 0.7709 - val_loss: 0.4907 - val_accuracy: 0.7687\nEpoch 123/500\n5286/5286 [==============================] - 0s 36us/step - loss: 0.4669 - accuracy: 0.7747 - val_loss: 0.4819 - val_accuracy: 0.7721\nEpoch 124/500\n5286/5286 [==============================] - 0s 36us/step - loss: 0.4636 - accuracy: 0.7730 - val_loss: 0.4806 - val_accuracy: 0.7738\nEpoch 125/500\n5286/5286 [==============================] - 0s 37us/step - loss: 0.4614 - accuracy: 0.7764 - val_loss: 0.4788 - val_accuracy: 0.7738\nEpoch 126/500\n5286/5286 [==============================] - 0s 37us/step - loss: 0.4616 - accuracy: 0.7771 - val_loss: 0.4849 - val_accuracy: 0.7738\nEpoch 127/500\n5286/5286 [==============================] - 0s 36us/step - loss: 0.4682 - accuracy: 0.7764 - val_loss: 0.4844 - val_accuracy: 0.7653\nEpoch 128/500\n5286/5286 [==============================] - 0s 37us/step - loss: 0.4627 - accuracy: 0.7753 - val_loss: 0.4778 - val_accuracy: 0.7721\nEpoch 129/500\n5286/5286 [==============================] - 0s 37us/step - loss: 0.4603 - accuracy: 0.7794 - val_loss: 0.4768 - val_accuracy: 0.7738\nEpoch 130/500\n5286/5286 [==============================] - 0s 35us/step - loss: 0.4582 - accuracy: 0.7785 - val_loss: 0.4754 - val_accuracy: 0.7738\nEpoch 131/500\n5286/5286 [==============================] - 0s 61us/step - loss: 0.4623 - accuracy: 0.7720 - val_loss: 0.4784 - val_accuracy: 0.7704\nEpoch 132/500\n5286/5286 [==============================] - 0s 48us/step - loss: 0.4580 - accuracy: 0.7770 - val_loss: 0.4744 - val_accuracy: 0.7738\nEpoch 133/500\n5286/5286 [==============================] - 0s 51us/step - loss: 0.4550 - accuracy: 0.7817 - val_loss: 0.4828 - val_accuracy: 0.7653\nEpoch 134/500\n5286/5286 [==============================] - 0s 36us/step - loss: 0.4627 - accuracy: 0.7775 - val_loss: 0.4824 - val_accuracy: 0.7653\nEpoch 135/500\n5286/5286 [==============================] - 0s 36us/step - loss: 0.4561 - accuracy: 0.7796 - val_loss: 0.4767 - val_accuracy: 0.7772\nEpoch 136/500\n5286/5286 [==============================] - 0s 36us/step - loss: 0.4532 - accuracy: 0.7836 - val_loss: 0.4723 - val_accuracy: 0.7755\nEpoch 137/500\n5286/5286 [==============================] - 0s 36us/step - loss: 0.4509 - accuracy: 0.7838 - val_loss: 0.4750 - val_accuracy: 0.7755\nEpoch 138/500\n5286/5286 [==============================] - 0s 37us/step - loss: 0.4540 - accuracy: 0.7874 - val_loss: 0.4706 - val_accuracy: 0.7704\nEpoch 139/500\n5286/5286 [==============================] - 0s 37us/step - loss: 0.4548 - accuracy: 0.7788 - val_loss: 0.4723 - val_accuracy: 0.7704\nEpoch 140/500\n5286/5286 [==============================] - 0s 37us/step - loss: 0.4516 - accuracy: 0.7811 - val_loss: 0.4709 - val_accuracy: 0.7755\nEpoch 141/500\n5286/5286 [==============================] - 0s 36us/step - loss: 0.4494 - accuracy: 0.7838 - val_loss: 0.4650 - val_accuracy: 0.7687\nEpoch 142/500\n5286/5286 [==============================] - 0s 35us/step - loss: 0.4536 - accuracy: 0.7838 - val_loss: 0.4696 - val_accuracy: 0.7806\nEpoch 143/500\n5286/5286 [==============================] - 0s 36us/step - loss: 0.4513 - accuracy: 0.7843 - val_loss: 0.4659 - val_accuracy: 0.7823\nEpoch 144/500\n5286/5286 [==============================] - 0s 37us/step - loss: 0.4452 - accuracy: 0.7862 - val_loss: 0.4613 - val_accuracy: 0.7806\nEpoch 145/500\n5286/5286 [==============================] - 0s 36us/step - loss: 0.4449 - accuracy: 0.7841 - val_loss: 0.4789 - val_accuracy: 0.7687\nEpoch 146/500\n5286/5286 [==============================] - 0s 36us/step - loss: 0.4428 - accuracy: 0.7889 - val_loss: 0.4679 - val_accuracy: 0.7755\nEpoch 147/500\n5286/5286 [==============================] - 0s 36us/step - loss: 0.4401 - accuracy: 0.7883 - val_loss: 0.4631 - val_accuracy: 0.7806\nEpoch 148/500\n5286/5286 [==============================] - 0s 40us/step - loss: 0.4453 - accuracy: 0.7841 - val_loss: 0.4580 - val_accuracy: 0.7823\nEpoch 149/500\n5286/5286 [==============================] - 0s 37us/step - loss: 0.4491 - accuracy: 0.7877 - val_loss: 0.4608 - val_accuracy: 0.7704\nEpoch 150/500\n5286/5286 [==============================] - 0s 36us/step - loss: 0.4451 - accuracy: 0.7917 - val_loss: 0.4572 - val_accuracy: 0.7772\nEpoch 151/500\n5286/5286 [==============================] - 0s 36us/step - loss: 0.4458 - accuracy: 0.7838 - val_loss: 0.4578 - val_accuracy: 0.7772\nEpoch 152/500\n5286/5286 [==============================] - 0s 36us/step - loss: 0.4430 - accuracy: 0.7858 - val_loss: 0.4680 - val_accuracy: 0.7670\nEpoch 153/500\n5286/5286 [==============================] - 0s 36us/step - loss: 0.4370 - accuracy: 0.7900 - val_loss: 0.4609 - val_accuracy: 0.7772\nEpoch 154/500\n5286/5286 [==============================] - 0s 37us/step - loss: 0.4429 - accuracy: 0.7870 - val_loss: 0.4710 - val_accuracy: 0.7789\nEpoch 155/500\n5286/5286 [==============================] - 0s 37us/step - loss: 0.4489 - accuracy: 0.7868 - val_loss: 0.4536 - val_accuracy: 0.7823\nEpoch 156/500\n5286/5286 [==============================] - 0s 37us/step - loss: 0.4412 - accuracy: 0.7847 - val_loss: 0.4519 - val_accuracy: 0.7755\nEpoch 157/500\n5286/5286 [==============================] - 0s 37us/step - loss: 0.4412 - accuracy: 0.7874 - val_loss: 0.4583 - val_accuracy: 0.7738\nEpoch 158/500\n5286/5286 [==============================] - 0s 36us/step - loss: 0.4405 - accuracy: 0.7906 - val_loss: 0.4653 - val_accuracy: 0.7789\nEpoch 159/500\n5286/5286 [==============================] - 0s 37us/step - loss: 0.4407 - accuracy: 0.7898 - val_loss: 0.4557 - val_accuracy: 0.7772\nEpoch 160/500\n5286/5286 [==============================] - 0s 36us/step - loss: 0.4376 - accuracy: 0.7961 - val_loss: 0.4547 - val_accuracy: 0.7806\nEpoch 161/500\n5286/5286 [==============================] - 0s 37us/step - loss: 0.4333 - accuracy: 0.7915 - val_loss: 0.4532 - val_accuracy: 0.7789\nEpoch 162/500\n5286/5286 [==============================] - 0s 39us/step - loss: 0.4302 - accuracy: 0.7932 - val_loss: 0.4530 - val_accuracy: 0.7721\nEpoch 163/500\n5286/5286 [==============================] - 0s 36us/step - loss: 0.4413 - accuracy: 0.7902 - val_loss: 0.4611 - val_accuracy: 0.7789\nEpoch 164/500\n5286/5286 [==============================] - 0s 38us/step - loss: 0.4362 - accuracy: 0.7915 - val_loss: 0.4505 - val_accuracy: 0.7738\nEpoch 165/500\n5286/5286 [==============================] - 0s 36us/step - loss: 0.4437 - accuracy: 0.7968 - val_loss: 0.4611 - val_accuracy: 0.7755\nEpoch 166/500\n5286/5286 [==============================] - 0s 37us/step - loss: 0.4370 - accuracy: 0.7906 - val_loss: 0.4507 - val_accuracy: 0.7772\nEpoch 167/500\n","name":"stdout"},{"output_type":"stream","text":"5286/5286 [==============================] - 0s 38us/step - loss: 0.4406 - accuracy: 0.7932 - val_loss: 0.4500 - val_accuracy: 0.7857\nEpoch 168/500\n5286/5286 [==============================] - 0s 36us/step - loss: 0.4356 - accuracy: 0.7940 - val_loss: 0.4506 - val_accuracy: 0.7755\nEpoch 169/500\n5286/5286 [==============================] - 0s 37us/step - loss: 0.4358 - accuracy: 0.7927 - val_loss: 0.4432 - val_accuracy: 0.7823\nEpoch 170/500\n5286/5286 [==============================] - 0s 37us/step - loss: 0.4320 - accuracy: 0.7923 - val_loss: 0.4509 - val_accuracy: 0.7840\nEpoch 171/500\n5286/5286 [==============================] - 0s 36us/step - loss: 0.4331 - accuracy: 0.7925 - val_loss: 0.4452 - val_accuracy: 0.7857\nEpoch 172/500\n5286/5286 [==============================] - 0s 36us/step - loss: 0.4350 - accuracy: 0.7944 - val_loss: 0.4514 - val_accuracy: 0.7772\nEpoch 173/500\n5286/5286 [==============================] - 0s 36us/step - loss: 0.4314 - accuracy: 0.7953 - val_loss: 0.4461 - val_accuracy: 0.7738\nEpoch 174/500\n5286/5286 [==============================] - 0s 37us/step - loss: 0.4289 - accuracy: 0.7978 - val_loss: 0.4474 - val_accuracy: 0.7857\nEpoch 175/500\n5286/5286 [==============================] - 0s 36us/step - loss: 0.4306 - accuracy: 0.7959 - val_loss: 0.4446 - val_accuracy: 0.7942\nEpoch 176/500\n5286/5286 [==============================] - 0s 36us/step - loss: 0.4312 - accuracy: 0.8000 - val_loss: 0.4493 - val_accuracy: 0.7789\nEpoch 177/500\n5286/5286 [==============================] - 0s 36us/step - loss: 0.4347 - accuracy: 0.7917 - val_loss: 0.4437 - val_accuracy: 0.7789\nEpoch 178/500\n5286/5286 [==============================] - 0s 36us/step - loss: 0.4295 - accuracy: 0.7966 - val_loss: 0.4433 - val_accuracy: 0.7942\nEpoch 179/500\n5286/5286 [==============================] - 0s 36us/step - loss: 0.4339 - accuracy: 0.7993 - val_loss: 0.4423 - val_accuracy: 0.7891\nEpoch 180/500\n5286/5286 [==============================] - 0s 42us/step - loss: 0.4287 - accuracy: 0.7974 - val_loss: 0.4418 - val_accuracy: 0.8044\nEpoch 181/500\n5286/5286 [==============================] - 0s 35us/step - loss: 0.4296 - accuracy: 0.7949 - val_loss: 0.4403 - val_accuracy: 0.7840\nEpoch 182/500\n5286/5286 [==============================] - 0s 37us/step - loss: 0.4238 - accuracy: 0.8017 - val_loss: 0.4380 - val_accuracy: 0.7908\nEpoch 183/500\n5286/5286 [==============================] - 0s 36us/step - loss: 0.4308 - accuracy: 0.7955 - val_loss: 0.4514 - val_accuracy: 0.7806\nEpoch 184/500\n5286/5286 [==============================] - 0s 36us/step - loss: 0.4253 - accuracy: 0.7989 - val_loss: 0.4397 - val_accuracy: 0.7891\nEpoch 185/500\n5286/5286 [==============================] - 0s 54us/step - loss: 0.4234 - accuracy: 0.8008 - val_loss: 0.4455 - val_accuracy: 0.7840\nEpoch 186/500\n5286/5286 [==============================] - 0s 56us/step - loss: 0.4276 - accuracy: 0.7987 - val_loss: 0.4369 - val_accuracy: 0.7823\nEpoch 187/500\n5286/5286 [==============================] - 0s 50us/step - loss: 0.4221 - accuracy: 0.8033 - val_loss: 0.4407 - val_accuracy: 0.8146\nEpoch 188/500\n5286/5286 [==============================] - 0s 36us/step - loss: 0.4217 - accuracy: 0.8025 - val_loss: 0.4399 - val_accuracy: 0.7891\nEpoch 189/500\n5286/5286 [==============================] - 0s 36us/step - loss: 0.4240 - accuracy: 0.8006 - val_loss: 0.4393 - val_accuracy: 0.7857\nEpoch 190/500\n5286/5286 [==============================] - 0s 39us/step - loss: 0.4247 - accuracy: 0.7961 - val_loss: 0.4359 - val_accuracy: 0.7874\nEpoch 191/500\n5286/5286 [==============================] - 0s 34us/step - loss: 0.4211 - accuracy: 0.8048 - val_loss: 0.4370 - val_accuracy: 0.7976\nEpoch 192/500\n5286/5286 [==============================] - 0s 35us/step - loss: 0.4242 - accuracy: 0.7993 - val_loss: 0.4339 - val_accuracy: 0.8010\nEpoch 193/500\n5286/5286 [==============================] - 0s 35us/step - loss: 0.4184 - accuracy: 0.8029 - val_loss: 0.4326 - val_accuracy: 0.8112\nEpoch 194/500\n5286/5286 [==============================] - 0s 39us/step - loss: 0.4192 - accuracy: 0.8025 - val_loss: 0.4348 - val_accuracy: 0.8146\nEpoch 195/500\n5286/5286 [==============================] - 0s 37us/step - loss: 0.4177 - accuracy: 0.8076 - val_loss: 0.4355 - val_accuracy: 0.8112\nEpoch 196/500\n5286/5286 [==============================] - 0s 35us/step - loss: 0.4246 - accuracy: 0.8012 - val_loss: 0.4311 - val_accuracy: 0.8163\nEpoch 197/500\n5286/5286 [==============================] - 0s 34us/step - loss: 0.4183 - accuracy: 0.8023 - val_loss: 0.4295 - val_accuracy: 0.8214\nEpoch 198/500\n5286/5286 [==============================] - 0s 34us/step - loss: 0.4203 - accuracy: 0.8103 - val_loss: 0.4336 - val_accuracy: 0.8129\nEpoch 199/500\n5286/5286 [==============================] - 0s 34us/step - loss: 0.4194 - accuracy: 0.8072 - val_loss: 0.4306 - val_accuracy: 0.8112\nEpoch 200/500\n5286/5286 [==============================] - 0s 34us/step - loss: 0.4163 - accuracy: 0.8084 - val_loss: 0.4316 - val_accuracy: 0.8129\nEpoch 201/500\n5286/5286 [==============================] - 0s 35us/step - loss: 0.4210 - accuracy: 0.8063 - val_loss: 0.4315 - val_accuracy: 0.8180\nEpoch 202/500\n5286/5286 [==============================] - 0s 35us/step - loss: 0.4144 - accuracy: 0.8065 - val_loss: 0.4343 - val_accuracy: 0.8197\nEpoch 203/500\n5286/5286 [==============================] - 0s 62us/step - loss: 0.4081 - accuracy: 0.8133 - val_loss: 0.4327 - val_accuracy: 0.8010\nEpoch 204/500\n5286/5286 [==============================] - 0s 63us/step - loss: 0.4194 - accuracy: 0.8036 - val_loss: 0.4312 - val_accuracy: 0.8163\nEpoch 205/500\n5286/5286 [==============================] - 0s 59us/step - loss: 0.4184 - accuracy: 0.8040 - val_loss: 0.4284 - val_accuracy: 0.8214\nEpoch 206/500\n5286/5286 [==============================] - 0s 49us/step - loss: 0.4051 - accuracy: 0.8137 - val_loss: 0.4367 - val_accuracy: 0.8112\nEpoch 207/500\n5286/5286 [==============================] - 0s 36us/step - loss: 0.4195 - accuracy: 0.8067 - val_loss: 0.4332 - val_accuracy: 0.8265\nEpoch 208/500\n5286/5286 [==============================] - 0s 37us/step - loss: 0.4139 - accuracy: 0.8095 - val_loss: 0.4227 - val_accuracy: 0.8197\nEpoch 209/500\n5286/5286 [==============================] - 0s 34us/step - loss: 0.4154 - accuracy: 0.8053 - val_loss: 0.4282 - val_accuracy: 0.8146\nEpoch 210/500\n5286/5286 [==============================] - 0s 34us/step - loss: 0.4135 - accuracy: 0.8078 - val_loss: 0.4216 - val_accuracy: 0.8231\nEpoch 211/500\n5286/5286 [==============================] - 0s 35us/step - loss: 0.4115 - accuracy: 0.8125 - val_loss: 0.4259 - val_accuracy: 0.8214\nEpoch 212/500\n5286/5286 [==============================] - 0s 35us/step - loss: 0.4063 - accuracy: 0.8163 - val_loss: 0.4293 - val_accuracy: 0.8095\nEpoch 213/500\n5286/5286 [==============================] - 0s 36us/step - loss: 0.4139 - accuracy: 0.8104 - val_loss: 0.4303 - val_accuracy: 0.8214\nEpoch 214/500\n5286/5286 [==============================] - 0s 36us/step - loss: 0.4102 - accuracy: 0.8142 - val_loss: 0.4324 - val_accuracy: 0.8248\nEpoch 215/500\n5286/5286 [==============================] - 0s 35us/step - loss: 0.4111 - accuracy: 0.8110 - val_loss: 0.4282 - val_accuracy: 0.8129\nEpoch 216/500\n5286/5286 [==============================] - 0s 34us/step - loss: 0.4132 - accuracy: 0.8121 - val_loss: 0.4286 - val_accuracy: 0.8180\nEpoch 217/500\n5286/5286 [==============================] - 0s 34us/step - loss: 0.4120 - accuracy: 0.8150 - val_loss: 0.4269 - val_accuracy: 0.8248\nEpoch 218/500\n5286/5286 [==============================] - 0s 35us/step - loss: 0.4105 - accuracy: 0.8148 - val_loss: 0.4238 - val_accuracy: 0.8078\nEpoch 219/500\n5286/5286 [==============================] - 0s 35us/step - loss: 0.4037 - accuracy: 0.8161 - val_loss: 0.4209 - val_accuracy: 0.8163\nEpoch 220/500\n5286/5286 [==============================] - 0s 34us/step - loss: 0.4077 - accuracy: 0.8169 - val_loss: 0.4268 - val_accuracy: 0.8112\nEpoch 221/500\n5286/5286 [==============================] - 0s 34us/step - loss: 0.4082 - accuracy: 0.8101 - val_loss: 0.4196 - val_accuracy: 0.8231\nEpoch 222/500\n","name":"stdout"},{"output_type":"stream","text":"5286/5286 [==============================] - 0s 35us/step - loss: 0.4064 - accuracy: 0.8156 - val_loss: 0.4276 - val_accuracy: 0.8129\nEpoch 223/500\n5286/5286 [==============================] - 0s 34us/step - loss: 0.4062 - accuracy: 0.8108 - val_loss: 0.4268 - val_accuracy: 0.8214\nEpoch 224/500\n5286/5286 [==============================] - 0s 35us/step - loss: 0.4114 - accuracy: 0.8129 - val_loss: 0.4163 - val_accuracy: 0.8163\nEpoch 225/500\n5286/5286 [==============================] - 0s 34us/step - loss: 0.4005 - accuracy: 0.8176 - val_loss: 0.4206 - val_accuracy: 0.8163\nEpoch 226/500\n5286/5286 [==============================] - 0s 34us/step - loss: 0.4122 - accuracy: 0.8163 - val_loss: 0.4268 - val_accuracy: 0.8010\nEpoch 227/500\n5286/5286 [==============================] - 0s 34us/step - loss: 0.4008 - accuracy: 0.8212 - val_loss: 0.4212 - val_accuracy: 0.8163\nEpoch 228/500\n5286/5286 [==============================] - 0s 40us/step - loss: 0.4063 - accuracy: 0.8140 - val_loss: 0.4359 - val_accuracy: 0.8197\nEpoch 229/500\n5286/5286 [==============================] - 0s 37us/step - loss: 0.4061 - accuracy: 0.8144 - val_loss: 0.4303 - val_accuracy: 0.8044\nEpoch 230/500\n5286/5286 [==============================] - 0s 35us/step - loss: 0.4054 - accuracy: 0.8197 - val_loss: 0.4248 - val_accuracy: 0.8316\nEpoch 231/500\n5286/5286 [==============================] - 0s 34us/step - loss: 0.4002 - accuracy: 0.8224 - val_loss: 0.4169 - val_accuracy: 0.8180\nEpoch 232/500\n5286/5286 [==============================] - 0s 34us/step - loss: 0.4063 - accuracy: 0.8182 - val_loss: 0.4261 - val_accuracy: 0.8112\nEpoch 233/500\n5286/5286 [==============================] - 0s 34us/step - loss: 0.4066 - accuracy: 0.8140 - val_loss: 0.4310 - val_accuracy: 0.8214\nEpoch 234/500\n5286/5286 [==============================] - 0s 38us/step - loss: 0.3964 - accuracy: 0.8188 - val_loss: 0.4213 - val_accuracy: 0.8129\nEpoch 235/500\n5286/5286 [==============================] - 0s 34us/step - loss: 0.4051 - accuracy: 0.8146 - val_loss: 0.4152 - val_accuracy: 0.8214\nEpoch 236/500\n5286/5286 [==============================] - 0s 34us/step - loss: 0.4041 - accuracy: 0.8176 - val_loss: 0.4217 - val_accuracy: 0.8129\nEpoch 237/500\n5286/5286 [==============================] - 0s 35us/step - loss: 0.4077 - accuracy: 0.8169 - val_loss: 0.4184 - val_accuracy: 0.8129\nEpoch 238/500\n5286/5286 [==============================] - 0s 36us/step - loss: 0.4033 - accuracy: 0.8218 - val_loss: 0.4126 - val_accuracy: 0.8214\nEpoch 239/500\n5286/5286 [==============================] - 0s 45us/step - loss: 0.3990 - accuracy: 0.8235 - val_loss: 0.4158 - val_accuracy: 0.8112\nEpoch 240/500\n5286/5286 [==============================] - 0s 56us/step - loss: 0.3975 - accuracy: 0.8233 - val_loss: 0.4162 - val_accuracy: 0.8078\nEpoch 241/500\n5286/5286 [==============================] - 0s 55us/step - loss: 0.3931 - accuracy: 0.8201 - val_loss: 0.4112 - val_accuracy: 0.8350\nEpoch 242/500\n5286/5286 [==============================] - 0s 36us/step - loss: 0.4037 - accuracy: 0.8167 - val_loss: 0.4209 - val_accuracy: 0.8231\nEpoch 243/500\n5286/5286 [==============================] - 0s 36us/step - loss: 0.4064 - accuracy: 0.8161 - val_loss: 0.4122 - val_accuracy: 0.8163\nEpoch 244/500\n5286/5286 [==============================] - 0s 34us/step - loss: 0.3989 - accuracy: 0.8226 - val_loss: 0.4104 - val_accuracy: 0.8316\nEpoch 245/500\n5286/5286 [==============================] - 0s 34us/step - loss: 0.4005 - accuracy: 0.8203 - val_loss: 0.4309 - val_accuracy: 0.8027\nEpoch 246/500\n5286/5286 [==============================] - 0s 35us/step - loss: 0.3932 - accuracy: 0.8261 - val_loss: 0.4107 - val_accuracy: 0.8129\nEpoch 247/500\n5286/5286 [==============================] - 0s 37us/step - loss: 0.3913 - accuracy: 0.8235 - val_loss: 0.4294 - val_accuracy: 0.8078\nEpoch 248/500\n5286/5286 [==============================] - 0s 38us/step - loss: 0.3936 - accuracy: 0.8273 - val_loss: 0.4224 - val_accuracy: 0.8197\nEpoch 249/500\n5286/5286 [==============================] - 0s 34us/step - loss: 0.3961 - accuracy: 0.8231 - val_loss: 0.4120 - val_accuracy: 0.8265\nEpoch 250/500\n5286/5286 [==============================] - 0s 35us/step - loss: 0.3994 - accuracy: 0.8224 - val_loss: 0.4261 - val_accuracy: 0.8180\nEpoch 251/500\n5286/5286 [==============================] - 0s 34us/step - loss: 0.3960 - accuracy: 0.8214 - val_loss: 0.4164 - val_accuracy: 0.8163\nEpoch 252/500\n5286/5286 [==============================] - 0s 35us/step - loss: 0.3943 - accuracy: 0.8199 - val_loss: 0.4155 - val_accuracy: 0.8299\nEpoch 253/500\n5286/5286 [==============================] - 0s 34us/step - loss: 0.3996 - accuracy: 0.8212 - val_loss: 0.4094 - val_accuracy: 0.8384\nEpoch 254/500\n5286/5286 [==============================] - 0s 35us/step - loss: 0.3903 - accuracy: 0.8284 - val_loss: 0.4121 - val_accuracy: 0.8316\nEpoch 255/500\n5286/5286 [==============================] - 0s 34us/step - loss: 0.3918 - accuracy: 0.8277 - val_loss: 0.4109 - val_accuracy: 0.8129\nEpoch 256/500\n5286/5286 [==============================] - 0s 34us/step - loss: 0.3905 - accuracy: 0.8239 - val_loss: 0.4094 - val_accuracy: 0.8248\nEpoch 257/500\n5286/5286 [==============================] - 0s 36us/step - loss: 0.3896 - accuracy: 0.8307 - val_loss: 0.4135 - val_accuracy: 0.8299\nEpoch 258/500\n5286/5286 [==============================] - 0s 37us/step - loss: 0.3951 - accuracy: 0.8241 - val_loss: 0.4309 - val_accuracy: 0.8010\nEpoch 259/500\n5286/5286 [==============================] - 0s 36us/step - loss: 0.3957 - accuracy: 0.8252 - val_loss: 0.4070 - val_accuracy: 0.8265\nEpoch 260/500\n5286/5286 [==============================] - 0s 35us/step - loss: 0.3894 - accuracy: 0.8292 - val_loss: 0.4084 - val_accuracy: 0.8197\nEpoch 261/500\n5286/5286 [==============================] - 0s 36us/step - loss: 0.3902 - accuracy: 0.8220 - val_loss: 0.4191 - val_accuracy: 0.8163\nEpoch 262/500\n5286/5286 [==============================] - 0s 35us/step - loss: 0.3859 - accuracy: 0.8278 - val_loss: 0.4177 - val_accuracy: 0.8129\nEpoch 263/500\n5286/5286 [==============================] - 0s 36us/step - loss: 0.3822 - accuracy: 0.8295 - val_loss: 0.4032 - val_accuracy: 0.8299\nEpoch 264/500\n5286/5286 [==============================] - 0s 35us/step - loss: 0.3845 - accuracy: 0.8305 - val_loss: 0.4244 - val_accuracy: 0.7993\nEpoch 265/500\n5286/5286 [==============================] - 0s 35us/step - loss: 0.3852 - accuracy: 0.8216 - val_loss: 0.4023 - val_accuracy: 0.8503\nEpoch 266/500\n5286/5286 [==============================] - 0s 36us/step - loss: 0.3843 - accuracy: 0.8294 - val_loss: 0.3962 - val_accuracy: 0.8350\nEpoch 267/500\n5286/5286 [==============================] - 0s 35us/step - loss: 0.3869 - accuracy: 0.8267 - val_loss: 0.4194 - val_accuracy: 0.8214\nEpoch 268/500\n5286/5286 [==============================] - 0s 34us/step - loss: 0.3830 - accuracy: 0.8284 - val_loss: 0.4168 - val_accuracy: 0.8163\nEpoch 269/500\n5286/5286 [==============================] - 0s 42us/step - loss: 0.3903 - accuracy: 0.8235 - val_loss: 0.4210 - val_accuracy: 0.8146\nEpoch 270/500\n5286/5286 [==============================] - 0s 36us/step - loss: 0.3830 - accuracy: 0.8318 - val_loss: 0.4225 - val_accuracy: 0.8163\nEpoch 271/500\n5286/5286 [==============================] - 0s 36us/step - loss: 0.3935 - accuracy: 0.8231 - val_loss: 0.4043 - val_accuracy: 0.8095\nEpoch 272/500\n5286/5286 [==============================] - 0s 36us/step - loss: 0.3859 - accuracy: 0.8301 - val_loss: 0.4149 - val_accuracy: 0.8214\nEpoch 273/500\n5286/5286 [==============================] - 0s 36us/step - loss: 0.3893 - accuracy: 0.8212 - val_loss: 0.4091 - val_accuracy: 0.8316\nEpoch 274/500\n5286/5286 [==============================] - 0s 36us/step - loss: 0.3826 - accuracy: 0.8307 - val_loss: 0.4070 - val_accuracy: 0.8265\nEpoch 275/500\n5286/5286 [==============================] - 0s 38us/step - loss: 0.3802 - accuracy: 0.8292 - val_loss: 0.4045 - val_accuracy: 0.8265\nEpoch 276/500\n5286/5286 [==============================] - 0s 36us/step - loss: 0.3842 - accuracy: 0.8292 - val_loss: 0.3991 - val_accuracy: 0.8316\nEpoch 277/500\n","name":"stdout"},{"output_type":"stream","text":"5286/5286 [==============================] - 0s 39us/step - loss: 0.3828 - accuracy: 0.8331 - val_loss: 0.4114 - val_accuracy: 0.8248\nEpoch 278/500\n5286/5286 [==============================] - 0s 35us/step - loss: 0.3838 - accuracy: 0.8318 - val_loss: 0.4183 - val_accuracy: 0.8299\nEpoch 279/500\n5286/5286 [==============================] - 0s 36us/step - loss: 0.3851 - accuracy: 0.8292 - val_loss: 0.4031 - val_accuracy: 0.8350\nEpoch 280/500\n5286/5286 [==============================] - 0s 40us/step - loss: 0.3834 - accuracy: 0.8309 - val_loss: 0.4240 - val_accuracy: 0.8044\nEpoch 281/500\n5286/5286 [==============================] - 0s 35us/step - loss: 0.3814 - accuracy: 0.8277 - val_loss: 0.4035 - val_accuracy: 0.8333\nEpoch 282/500\n5286/5286 [==============================] - 0s 34us/step - loss: 0.3845 - accuracy: 0.8316 - val_loss: 0.4056 - val_accuracy: 0.8214\nEpoch 283/500\n5286/5286 [==============================] - 0s 35us/step - loss: 0.3794 - accuracy: 0.8326 - val_loss: 0.4226 - val_accuracy: 0.8163\nEpoch 284/500\n5286/5286 [==============================] - 0s 36us/step - loss: 0.3842 - accuracy: 0.8288 - val_loss: 0.3996 - val_accuracy: 0.8214\nEpoch 285/500\n5286/5286 [==============================] - 0s 36us/step - loss: 0.3897 - accuracy: 0.8305 - val_loss: 0.4147 - val_accuracy: 0.8112\nEpoch 286/500\n5286/5286 [==============================] - 0s 35us/step - loss: 0.3788 - accuracy: 0.8295 - val_loss: 0.4329 - val_accuracy: 0.8095\nEpoch 287/500\n5286/5286 [==============================] - 0s 36us/step - loss: 0.3839 - accuracy: 0.8282 - val_loss: 0.4032 - val_accuracy: 0.8231\nEpoch 288/500\n5286/5286 [==============================] - 0s 36us/step - loss: 0.3761 - accuracy: 0.8383 - val_loss: 0.4030 - val_accuracy: 0.8299\nEpoch 289/500\n5286/5286 [==============================] - 0s 36us/step - loss: 0.3795 - accuracy: 0.8345 - val_loss: 0.4071 - val_accuracy: 0.8282\nEpoch 290/500\n5286/5286 [==============================] - 0s 36us/step - loss: 0.3825 - accuracy: 0.8320 - val_loss: 0.4057 - val_accuracy: 0.8418\nEpoch 291/500\n5286/5286 [==============================] - 0s 36us/step - loss: 0.3788 - accuracy: 0.8267 - val_loss: 0.3970 - val_accuracy: 0.8316\nEpoch 292/500\n5286/5286 [==============================] - 0s 36us/step - loss: 0.3790 - accuracy: 0.8326 - val_loss: 0.4397 - val_accuracy: 0.8027\nEpoch 293/500\n5286/5286 [==============================] - 0s 36us/step - loss: 0.3783 - accuracy: 0.8337 - val_loss: 0.4087 - val_accuracy: 0.8350\nEpoch 294/500\n5286/5286 [==============================] - 0s 36us/step - loss: 0.3804 - accuracy: 0.8335 - val_loss: 0.4028 - val_accuracy: 0.8333\nEpoch 295/500\n5286/5286 [==============================] - 0s 53us/step - loss: 0.3764 - accuracy: 0.8365 - val_loss: 0.4115 - val_accuracy: 0.8248\nEpoch 296/500\n5286/5286 [==============================] - 0s 55us/step - loss: 0.3811 - accuracy: 0.8314 - val_loss: 0.4003 - val_accuracy: 0.8401\nEpoch 297/500\n5286/5286 [==============================] - 0s 50us/step - loss: 0.3718 - accuracy: 0.8335 - val_loss: 0.4085 - val_accuracy: 0.8401\nEpoch 298/500\n5286/5286 [==============================] - 0s 36us/step - loss: 0.3761 - accuracy: 0.8360 - val_loss: 0.3894 - val_accuracy: 0.8350\nEpoch 299/500\n5286/5286 [==============================] - 0s 34us/step - loss: 0.3781 - accuracy: 0.8347 - val_loss: 0.4044 - val_accuracy: 0.8316\nEpoch 300/500\n5286/5286 [==============================] - 0s 34us/step - loss: 0.3761 - accuracy: 0.8339 - val_loss: 0.3917 - val_accuracy: 0.8452\nEpoch 301/500\n5286/5286 [==============================] - 0s 34us/step - loss: 0.3724 - accuracy: 0.8379 - val_loss: 0.3996 - val_accuracy: 0.8316\nEpoch 302/500\n5286/5286 [==============================] - 0s 35us/step - loss: 0.3708 - accuracy: 0.8379 - val_loss: 0.4183 - val_accuracy: 0.8282\nEpoch 303/500\n5286/5286 [==============================] - 0s 34us/step - loss: 0.3758 - accuracy: 0.8347 - val_loss: 0.3950 - val_accuracy: 0.8469\nEpoch 304/500\n5286/5286 [==============================] - 0s 34us/step - loss: 0.3727 - accuracy: 0.8288 - val_loss: 0.3982 - val_accuracy: 0.8299\nEpoch 305/500\n5286/5286 [==============================] - 0s 34us/step - loss: 0.3714 - accuracy: 0.8383 - val_loss: 0.4066 - val_accuracy: 0.8282\nEpoch 306/500\n5286/5286 [==============================] - 0s 35us/step - loss: 0.3713 - accuracy: 0.8320 - val_loss: 0.3980 - val_accuracy: 0.8418\nEpoch 307/500\n5286/5286 [==============================] - 0s 36us/step - loss: 0.3683 - accuracy: 0.8373 - val_loss: 0.3990 - val_accuracy: 0.8231\nEpoch 308/500\n5286/5286 [==============================] - 0s 36us/step - loss: 0.3822 - accuracy: 0.8328 - val_loss: 0.4022 - val_accuracy: 0.8197\nEpoch 309/500\n5286/5286 [==============================] - 0s 35us/step - loss: 0.3762 - accuracy: 0.8326 - val_loss: 0.3923 - val_accuracy: 0.8401\nEpoch 310/500\n5286/5286 [==============================] - 0s 35us/step - loss: 0.3663 - accuracy: 0.8392 - val_loss: 0.3926 - val_accuracy: 0.8367\nEpoch 311/500\n5286/5286 [==============================] - 0s 35us/step - loss: 0.3730 - accuracy: 0.8356 - val_loss: 0.3935 - val_accuracy: 0.8265\nEpoch 312/500\n5286/5286 [==============================] - 0s 34us/step - loss: 0.3740 - accuracy: 0.8331 - val_loss: 0.3904 - val_accuracy: 0.8486\nEpoch 313/500\n5286/5286 [==============================] - 0s 34us/step - loss: 0.3749 - accuracy: 0.8345 - val_loss: 0.3920 - val_accuracy: 0.8452\nEpoch 314/500\n5286/5286 [==============================] - 0s 35us/step - loss: 0.3721 - accuracy: 0.8396 - val_loss: 0.3881 - val_accuracy: 0.8469\nEpoch 315/500\n5286/5286 [==============================] - 0s 36us/step - loss: 0.3815 - accuracy: 0.8343 - val_loss: 0.4043 - val_accuracy: 0.8197\nEpoch 316/500\n5286/5286 [==============================] - 0s 36us/step - loss: 0.3644 - accuracy: 0.8348 - val_loss: 0.3985 - val_accuracy: 0.8299\nEpoch 317/500\n5286/5286 [==============================] - 0s 37us/step - loss: 0.3687 - accuracy: 0.8371 - val_loss: 0.3994 - val_accuracy: 0.8401\nEpoch 318/500\n5286/5286 [==============================] - 0s 36us/step - loss: 0.3664 - accuracy: 0.8388 - val_loss: 0.4104 - val_accuracy: 0.8163\nEpoch 319/500\n5286/5286 [==============================] - 0s 34us/step - loss: 0.3709 - accuracy: 0.8301 - val_loss: 0.3961 - val_accuracy: 0.8401\nEpoch 320/500\n5286/5286 [==============================] - 0s 35us/step - loss: 0.3718 - accuracy: 0.8394 - val_loss: 0.3921 - val_accuracy: 0.8469\nEpoch 321/500\n5286/5286 [==============================] - 0s 36us/step - loss: 0.3699 - accuracy: 0.8398 - val_loss: 0.3993 - val_accuracy: 0.8401\nEpoch 322/500\n5286/5286 [==============================] - 0s 36us/step - loss: 0.3734 - accuracy: 0.8354 - val_loss: 0.4034 - val_accuracy: 0.8231\nEpoch 323/500\n5286/5286 [==============================] - 0s 36us/step - loss: 0.3699 - accuracy: 0.8354 - val_loss: 0.3960 - val_accuracy: 0.8248\nEpoch 324/500\n5286/5286 [==============================] - 0s 36us/step - loss: 0.3648 - accuracy: 0.8435 - val_loss: 0.3920 - val_accuracy: 0.8452\nEpoch 325/500\n5286/5286 [==============================] - 0s 36us/step - loss: 0.3636 - accuracy: 0.8430 - val_loss: 0.4085 - val_accuracy: 0.8282\nEpoch 326/500\n5286/5286 [==============================] - 0s 36us/step - loss: 0.3693 - accuracy: 0.8394 - val_loss: 0.3921 - val_accuracy: 0.8418\nEpoch 327/500\n5286/5286 [==============================] - 0s 36us/step - loss: 0.3604 - accuracy: 0.8403 - val_loss: 0.4142 - val_accuracy: 0.8129\nEpoch 328/500\n5286/5286 [==============================] - 0s 39us/step - loss: 0.3636 - accuracy: 0.8365 - val_loss: 0.3982 - val_accuracy: 0.8248\nEpoch 329/500\n5286/5286 [==============================] - 0s 35us/step - loss: 0.3683 - accuracy: 0.8347 - val_loss: 0.3973 - val_accuracy: 0.8231\nEpoch 330/500\n5286/5286 [==============================] - 0s 37us/step - loss: 0.3656 - accuracy: 0.8424 - val_loss: 0.4130 - val_accuracy: 0.8265\nEpoch 331/500\n5286/5286 [==============================] - 0s 39us/step - loss: 0.3687 - accuracy: 0.8415 - val_loss: 0.3933 - val_accuracy: 0.8384\nEpoch 332/500\n","name":"stdout"},{"output_type":"stream","text":"5286/5286 [==============================] - 0s 39us/step - loss: 0.3663 - accuracy: 0.8354 - val_loss: 0.3933 - val_accuracy: 0.8316\nEpoch 333/500\n5286/5286 [==============================] - 0s 37us/step - loss: 0.3718 - accuracy: 0.8375 - val_loss: 0.3908 - val_accuracy: 0.8316\nEpoch 334/500\n5286/5286 [==============================] - 0s 37us/step - loss: 0.3577 - accuracy: 0.8377 - val_loss: 0.3961 - val_accuracy: 0.8197\nEpoch 335/500\n5286/5286 [==============================] - 0s 38us/step - loss: 0.3625 - accuracy: 0.8371 - val_loss: 0.3837 - val_accuracy: 0.8418\nEpoch 336/500\n5286/5286 [==============================] - 0s 38us/step - loss: 0.3705 - accuracy: 0.8345 - val_loss: 0.3897 - val_accuracy: 0.8265\nEpoch 337/500\n5286/5286 [==============================] - 0s 37us/step - loss: 0.3665 - accuracy: 0.8381 - val_loss: 0.3858 - val_accuracy: 0.8401\nEpoch 338/500\n5286/5286 [==============================] - 0s 37us/step - loss: 0.3612 - accuracy: 0.8396 - val_loss: 0.3862 - val_accuracy: 0.8520\nEpoch 339/500\n5286/5286 [==============================] - 0s 37us/step - loss: 0.3534 - accuracy: 0.8449 - val_loss: 0.3969 - val_accuracy: 0.8282\nEpoch 340/500\n5286/5286 [==============================] - 0s 35us/step - loss: 0.3611 - accuracy: 0.8379 - val_loss: 0.3998 - val_accuracy: 0.8418\nEpoch 341/500\n5286/5286 [==============================] - 0s 35us/step - loss: 0.3658 - accuracy: 0.8394 - val_loss: 0.3930 - val_accuracy: 0.8333\nEpoch 342/500\n5286/5286 [==============================] - 0s 37us/step - loss: 0.3556 - accuracy: 0.8403 - val_loss: 0.3871 - val_accuracy: 0.8435\nEpoch 343/500\n5286/5286 [==============================] - 0s 37us/step - loss: 0.3662 - accuracy: 0.8369 - val_loss: 0.4128 - val_accuracy: 0.8282\nEpoch 344/500\n5286/5286 [==============================] - 0s 37us/step - loss: 0.3615 - accuracy: 0.8409 - val_loss: 0.3998 - val_accuracy: 0.8316\nEpoch 345/500\n5286/5286 [==============================] - 0s 38us/step - loss: 0.3634 - accuracy: 0.8432 - val_loss: 0.3855 - val_accuracy: 0.8520\nEpoch 346/500\n5286/5286 [==============================] - 0s 36us/step - loss: 0.3559 - accuracy: 0.8445 - val_loss: 0.4007 - val_accuracy: 0.8214\nEpoch 347/500\n5286/5286 [==============================] - 0s 37us/step - loss: 0.3586 - accuracy: 0.8388 - val_loss: 0.4274 - val_accuracy: 0.8078\nEpoch 348/500\n5286/5286 [==============================] - 0s 37us/step - loss: 0.3523 - accuracy: 0.8441 - val_loss: 0.4098 - val_accuracy: 0.8197\nEpoch 349/500\n5286/5286 [==============================] - 0s 37us/step - loss: 0.3509 - accuracy: 0.8451 - val_loss: 0.4071 - val_accuracy: 0.8248\nEpoch 350/500\n5286/5286 [==============================] - 0s 37us/step - loss: 0.3587 - accuracy: 0.8407 - val_loss: 0.3800 - val_accuracy: 0.8452\nEpoch 351/500\n5286/5286 [==============================] - 0s 65us/step - loss: 0.3638 - accuracy: 0.8405 - val_loss: 0.4348 - val_accuracy: 0.8010\nEpoch 352/500\n5286/5286 [==============================] - 0s 57us/step - loss: 0.3596 - accuracy: 0.8435 - val_loss: 0.3862 - val_accuracy: 0.8452\nEpoch 353/500\n5286/5286 [==============================] - 0s 43us/step - loss: 0.3608 - accuracy: 0.8409 - val_loss: 0.3892 - val_accuracy: 0.8435\nEpoch 354/500\n5286/5286 [==============================] - 0s 34us/step - loss: 0.3583 - accuracy: 0.8411 - val_loss: 0.4013 - val_accuracy: 0.8333\nEpoch 355/500\n5286/5286 [==============================] - 0s 35us/step - loss: 0.3631 - accuracy: 0.8401 - val_loss: 0.4034 - val_accuracy: 0.8316\nEpoch 356/500\n5286/5286 [==============================] - 0s 36us/step - loss: 0.3552 - accuracy: 0.8473 - val_loss: 0.4079 - val_accuracy: 0.8265\nEpoch 357/500\n5286/5286 [==============================] - 0s 36us/step - loss: 0.3565 - accuracy: 0.8418 - val_loss: 0.3957 - val_accuracy: 0.8316\nEpoch 358/500\n5286/5286 [==============================] - 0s 36us/step - loss: 0.3560 - accuracy: 0.8447 - val_loss: 0.3864 - val_accuracy: 0.8435\nEpoch 359/500\n5286/5286 [==============================] - 0s 36us/step - loss: 0.3593 - accuracy: 0.8390 - val_loss: 0.3907 - val_accuracy: 0.8248\nEpoch 360/500\n5286/5286 [==============================] - 0s 35us/step - loss: 0.3514 - accuracy: 0.8451 - val_loss: 0.3839 - val_accuracy: 0.8350\nEpoch 361/500\n5286/5286 [==============================] - 0s 34us/step - loss: 0.3609 - accuracy: 0.8388 - val_loss: 0.3987 - val_accuracy: 0.8214\nEpoch 362/500\n5286/5286 [==============================] - 0s 55us/step - loss: 0.3627 - accuracy: 0.8420 - val_loss: 0.3932 - val_accuracy: 0.8503\nEpoch 363/500\n5286/5286 [==============================] - 0s 64us/step - loss: 0.3619 - accuracy: 0.8418 - val_loss: 0.3785 - val_accuracy: 0.8401\nEpoch 364/500\n5286/5286 [==============================] - 0s 59us/step - loss: 0.3519 - accuracy: 0.8437 - val_loss: 0.4095 - val_accuracy: 0.8180\nEpoch 365/500\n5286/5286 [==============================] - 0s 55us/step - loss: 0.3516 - accuracy: 0.8422 - val_loss: 0.3858 - val_accuracy: 0.8333\nEpoch 366/500\n5286/5286 [==============================] - 0s 35us/step - loss: 0.3591 - accuracy: 0.8441 - val_loss: 0.3788 - val_accuracy: 0.8418\nEpoch 367/500\n5286/5286 [==============================] - 0s 36us/step - loss: 0.3597 - accuracy: 0.8409 - val_loss: 0.3939 - val_accuracy: 0.8435\nEpoch 368/500\n5286/5286 [==============================] - 0s 37us/step - loss: 0.3556 - accuracy: 0.8424 - val_loss: 0.3804 - val_accuracy: 0.8503\nEpoch 369/500\n5286/5286 [==============================] - 0s 36us/step - loss: 0.3519 - accuracy: 0.8502 - val_loss: 0.3840 - val_accuracy: 0.8571\nEpoch 370/500\n5286/5286 [==============================] - 0s 36us/step - loss: 0.3612 - accuracy: 0.8407 - val_loss: 0.3831 - val_accuracy: 0.8401\nEpoch 371/500\n5286/5286 [==============================] - 0s 34us/step - loss: 0.3587 - accuracy: 0.8443 - val_loss: 0.3734 - val_accuracy: 0.8554\nEpoch 372/500\n5286/5286 [==============================] - 0s 34us/step - loss: 0.3565 - accuracy: 0.8445 - val_loss: 0.4004 - val_accuracy: 0.8384\nEpoch 373/500\n5286/5286 [==============================] - 0s 34us/step - loss: 0.3497 - accuracy: 0.8485 - val_loss: 0.4307 - val_accuracy: 0.8078\nEpoch 374/500\n5286/5286 [==============================] - 0s 34us/step - loss: 0.3601 - accuracy: 0.8396 - val_loss: 0.3856 - val_accuracy: 0.8435\nEpoch 375/500\n5286/5286 [==============================] - 0s 34us/step - loss: 0.3495 - accuracy: 0.8456 - val_loss: 0.3931 - val_accuracy: 0.8333\nEpoch 376/500\n5286/5286 [==============================] - 0s 34us/step - loss: 0.3477 - accuracy: 0.8511 - val_loss: 0.3771 - val_accuracy: 0.8486\nEpoch 377/500\n5286/5286 [==============================] - 0s 39us/step - loss: 0.3477 - accuracy: 0.8483 - val_loss: 0.3895 - val_accuracy: 0.8384\nEpoch 378/500\n5286/5286 [==============================] - 0s 41us/step - loss: 0.3499 - accuracy: 0.8470 - val_loss: 0.3976 - val_accuracy: 0.8350\nEpoch 379/500\n5286/5286 [==============================] - 0s 37us/step - loss: 0.3378 - accuracy: 0.8471 - val_loss: 0.4108 - val_accuracy: 0.8282\nEpoch 380/500\n5286/5286 [==============================] - 0s 36us/step - loss: 0.3543 - accuracy: 0.8483 - val_loss: 0.3908 - val_accuracy: 0.8333\nEpoch 381/500\n5286/5286 [==============================] - 0s 37us/step - loss: 0.3516 - accuracy: 0.8439 - val_loss: 0.3875 - val_accuracy: 0.8452\nEpoch 382/500\n5286/5286 [==============================] - 0s 38us/step - loss: 0.3498 - accuracy: 0.8496 - val_loss: 0.3755 - val_accuracy: 0.8503\nEpoch 383/500\n5286/5286 [==============================] - 0s 35us/step - loss: 0.3502 - accuracy: 0.8470 - val_loss: 0.3801 - val_accuracy: 0.8503\nEpoch 384/500\n5286/5286 [==============================] - 0s 34us/step - loss: 0.3523 - accuracy: 0.8483 - val_loss: 0.3716 - val_accuracy: 0.8418\nEpoch 385/500\n5286/5286 [==============================] - 0s 35us/step - loss: 0.3513 - accuracy: 0.8409 - val_loss: 0.3797 - val_accuracy: 0.8435\nEpoch 386/500\n5286/5286 [==============================] - 0s 36us/step - loss: 0.3511 - accuracy: 0.8483 - val_loss: 0.3879 - val_accuracy: 0.8435\nEpoch 387/500\n","name":"stdout"},{"output_type":"stream","text":"5286/5286 [==============================] - 0s 34us/step - loss: 0.3519 - accuracy: 0.8468 - val_loss: 0.3885 - val_accuracy: 0.8418\nEpoch 388/500\n5286/5286 [==============================] - 0s 34us/step - loss: 0.3469 - accuracy: 0.8519 - val_loss: 0.3820 - val_accuracy: 0.8384\nEpoch 389/500\n5286/5286 [==============================] - 0s 35us/step - loss: 0.3462 - accuracy: 0.8475 - val_loss: 0.3854 - val_accuracy: 0.8384\nEpoch 390/500\n5286/5286 [==============================] - 0s 36us/step - loss: 0.3584 - accuracy: 0.8439 - val_loss: 0.4035 - val_accuracy: 0.8316\nEpoch 391/500\n5286/5286 [==============================] - 0s 36us/step - loss: 0.3510 - accuracy: 0.8462 - val_loss: 0.3799 - val_accuracy: 0.8367\nEpoch 392/500\n5286/5286 [==============================] - 0s 34us/step - loss: 0.3442 - accuracy: 0.8454 - val_loss: 0.3994 - val_accuracy: 0.8282\nEpoch 393/500\n5286/5286 [==============================] - 0s 34us/step - loss: 0.3432 - accuracy: 0.8496 - val_loss: 0.3903 - val_accuracy: 0.8537\nEpoch 394/500\n5286/5286 [==============================] - 0s 36us/step - loss: 0.3496 - accuracy: 0.8473 - val_loss: 0.3877 - val_accuracy: 0.8367\nEpoch 395/500\n5286/5286 [==============================] - 0s 36us/step - loss: 0.3435 - accuracy: 0.8490 - val_loss: 0.3818 - val_accuracy: 0.8452\nEpoch 396/500\n5286/5286 [==============================] - 0s 36us/step - loss: 0.3474 - accuracy: 0.8464 - val_loss: 0.3991 - val_accuracy: 0.8350\nEpoch 397/500\n5286/5286 [==============================] - 0s 36us/step - loss: 0.3472 - accuracy: 0.8445 - val_loss: 0.3795 - val_accuracy: 0.8435\nEpoch 398/500\n5286/5286 [==============================] - 0s 36us/step - loss: 0.3439 - accuracy: 0.8505 - val_loss: 0.3862 - val_accuracy: 0.8316\nEpoch 399/500\n5286/5286 [==============================] - 0s 37us/step - loss: 0.3436 - accuracy: 0.8511 - val_loss: 0.3789 - val_accuracy: 0.8367\nEpoch 400/500\n5286/5286 [==============================] - 0s 36us/step - loss: 0.3455 - accuracy: 0.8504 - val_loss: 0.3992 - val_accuracy: 0.8350\nEpoch 401/500\n5286/5286 [==============================] - 0s 36us/step - loss: 0.3426 - accuracy: 0.8500 - val_loss: 0.4037 - val_accuracy: 0.8197\nEpoch 402/500\n5286/5286 [==============================] - 0s 36us/step - loss: 0.3447 - accuracy: 0.8477 - val_loss: 0.4453 - val_accuracy: 0.7908\nEpoch 403/500\n5286/5286 [==============================] - 0s 37us/step - loss: 0.3463 - accuracy: 0.8445 - val_loss: 0.3947 - val_accuracy: 0.8231\nEpoch 404/500\n5286/5286 [==============================] - 0s 58us/step - loss: 0.3517 - accuracy: 0.8473 - val_loss: 0.3781 - val_accuracy: 0.8520\nEpoch 405/500\n5286/5286 [==============================] - 0s 57us/step - loss: 0.3418 - accuracy: 0.8515 - val_loss: 0.4086 - val_accuracy: 0.8112\nEpoch 406/500\n5286/5286 [==============================] - 0s 45us/step - loss: 0.3450 - accuracy: 0.8513 - val_loss: 0.3755 - val_accuracy: 0.8520\nEpoch 407/500\n5286/5286 [==============================] - 0s 35us/step - loss: 0.3437 - accuracy: 0.8487 - val_loss: 0.3781 - val_accuracy: 0.8367\nEpoch 408/500\n5286/5286 [==============================] - 0s 35us/step - loss: 0.3387 - accuracy: 0.8492 - val_loss: 0.3993 - val_accuracy: 0.8180\nEpoch 409/500\n5286/5286 [==============================] - 0s 34us/step - loss: 0.3508 - accuracy: 0.8513 - val_loss: 0.3672 - val_accuracy: 0.8571\nEpoch 410/500\n5286/5286 [==============================] - 0s 35us/step - loss: 0.3545 - accuracy: 0.8451 - val_loss: 0.3699 - val_accuracy: 0.8435\nEpoch 411/500\n5286/5286 [==============================] - 0s 36us/step - loss: 0.3493 - accuracy: 0.8458 - val_loss: 0.4003 - val_accuracy: 0.8452\nEpoch 412/500\n5286/5286 [==============================] - 0s 35us/step - loss: 0.3394 - accuracy: 0.8466 - val_loss: 0.3818 - val_accuracy: 0.8350\nEpoch 413/500\n5286/5286 [==============================] - 0s 34us/step - loss: 0.3400 - accuracy: 0.8523 - val_loss: 0.3907 - val_accuracy: 0.8265\nEpoch 414/500\n5286/5286 [==============================] - 0s 34us/step - loss: 0.3365 - accuracy: 0.8517 - val_loss: 0.3697 - val_accuracy: 0.8571\nEpoch 415/500\n5286/5286 [==============================] - 0s 34us/step - loss: 0.3423 - accuracy: 0.8515 - val_loss: 0.3952 - val_accuracy: 0.8282\nEpoch 416/500\n5286/5286 [==============================] - 0s 34us/step - loss: 0.3442 - accuracy: 0.8473 - val_loss: 0.3803 - val_accuracy: 0.8571\nEpoch 417/500\n5286/5286 [==============================] - 0s 34us/step - loss: 0.3420 - accuracy: 0.8468 - val_loss: 0.3735 - val_accuracy: 0.8469\nEpoch 418/500\n5286/5286 [==============================] - 0s 36us/step - loss: 0.3372 - accuracy: 0.8560 - val_loss: 0.3830 - val_accuracy: 0.8469\nEpoch 419/500\n5286/5286 [==============================] - 0s 36us/step - loss: 0.3338 - accuracy: 0.8494 - val_loss: 0.3972 - val_accuracy: 0.8282\nEpoch 420/500\n5286/5286 [==============================] - 0s 35us/step - loss: 0.3443 - accuracy: 0.8504 - val_loss: 0.3885 - val_accuracy: 0.8367\nEpoch 421/500\n5286/5286 [==============================] - 0s 35us/step - loss: 0.3396 - accuracy: 0.8553 - val_loss: 0.3835 - val_accuracy: 0.8316\nEpoch 422/500\n5286/5286 [==============================] - 0s 36us/step - loss: 0.3401 - accuracy: 0.8488 - val_loss: 0.3964 - val_accuracy: 0.8367\nEpoch 423/500\n5286/5286 [==============================] - 0s 37us/step - loss: 0.3386 - accuracy: 0.8521 - val_loss: 0.3674 - val_accuracy: 0.8452\nEpoch 424/500\n5286/5286 [==============================] - 0s 36us/step - loss: 0.3367 - accuracy: 0.8555 - val_loss: 0.3937 - val_accuracy: 0.8333\nEpoch 425/500\n5286/5286 [==============================] - 0s 36us/step - loss: 0.3414 - accuracy: 0.8528 - val_loss: 0.3804 - val_accuracy: 0.8469\nEpoch 426/500\n5286/5286 [==============================] - 0s 35us/step - loss: 0.3354 - accuracy: 0.8551 - val_loss: 0.3748 - val_accuracy: 0.8503\nEpoch 427/500\n5286/5286 [==============================] - 0s 36us/step - loss: 0.3378 - accuracy: 0.8591 - val_loss: 0.3728 - val_accuracy: 0.8401\nEpoch 428/500\n5286/5286 [==============================] - 0s 38us/step - loss: 0.3478 - accuracy: 0.8505 - val_loss: 0.3718 - val_accuracy: 0.8350\nEpoch 429/500\n5286/5286 [==============================] - 0s 40us/step - loss: 0.3341 - accuracy: 0.8538 - val_loss: 0.3740 - val_accuracy: 0.8418\nEpoch 430/500\n5286/5286 [==============================] - 0s 36us/step - loss: 0.3472 - accuracy: 0.8473 - val_loss: 0.3894 - val_accuracy: 0.8435\nEpoch 431/500\n5286/5286 [==============================] - 0s 36us/step - loss: 0.3361 - accuracy: 0.8538 - val_loss: 0.3631 - val_accuracy: 0.8571\nEpoch 432/500\n5286/5286 [==============================] - 0s 35us/step - loss: 0.3348 - accuracy: 0.8504 - val_loss: 0.3745 - val_accuracy: 0.8469\nEpoch 433/500\n5286/5286 [==============================] - 0s 35us/step - loss: 0.3396 - accuracy: 0.8505 - val_loss: 0.3740 - val_accuracy: 0.8469\nEpoch 434/500\n5286/5286 [==============================] - 0s 34us/step - loss: 0.3407 - accuracy: 0.8485 - val_loss: 0.3718 - val_accuracy: 0.8316\nEpoch 435/500\n5286/5286 [==============================] - 0s 35us/step - loss: 0.3342 - accuracy: 0.8530 - val_loss: 0.3764 - val_accuracy: 0.8503\nEpoch 436/500\n5286/5286 [==============================] - 0s 36us/step - loss: 0.3411 - accuracy: 0.8504 - val_loss: 0.3817 - val_accuracy: 0.8622\nEpoch 437/500\n5286/5286 [==============================] - 0s 36us/step - loss: 0.3403 - accuracy: 0.8507 - val_loss: 0.3729 - val_accuracy: 0.8571\nEpoch 438/500\n5286/5286 [==============================] - 0s 39us/step - loss: 0.3290 - accuracy: 0.8602 - val_loss: 0.3796 - val_accuracy: 0.8469\nEpoch 439/500\n5286/5286 [==============================] - 0s 37us/step - loss: 0.3382 - accuracy: 0.8538 - val_loss: 0.3741 - val_accuracy: 0.8486\nEpoch 440/500\n5286/5286 [==============================] - 0s 36us/step - loss: 0.3427 - accuracy: 0.8551 - val_loss: 0.3896 - val_accuracy: 0.8333\nEpoch 441/500\n5286/5286 [==============================] - 0s 37us/step - loss: 0.3285 - accuracy: 0.8549 - val_loss: 0.3923 - val_accuracy: 0.8299\nEpoch 442/500\n","name":"stdout"},{"output_type":"stream","text":"5286/5286 [==============================] - 0s 37us/step - loss: 0.3300 - accuracy: 0.8577 - val_loss: 0.3877 - val_accuracy: 0.8384\nEpoch 443/500\n5286/5286 [==============================] - 0s 36us/step - loss: 0.3311 - accuracy: 0.8553 - val_loss: 0.3652 - val_accuracy: 0.8452\nEpoch 444/500\n5286/5286 [==============================] - 0s 37us/step - loss: 0.3434 - accuracy: 0.8545 - val_loss: 0.3849 - val_accuracy: 0.8469\nEpoch 445/500\n5286/5286 [==============================] - 0s 36us/step - loss: 0.3364 - accuracy: 0.8555 - val_loss: 0.3636 - val_accuracy: 0.8537\nEpoch 446/500\n5286/5286 [==============================] - 0s 36us/step - loss: 0.3315 - accuracy: 0.8519 - val_loss: 0.3794 - val_accuracy: 0.8486\nEpoch 447/500\n5286/5286 [==============================] - 0s 36us/step - loss: 0.3332 - accuracy: 0.8519 - val_loss: 0.3767 - val_accuracy: 0.8367\nEpoch 448/500\n5286/5286 [==============================] - 0s 39us/step - loss: 0.3417 - accuracy: 0.8490 - val_loss: 0.3888 - val_accuracy: 0.8265\nEpoch 449/500\n5286/5286 [==============================] - 0s 36us/step - loss: 0.3326 - accuracy: 0.8575 - val_loss: 0.3693 - val_accuracy: 0.8537\nEpoch 450/500\n5286/5286 [==============================] - 0s 37us/step - loss: 0.3321 - accuracy: 0.8547 - val_loss: 0.3663 - val_accuracy: 0.8520\nEpoch 451/500\n5286/5286 [==============================] - 0s 36us/step - loss: 0.3324 - accuracy: 0.8521 - val_loss: 0.3618 - val_accuracy: 0.8605\nEpoch 452/500\n5286/5286 [==============================] - 0s 36us/step - loss: 0.3382 - accuracy: 0.8574 - val_loss: 0.3714 - val_accuracy: 0.8452\nEpoch 453/500\n5286/5286 [==============================] - 0s 36us/step - loss: 0.3330 - accuracy: 0.8593 - val_loss: 0.3914 - val_accuracy: 0.8299\nEpoch 454/500\n5286/5286 [==============================] - 0s 37us/step - loss: 0.3283 - accuracy: 0.8568 - val_loss: 0.4011 - val_accuracy: 0.8316\nEpoch 455/500\n5286/5286 [==============================] - 0s 36us/step - loss: 0.3317 - accuracy: 0.8560 - val_loss: 0.3668 - val_accuracy: 0.8503\nEpoch 456/500\n5286/5286 [==============================] - 0s 36us/step - loss: 0.3342 - accuracy: 0.8560 - val_loss: 0.3637 - val_accuracy: 0.8537\nEpoch 457/500\n5286/5286 [==============================] - 0s 39us/step - loss: 0.3275 - accuracy: 0.8574 - val_loss: 0.3894 - val_accuracy: 0.8316\nEpoch 458/500\n5286/5286 [==============================] - 0s 36us/step - loss: 0.3379 - accuracy: 0.8519 - val_loss: 0.3609 - val_accuracy: 0.8605\nEpoch 459/500\n5286/5286 [==============================] - 0s 39us/step - loss: 0.3327 - accuracy: 0.8579 - val_loss: 0.3708 - val_accuracy: 0.8571\nEpoch 460/500\n5286/5286 [==============================] - 0s 64us/step - loss: 0.3397 - accuracy: 0.8526 - val_loss: 0.3813 - val_accuracy: 0.8520\nEpoch 461/500\n5286/5286 [==============================] - 0s 53us/step - loss: 0.3356 - accuracy: 0.8547 - val_loss: 0.3537 - val_accuracy: 0.8537\nEpoch 462/500\n5286/5286 [==============================] - 0s 36us/step - loss: 0.3363 - accuracy: 0.8526 - val_loss: 0.3810 - val_accuracy: 0.8452\nEpoch 463/500\n5286/5286 [==============================] - 0s 36us/step - loss: 0.3338 - accuracy: 0.8568 - val_loss: 0.3996 - val_accuracy: 0.8231\nEpoch 464/500\n5286/5286 [==============================] - 0s 35us/step - loss: 0.3264 - accuracy: 0.8628 - val_loss: 0.3809 - val_accuracy: 0.8367\nEpoch 465/500\n5286/5286 [==============================] - 0s 35us/step - loss: 0.3338 - accuracy: 0.8568 - val_loss: 0.4044 - val_accuracy: 0.8333\nEpoch 466/500\n5286/5286 [==============================] - 0s 35us/step - loss: 0.3302 - accuracy: 0.8589 - val_loss: 0.3645 - val_accuracy: 0.8554\nEpoch 467/500\n5286/5286 [==============================] - 0s 35us/step - loss: 0.3293 - accuracy: 0.8540 - val_loss: 0.3598 - val_accuracy: 0.8452\nEpoch 468/500\n5286/5286 [==============================] - 0s 36us/step - loss: 0.3288 - accuracy: 0.8549 - val_loss: 0.3572 - val_accuracy: 0.8588\nEpoch 469/500\n5286/5286 [==============================] - 0s 36us/step - loss: 0.3359 - accuracy: 0.8570 - val_loss: 0.3896 - val_accuracy: 0.8435\nEpoch 470/500\n5286/5286 [==============================] - 0s 36us/step - loss: 0.3351 - accuracy: 0.8538 - val_loss: 0.3826 - val_accuracy: 0.8469\nEpoch 471/500\n5286/5286 [==============================] - 0s 36us/step - loss: 0.3299 - accuracy: 0.8575 - val_loss: 0.3909 - val_accuracy: 0.8622\nEpoch 472/500\n5286/5286 [==============================] - 0s 36us/step - loss: 0.3356 - accuracy: 0.8526 - val_loss: 0.3728 - val_accuracy: 0.8435\nEpoch 473/500\n5286/5286 [==============================] - 0s 36us/step - loss: 0.3234 - accuracy: 0.8564 - val_loss: 0.3599 - val_accuracy: 0.8537\nEpoch 474/500\n5286/5286 [==============================] - 0s 36us/step - loss: 0.3321 - accuracy: 0.8581 - val_loss: 0.3955 - val_accuracy: 0.8350\nEpoch 475/500\n5286/5286 [==============================] - 0s 36us/step - loss: 0.3297 - accuracy: 0.8589 - val_loss: 0.3575 - val_accuracy: 0.8537\nEpoch 476/500\n5286/5286 [==============================] - 0s 36us/step - loss: 0.3231 - accuracy: 0.8593 - val_loss: 0.3603 - val_accuracy: 0.8571\nEpoch 477/500\n5286/5286 [==============================] - 0s 35us/step - loss: 0.3264 - accuracy: 0.8587 - val_loss: 0.3729 - val_accuracy: 0.8401\nEpoch 478/500\n5286/5286 [==============================] - 0s 36us/step - loss: 0.3336 - accuracy: 0.8566 - val_loss: 0.3636 - val_accuracy: 0.8605\nEpoch 479/500\n5286/5286 [==============================] - 0s 44us/step - loss: 0.3287 - accuracy: 0.8579 - val_loss: 0.3603 - val_accuracy: 0.8639\nEpoch 480/500\n5286/5286 [==============================] - 0s 36us/step - loss: 0.3344 - accuracy: 0.8558 - val_loss: 0.3839 - val_accuracy: 0.8435\nEpoch 481/500\n5286/5286 [==============================] - 0s 36us/step - loss: 0.3339 - accuracy: 0.8534 - val_loss: 0.3689 - val_accuracy: 0.8486\nEpoch 482/500\n5286/5286 [==============================] - 0s 35us/step - loss: 0.3296 - accuracy: 0.8585 - val_loss: 0.3657 - val_accuracy: 0.8435\nEpoch 483/500\n5286/5286 [==============================] - 0s 35us/step - loss: 0.3256 - accuracy: 0.8619 - val_loss: 0.3634 - val_accuracy: 0.8588\nEpoch 484/500\n5286/5286 [==============================] - 0s 35us/step - loss: 0.3226 - accuracy: 0.8611 - val_loss: 0.3659 - val_accuracy: 0.8520\nEpoch 485/500\n5286/5286 [==============================] - 0s 36us/step - loss: 0.3329 - accuracy: 0.8594 - val_loss: 0.3717 - val_accuracy: 0.8571\nEpoch 486/500\n5286/5286 [==============================] - 0s 36us/step - loss: 0.3265 - accuracy: 0.8581 - val_loss: 0.3858 - val_accuracy: 0.8401\nEpoch 487/500\n5286/5286 [==============================] - 0s 34us/step - loss: 0.3202 - accuracy: 0.8615 - val_loss: 0.3629 - val_accuracy: 0.8588\nEpoch 488/500\n5286/5286 [==============================] - 0s 37us/step - loss: 0.3246 - accuracy: 0.8564 - val_loss: 0.3697 - val_accuracy: 0.8622\nEpoch 489/500\n5286/5286 [==============================] - 0s 39us/step - loss: 0.3263 - accuracy: 0.8583 - val_loss: 0.3652 - val_accuracy: 0.8622\nEpoch 490/500\n5286/5286 [==============================] - 0s 41us/step - loss: 0.3220 - accuracy: 0.8596 - val_loss: 0.3528 - val_accuracy: 0.8486\nEpoch 491/500\n5286/5286 [==============================] - 0s 39us/step - loss: 0.3269 - accuracy: 0.8543 - val_loss: 0.3748 - val_accuracy: 0.8503\nEpoch 492/500\n5286/5286 [==============================] - 0s 36us/step - loss: 0.3263 - accuracy: 0.8577 - val_loss: 0.3582 - val_accuracy: 0.8588\nEpoch 493/500\n5286/5286 [==============================] - 0s 36us/step - loss: 0.3274 - accuracy: 0.8562 - val_loss: 0.3685 - val_accuracy: 0.8384\nEpoch 494/500\n5286/5286 [==============================] - 0s 37us/step - loss: 0.3299 - accuracy: 0.8587 - val_loss: 0.3749 - val_accuracy: 0.8452\nEpoch 495/500\n5286/5286 [==============================] - 0s 37us/step - loss: 0.3216 - accuracy: 0.8627 - val_loss: 0.3666 - val_accuracy: 0.8537\nEpoch 496/500\n5286/5286 [==============================] - 0s 36us/step - loss: 0.3252 - accuracy: 0.8566 - val_loss: 0.3589 - val_accuracy: 0.8605\nEpoch 497/500\n","name":"stdout"},{"output_type":"stream","text":"5286/5286 [==============================] - 0s 36us/step - loss: 0.3265 - accuracy: 0.8568 - val_loss: 0.3777 - val_accuracy: 0.8537\nEpoch 498/500\n5286/5286 [==============================] - 0s 36us/step - loss: 0.3241 - accuracy: 0.8540 - val_loss: 0.3625 - val_accuracy: 0.8554\nEpoch 499/500\n5286/5286 [==============================] - 0s 36us/step - loss: 0.3282 - accuracy: 0.8583 - val_loss: 0.3956 - val_accuracy: 0.8401\nEpoch 500/500\n5286/5286 [==============================] - 0s 36us/step - loss: 0.3218 - accuracy: 0.8619 - val_loss: 0.3630 - val_accuracy: 0.8588\nover\nAccuracy= 0.8707865168539326\nPrecision= 0.8707570769560161\nF1Score= 0.8707715620602494\ntn, fp, fn, tp\n              precision    recall  f1-score   support\n\n           0       0.89      0.89      0.89      1190\n           1       0.84      0.83      0.84       768\n\n    accuracy                           0.87      1958\n   macro avg       0.86      0.86      0.86      1958\nweighted avg       0.87      0.87      0.87      1958\n\n[[1064  126]\n [ 127  641]]\n","name":"stdout"}]},{"metadata":{"trusted":true},"cell_type":"code","source":"from keras.models import Sequential\nfrom keras.layers import Dense\nimport matplotlib.pyplot as plt\nimport numpy\n\n# create model\nmodel = Sequential()\nmodel.add(Dense(32, input_dim=10, kernel_initializer='uniform', activation='relu'))\nmodel.add(Dense(32, kernel_initializer='uniform', activation='relu'))\nmodel.add(Dense(1, kernel_initializer='uniform', activation='sigmoid'))\n# Compile model\nmodel.compile(loss='binary_crossentropy', optimizer='adam', metrics=['accuracy'])\nmodel.summary()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"# Fit the model\nprint('training')\nhistory = model.fit(X_train, y_train, validation_split=0.10, epochs=500, batch_size=100, verbose=1)\nprint('over')","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"#plot_graph(history)\nplot_graph_train_val(history)","execution_count":41,"outputs":[{"output_type":"stream","text":"dict_keys(['val_loss', 'val_accuracy', 'loss', 'accuracy'])\n","name":"stdout"},{"output_type":"display_data","data":{"text/plain":"<Figure size 432x288 with 1 Axes>","image/png":"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\n"},"metadata":{"needs_background":"light"}},{"output_type":"display_data","data":{"text/plain":"<Figure size 432x288 with 1 Axes>","image/png":"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\n"},"metadata":{"needs_background":"light"}}]},{"metadata":{"trusted":true},"cell_type":"code","source":"","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"def model_6(dim):\n    model=Sequential()\n    model.add(Dense(1000,input_dim=dim,activation='relu'))\n    model.add(Dropout(0.2))\n    model.add(Dense(500,activation='relu'))\n    model.add(Dropout(0.2))\n    model.add(Dense(300,activation='relu'))\n    model.add(Dropout(0.2))\n    model.add(Dense(1,activation='sigmoid'))\n    return model","execution_count":21,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"def model_5(dim):\n    model=Sequential()\n    model.add(Dense(1024,input_dim=dim,activation='relu'))  \n    model.add(Dropout(0.01))\n    model.add(Dense(768,activation='relu'))  \n    model.add(Dropout(0.01))\n    model.add(Dense(512,activation='relu'))  \n    model.add(Dropout(0.01))\n    model.add(Dense(256,activation='relu'))  \n    model.add(Dropout(0.01))\n    model.add(Dense(128,activation='relu'))  \n    model.add(Dropout(0.01))\n    model.add(Dense(1,activation='sigmoid'))\n    return model","execution_count":22,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"def model_4(dim):\n    model=Sequential()\n    model.add(Dense(1024,input_dim=dim,activation='relu'))  \n    model.add(Dropout(0.01))\n    model.add(Dense(768,activation='relu'))  \n    model.add(Dropout(0.01))\n    model.add(Dense(512,activation='relu'))  \n    model.add(Dropout(0.01))\n    model.add(Dense(256,activation='relu'))  \n    model.add(Dropout(0.01))\n    model.add(Dense(1,activation='sigmoid'))\n    return model","execution_count":23,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"def model_3(dim):\n    model=Sequential()\n    model.add(Dense(1024,input_dim=dim,activation='relu'))  \n    model.add(Dropout(0.01))\n    model.add(Dense(768,activation='relu'))  \n    model.add(Dropout(0.01))\n    model.add(Dense(512,activation='relu'))  \n    model.add(Dropout(0.01))\n    model.add(Dense(1,activation='sigmoid'))\n    return model","execution_count":24,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"def model_2(dim):\n    model=Sequential()\n    model.add(Dense(1024,input_dim=dim,activation='relu'))  \n    model.add(Dropout(0.01))\n    model.add(Dense(768,activation='relu'))  \n    model.add(Dropout(0.01))\n    model.add(Dense(1,activation='sigmoid'))\n    return model","execution_count":25,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"def model_1(dim):\n    model=Sequential()\n    model.add(Dense(1024,input_dim=dim,activation='relu'))  \n    model.add(Dropout(0.01))\n    model.add(Dense(1,activation='sigmoid'))\n    return model","execution_count":26,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"def final_model(dim):\n    model=Sequential()\n    model.add(Dense(1000,input_dim=dim,activation='relu'))\n    model.add(Dropout(0.2))\n    model.add(Dense(700,activation='relu'))\n    model.add(Dropout(0.2))\n    model.add(Dense(500,activation='relu'))\n    model.add(Dropout(0.2))\n    model.add(Dense(300,activation='relu'))\n    model.add(Dropout(0.2))\n    model.add(Dense(100,activation='relu'))\n    model.add(Dropout(0.2))\n    model.add(Dense(1,activation='sigmoid'))\n    return model","execution_count":27,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"def Smote_imbalance():\n    X,y=scaledData.iloc[:,0:10], data.type.astype(\"str\")\n    print(Counter(y))\n    sm = SMOTE(random_state=42)\n    X1, y1 = sm.fit_resample(X, y)\n    X= pd.DataFrame(data = X1, columns=X.columns) \n    y=pd.DataFrame.from_records(y1)\n    print(Counter(y1))\n    return X,y","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"def plot_graph_train(history):\n    # list all data in history\n    print(history.history.keys())\n    # summarize history for accuracy\n    plt.plot(history.history['accuracy'])\n    plt.title('model accuracy')\n    plt.ylabel('accuracy')\n    plt.xlabel('epoch')\n    plt.legend(['train'], loc='upper left')\n    plt.show()\n    # summarize history for loss\n    plt.plot(history.history['loss'])\n    plt.title('model loss')\n    plt.ylabel('Loss')\n    plt.xlabel('Epoch')\n    plt.legend(['Train'], loc='upper left')\n    plt.show()","execution_count":28,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"def plot_graph_train_val(history):\n    # list all data in history\n    print(history.history.keys())\n    # summarize history for accuracy\n    plt.plot(history.history['accuracy'])\n    plt.plot(history.history['val_accuracy'])\n    plt.title('model accuracy')\n    plt.ylabel('accuracy')\n    plt.xlabel('epoch')\n    plt.legend(['train', 'val'], loc='upper left')\n    plt.show()\n    # summarize history for loss\n    plt.plot(history.history['loss'])\n    plt.plot(history.history['val_loss'])\n    plt.title('model loss')\n    plt.ylabel('Loss')\n    plt.xlabel('Epoch')\n    plt.legend(['Train', 'Val'], loc='upper left')\n    plt.show()","execution_count":29,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"def calculate_scores(y_true,y_pred):\n    acc=accuracy_score(y_true, y_pred)\n    precision=precision_score(y_true, y_pred, average='weighted')\n    f1=f1_score(y_true, y_pred, average='weighted')\n    print('Accuracy=',acc)\n    print('Precision=',precision)\n    print('F1Score=',f1)\n    return acc,precision,f1\ndef calculate_scores2(y_true,y_pred):\n    acc=accuracy_score(y_true, y_pred)\n    precision=precision_score(y_true, y_pred, average='weighted')\n    f1=f1_score(y_true, y_pred, average='weighted')\n    #print('Accuracy=',acc)\n    #print('Precision=',precision)\n    #print('F1Score=',f1)\n    return acc,precision,f1","execution_count":30,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"from sklearn.model_selection import StratifiedKFold, KFold\n# K-fold Cross Validation model evaluation\n\n# Define per-fold score containers\nacc_per_fold = []\nloss_per_fold = []\n\n\n\n# Define the K-fold Cross Validator\n#kfold = KFold(n_splits=5, random_state=42, shuffle=False)\nskf = StratifiedKFold(n_splits=5,shuffle=True, random_state=1)\nfold_no = 1 \n\naccuracy_scores=[]\nf1_scores=[]\nprecision_scores=[]\nrecall_scores=[]\nr2_score=[]\nmcc_scores=[]\nbrier_scores=[]\nckappa_scores=[]\n\n\nX,y=scaledData.iloc[:,0:10], data.type.astype(\"str\")\n#X,y=scaledData.iloc[:,0:9], data.type.astype(\"str\")\n#X,y=Smote_imbalance()\n#X,y=X_train,y_train\nfor train_index, test_index in skf.split(X,y):\n  X_train, X_test, y_train, y_test = X.iloc[train_index], X.iloc[test_index], y.iloc[train_index], y.iloc[test_index] \n  #X_train, X_test, y_train, y_test = X[train_index], X[test_index], y[train_index], y[test_index] \n  #print(X_train.head())\n  #X_train,y_train=X.iloc[train_index],y.iloc[train_index]\n  print(Counter(y_train),Counter(y_test))  \n  # Define the model architecture  \n  model=final_model(10)\n  #model = Sequential()\n  #model.add(Dense(1000,input_dim=10,activation='relu'))\n  #model.add(Dropout(0.2))\n  #model.add(Dense(700,activation='relu'))\n  #model.add(Dropout(0.2))\n  #model.add(Dense(500,activation='relu'))\n  #model.add(Dropout(0.2))\n  #model.add(Dense(300,activation='relu'))\n  #model.add(Dropout(0.2))\n  #model.add(Dense(100,activation='relu'))\n  #model.add(Dropout(0.2))\n  #model.add(Dense(1,activation='sigmoid'))\n  model.compile(loss='binary_crossentropy',optimizer='SGD',metrics=['accuracy'])\n\n  # Generate a print\n  print('------------------------------------------------------------------------')\n  print(f'Training for fold {fold_no} ...') \n  sm = SMOTE(random_state=42)\n  #X_train, y_train = sm.fit_resample(X_train, y_train)\n  #X_test, y_test = sm.fit_resample(X_test, y_test)  \n  print(Counter(y_train),Counter(y_test))  \n  # Fit data to model\n  history = model.fit(X_train, y_train,batch_size=100,epochs=500,verbose=0)\n  \n  y_pred = model.predict(X_test)\n  #y_pred=np.argmax(y_pred,axis=-1)\n  y_pred=np.where(y_pred> 0.5, 1, 0)\n  #print(y_pred[0:5])\n  y_true=y_test.astype('int64')\n  #s=scoreset(y_true,y_pred)\n  acc,precision,f1=calculate_scores(y_true, y_pred)\n  accuracy_scores.append(acc)\n  f1_scores.append(f1)\n  precision_scores.append(precision)  \n  #plot_graph_train(history)  \n  # Increase fold number\n  fold_no = fold_no + 1\n\nprint(\"The mean value of accuracy is\",np.mean(accuracy_scores)) \nprint(\"The mean value of precision is\",np.mean(f1_scores))\nprint(\"The mean value of F1 score is\",np.mean(precision_scores))","execution_count":33,"outputs":[{"output_type":"stream","text":"Counter({'0': 3752, '1': 2512}) Counter({'0': 939, '1': 629})\n------------------------------------------------------------------------\nTraining for fold 1 ...\nCounter({'0': 3752, '1': 2512}) Counter({'0': 939, '1': 629})\nAccuracy= 0.875\nPrecision= 0.8756269717133692\nF1Score= 0.8736613431104483\nCounter({'0': 3753, '1': 2513}) Counter({'0': 938, '1': 628})\n------------------------------------------------------------------------\nTraining for fold 2 ...\nCounter({'0': 3753, '1': 2513}) Counter({'0': 938, '1': 628})\nAccuracy= 0.8754789272030651\nPrecision= 0.8751316710077431\nF1Score= 0.8747933515644938\nCounter({'0': 3753, '1': 2513}) Counter({'0': 938, '1': 628})\n------------------------------------------------------------------------\nTraining for fold 3 ...\nCounter({'0': 3753, '1': 2513}) Counter({'0': 938, '1': 628})\nAccuracy= 0.855683269476373\nPrecision= 0.856554660406295\nF1Score= 0.856003716985651\nCounter({'0': 3753, '1': 2513}) Counter({'0': 938, '1': 628})\n------------------------------------------------------------------------\nTraining for fold 4 ...\nCounter({'0': 3753, '1': 2513}) Counter({'0': 938, '1': 628})\nAccuracy= 0.851213282247765\nPrecision= 0.8524156635780359\nF1Score= 0.8516276900583911\nCounter({'0': 3753, '1': 2513}) Counter({'0': 938, '1': 628})\n------------------------------------------------------------------------\nTraining for fold 5 ...\nCounter({'0': 3753, '1': 2513}) Counter({'0': 938, '1': 628})\nAccuracy= 0.8378033205619413\nPrecision= 0.8490179782135431\nF1Score= 0.8392825081062495\nThe mean value of accuracy is 0.859035759897829\nThe mean value of precision is 0.8590737219650467\nThe mean value of F1 score is 0.8617493889837972\n","name":"stdout"}]},{"metadata":{"trusted":true},"cell_type":"code","source":"","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"###############################################################################\nfrom sklearn.model_selection import StratifiedKFold, KFold\n# K-fold Cross Validation model evaluation\n\n# Define per-fold score containers\nacc_per_fold = []\nloss_per_fold = []\n\n\n\n# Define the K-fold Cross Validator\n#kfold = KFold(n_splits=5, random_state=42, shuffle=False)\nskf = StratifiedKFold(n_splits=5,shuffle=True, random_state=1)\nfold_no = 1 \n\naccuracy_scores=[]\nf1_scores=[]\nprecision_scores=[]\nrecall_scores=[]\nr2_score=[]\nmcc_scores=[]\nbrier_scores=[]\nckappa_scores=[]\n\n\nX,y=scaledData.iloc[:,0:10], data.type.astype(\"str\")\n#X,y=scaledData.iloc[:,0:9], data.type.astype(\"str\")\n#X,y=Smote_imbalance()\n#X,y=X_train,y_train\nfor i in range(5,8):\n    for train_index, test_index in skf.split(X,y):\n      X_train, X_test, y_train, y_test = X.iloc[train_index], X.iloc[test_index], y.iloc[train_index], y.iloc[test_index] \n      #print(Counter(y_train),Counter(y_test))  \n      \n      #model=final_model(10)\n      #model.compile(loss='binary_crossentropy',optimizer='SGD',metrics=['accuracy'])\n\n      # Generate a print\n      #print('------------------------------------------------------------------------')\n      #print(f'Training for fold {fold_no} ...')   \n      #print(Counter(y_train),Counter(y_test))  \n      # Fit data to model\n      if(i==1):\n        model=model_1(10)\n      elif (i==2):\n        model=model_2(10)\n      elif (i==3):\n        model=model_3(10)\n      elif (i==4):\n        model=model_4(10)\n      elif (i==5):\n        model=model_5(10)\n      elif (i==6):\n        model=model_6(10)\n      elif (i==7):\n        model=final_model(10)\n        \n      model.compile(loss='binary_crossentropy',optimizer='SGD',metrics=['accuracy'])\n      history = model.fit(X_train, y_train,batch_size=100,epochs=500,verbose=0)\n  \n      y_pred = model.predict(X_test)\n      #y_pred=np.argmax(y_pred,axis=-1)\n      y_pred=np.where(y_pred> 0.5, 1, 0)\n      #print(y_pred[0:5])\n      y_true=y_test.astype('int64')\n      #s=scoreset(y_true,y_pred)\n      acc,precision,f1=calculate_scores2(y_true, y_pred)\n      accuracy_scores.append(acc)\n      f1_scores.append(f1)\n      precision_scores.append(precision)  \n      #plot_graph_train(history)  \n      # Increase fold number\n      fold_no = fold_no + 1\n    print(\"For model\",i)\n    print(\"The mean value of accuracy is\",np.mean(accuracy_scores)) \n    print(\"The mean value of precision is\",np.mean(f1_scores))\n    print(\"The mean value of F1 score is\",np.mean(precision_scores))\n    print(\" \")","execution_count":32,"outputs":[{"output_type":"stream","text":"For model 5\nThe mean value of accuracy is 0.8659290536137826\nThe mean value of precision is 0.8661241981936811\nThe mean value of F1 score is 0.8698857163553739\n \nFor model 6\nThe mean value of accuracy is 0.8525218612349155\nThe mean value of precision is 0.8528931098605594\nThe mean value of F1 score is 0.856185833223081\n \nFor model 7\nThe mean value of accuracy is 0.8540563830896344\nThe mean value of precision is 0.8544255146884322\nThe mean value of F1 score is 0.8576607209163138\n \n","name":"stdout"}]},{"metadata":{"trusted":true},"cell_type":"code","source":"##test dataset\ny_pred = model.predict(X_test)\n#y_pred=np.argmax(y_pred,axis=-1)\ny_pred=np.where(y_pred> 0.5, 1, 0)\n#print(y_pred[0:5])\ny_true=y_test\nfrom sklearn.metrics import confusion_matrix,accuracy_score,precision_score,f1_score\n#tn, fp, fn, tp = confusion_matrix(y_true,y_pred).ravel()\nacc=accuracy_score(y_true, y_pred)\nprecision=precision_score(y_true, y_pred, average='weighted')\nf1=f1_score(y_true, y_pred, average='weighted')\nprint('Accuracy=',acc)\nprint('Precision=',precision)\nprint('F1Score=',f1)\nprint('tn, fp, fn, tp' )\n\nfrom sklearn.metrics import classification_report\n\n# Generate a classification report\nreport = classification_report(y_test, y_pred, target_names=['0','1'])\nprint(report)\nprint(confusion_matrix(y_true,y_pred))\n","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"# Conclusions"},{"metadata":{},"cell_type":"markdown","source":"**Great 💪🤖!!**, now we have seen the two approaches to analyze a cyber threat. Of course we can use a lot of variables that in this case we didn't use them, for example netflows, methods callings, graph analysis, and many others, but the idea behind this work is to understand that we need to pay attention of all of the environments because when we are working in **cybersecurity** we face with a **complex problem**. "},{"metadata":{},"cell_type":"markdown","source":"# References\n\n+ [1] López, U., Camilo, C., García Peña, M., Osorio Quintero, J. L., & Navarro Cadavid, A. (2018). Ciberseguridad: un enfoque desde la ciencia de datos-Primera edición.\n+ [2] Navarro Cadavid, A., Londoño, S., Urcuqui López, C. C., & Gomez, J. (2014, June). Análisis y caracterización de frameworks para detección de aplicaciones maliciosas en Android. In Conference: XIV Jornada Internacional de Seguridad Informática ACIS-2014 (Vol. 14). ResearchGate.\n+ [3] Urcuqui-López, C., & Cadavid, A. N. (2016). Framework for malware analysis in Android.\n+ [4] Urcuqui,  C.,  Navarro,  A.,  Osorio,  J.,  &  Garcıa,  M.  (2017). Machine Learning  Classifiers  to  Detect  Malicious  Websites. CEUR  Workshop Proceedings. Vol 1950, 14-17.\n+ [5] López, C. C. U., Villarreal, J. S. D., Belalcazar, A. F. P., Cadavid, A. N., & Cely, J. G. D. (2018, May). Features to Detect Android Malware. In 2018 IEEE Colombian Conference on Communications and Computing (COLCOM) (pp. 1-6). IEEE."}],"metadata":{"kernelspec":{"name":"python3","display_name":"Python 3","language":"python"},"language_info":{"name":"python","version":"3.6.6","mimetype":"text/x-python","codemirror_mode":{"name":"ipython","version":3},"pygments_lexer":"ipython3","nbconvert_exporter":"python","file_extension":".py"}},"nbformat":4,"nbformat_minor":4}